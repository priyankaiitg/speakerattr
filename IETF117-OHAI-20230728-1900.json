{
  "speakers": [
    "none",
    "Tommy Pauly",
    "Jonathan Hoyland",
    "Ted Hardie",
    "Dennis Jackson",
    "David Schinazi",
    "Richard Barnes",
    "Shivan Kaul Sahib",
    "Mark Nottingham",
    "Mirja Kuhlewind",
    "Eric Rescola",
    "Eric Orth",
    "Ben Schwartz"
  ],
  "text": " Your seats if you're interested in talking about oblivious HTTP. Thank you. All right, folks, we're going to go ahead and get started here. Could someone close the doors in the back, please? Thank you. Hear ye, hear ye, Ohio Working Group, now starting. The Oblivious HTTP Application Intermediation Group, folks who like to care about the privacy of their IP addresses. So, welcome. This is our meeting at IETF 17. I think the next slide is probably the note well. Please note the note well. We all, in our participation here, have certain obligations as regards things like IPR and our code of conduct. We take those very seriously. And please know your obligations and abide by them. In case this is the first session at IETF you are participating in, you missed a lot of good stuff. But there's still some tips to pay attention to. Please keep your mic off if you're remote, unless you're going to be speaking. And headsets are obviously strongly recommended. On-site folks, please check in. Scan the QR code. It's not as good as Foursquare, but it will get us a better room and ensure that we get a similarly capacious room at the next IETF. So, much appreciated to check in there. And we'll use that for queue management as well. All right, here's our agenda. We have already welcomed you. We will need a note taker for this session. Could I have a volunteer for that, please? Thank you, Ben Schwartz. Blue sheets are no longer blue. We were just using the QR code. And we already covered the note well. So, thank you to Ben for covering that aspect. All right, I'm going to pass it over to Siobhan to do an update on the working group drafts and a potential working group draft. And then the main content we have today for new stuff is a new proposal from Tommy on doing OHGTP, but streaming. That is our proposed agenda. Do we have any modifications, bashes to that agenda before we launch into it? All right, Siobhan, would you like to go? It's a review of the status. Thanks, Richard. Welcome all. Yeah, so our main protocol draft was in the RFC editor queue, but there was a late-breaking change that the authors found. So, just to make sure that we are crossing our T's and dotting our I's, we brought it back for a second working group last call that passed. We got some plus ones on a couple of different email threads and some discussion on the PR. So, we merged that in and published a new version and sent it on again. Second time's the charm, I think. I hope. And then, yeah. And then for this, for the SVCB config draft, we submitted a publication. So, both this one and the main protocol draft are being handled by Murray. So, thanks to our AD. Yeah, and then there's the third draft, which is an individual draft. The feedback to proxy one. And we talked about that at the previous meeting. Folks generally seemed satisfied with the content of the draft. There was some apprehension about whether there's enough interest to justify taking that item on. So, we have, the chairs have a task to do a call for adoption to solicit that interest. And we're looking for strong, like, interest and folks speaking up. Does that plan still sound okay to folks? Or if you don't agree with that plan of action, please speak up now. All right. So, we'll be doing that call for adoption very soon. Yeah. I think that's everything. Tommy. Yeah. Yeah. All right. Hello, everybody. I hope you've had a good IETF week. We're almost done. So, I'm Tommy Pauley from Apple. And this is a discussion on adding streaming capabilities to ObliviousHTP. I apologize for not having had a draft out before the deadlines. But I expect to have something written up soon. The purpose here is to talk about the use cases. See how we feel about that. And then go into the proposed details of what would be in this 0, 0 draft. All right. So, first to review what our status quo is for OETP. We currently have a single message in each direction for request and response. That must be encrypted and decrypted in a single chunk each. And that fit very, very well for many of the use cases that we have. And it's still a very, very good thing. I'm glad we are publishing this document soon. So, things like DNS, et cetera, are small messages. And really, there's essentially nothing you can meaningfully do with the data in the request or response without having the complete thing. You can't just be like, all right, I read the first half of your DNS query. That's enough for me to start responding. Like, not useful. And OETP is communicating binary HTTP as its content by default. And very helpfully, binary HTTP does support effectively streaming. It has this mode for indeterminate messages. So, either you have a message where you know all of the, like, the length of the fields and how many fields you'll have and the body up front. Or you don't know up front and you do, instead of length value, you have a delimiter at the end of these areas. So, binary HTTP is capable of doing more than what we have in OHDP today. And actually, also, HPKE that OHDP is based on also allows you to have multiple chunks. We just don't exercise that today. So, there's, it's definitely possible to do streamed OHDP. And why would we want to do this? And I was a skeptic when Martin originally brought this up. And, you know, for use cases like DNS, I still am. But there are definitely usage patterns that can benefit from streaming. We've talked about this on the list a bit. You can have very long messages or messages that take a long time to generate that can be processed in multiple parts. Maybe you have some database lookup or some other thing that's being interactively generated that you can start displaying something to the user or doing local processing on while you're waiting for more content to be generated. And you can also have cases where you have a more interactive workflow in which someone makes the initial request and sends, you know, no body or some of the body. And then the server sends to the response. But then you can keep sending more body data in both directions and actually have a little interactive session over OHDP. And you could do that if you had chunks and streaming. So assuming we actually want this and we think that these are good use cases, what would we need to change to make the streaming work? Here I've identified three primary things. Happy to hear if there are more. First, there is just kind of the cryptographic mechanisms about how we're doing chunk encapsulation, how we do the HPKE for the requests and how we do the AADs for the responses to make them chunkable. We have the request and response format for how we actually send these chunks and delimit the chunks and everything else like that. And then, as we talked about, when this was originally proposed, it makes sense to have a new media type for this, since if we are changing our request response format, we need to know that we're doing that. Jonathan Hoyland. Chunk enthusiast. Jonathan Hoyland, Cloudflare. Does this not change the security properties at all? Like, I haven't had a chance to think about this at all, but I'm surprised it's not on the list. Oh, that's why you're here. No, no, it's certainly, I think it does. I mean, these, sorry, I was trying to say, like, you know, what are the technical, like, on the wire changes that need to happen? I think we need to make sure that we do correctly analyze the security properties and any implications it has. For things like HPKE that do support chunks, like, I believe that analysis was done. Yeah, that bit's fine. It's the doing it in multiple shots. Does that change the obliviousness? Oh, for the privacy aspects of it. Yeah. Yes, do a security analysis. Standing on one foot, it's fine. Yes. I mean, certainly, like, it's not going as far as saying that you are being stateful between multiple HTTP requests and responses. You know, so I think there are two cases here, right? There's the case where you just have, like, I'm just really slow at sending the response I would have done anyway. In this case, I think it's purely a performance thing. If we are going back and forth, then that does allow you to be stateful within that one session, and I think it's then, to some degree, a use case, an application analysis decision about how are we using this, and are we okay with that, whatever state we are building up within that one session? Like, if you had an extremely long-lived session, and you tunneled all of your web browsing over this, then, like, yes, that's worse for privacy. Thank you. Yeah. All right. There's more of a queue. I can't see who's next. Ted. Ted. Ted, Friday enthusiast. So I believe the way the charter is currently written, it focuses a good bit on the use cases for this protocol being those which do not require correlation, where the requests and responses do not require any form of correlation. And so when we went out with the charter, it was like, hey, you're not going to use this to go talk to Gmail, right? Because you're not going to do that. And specifically here, like, we're talking about correlation across different requests? Yes. Yeah. Totally. So that if you went to Ojai server one for request one and Ojai server two for request two, and they talked to two different backends because of ECMP, it's all good, right? But so when I first saw this and I thought and saw streaming, there was like a little moment of panic and like, what? Speaking as mock chair, I was like, am I going to have to have an Ojai for mock? This is going to be really bad. Don't do this to me. Please, no. So thank you for the actual know that you calm down. It's Friday. You don't have to panic now. Pieces of this. But I do think it's going to be very hard for you to describe in simple terms, and I encourage you in the zero, zero to do it in very simple terms, which kinds of flows will need this sort of chunked indeterminate behavior, but will retain the lack of correlation property? Yeah, that's a really good point. And so please ditch the phrase streaming at some point along the way, if I may suggest, and pick something else that highlights that particular pairing, because I think that's the core of what you're proposing, and it's a little bit different from what leaps to mind. Thanks. Yeah. And here, streaming is inherited from the PR that was abandoned previously. I would love to hear people's suggestions. Maybe it's just chunked, or I don't know what it is. Yeah. Okay. Yeah. Send me your bad ideas. Dennis Jackson, Mozilla. Just to echo the two previous comments and questions and understand better, the intent here is that the server is going to start processing this before it reaches the end of the stream, or just that the client is able to send this a bit at a time? One of the main use cases I have in my head is that actually, you know, probably your request is small and simple, and then you get a large response that may take a while to look up or to generate. And rather than saying, I need to wait for this giant multi-megabyte response to all come in to be able to decrypt it at all, I can start using it in chunks and processing it while I'm waiting for this very slow server or slow network to keep giving me the rest of the data. So at least on one side, you're going to start processing the data in the stream before the end of the stream. Yeah. Okay. Thanks. That's right. David Skenazi, privacy enthusiast in this case. So in our world where we're, you know, working to improve privacy, OHI is one of my favorite tools, but it's not the only one. And you can guess which one is my really, really favorite one. So, so mask, if it wasn't obvious, but no, but so here I regularly answer the question at work of, wait, why are we implementing both mask and OHI? I also do this. Yeah, right. I kind of wrote a draft explaining that, so I don't need to repeat it anymore. But anyway, there's, there, you scope for different things. And my answer when people ask is when you have multiple requests that are de-correlated, you reach for OHI. Yes. And then if you have a case like a web browsing context where multiple requests are correlated, you reach for mask. And so my question is, this almost, like my naive impression is why don't you use mask and I hate to be that guy, but can you explain why that's not the right solution here? Yeah. And having also answered this question a multitude of times, I think there are a couple other dimensions to why you would choose OHI over mask. And it's certainly not the generic one. But so first of all, I think the big one is OHI does require modification or coordination by the servers along the system. I think that's a really big one that you need to intentionally be using this and working with the server that is trying to preserve your privacy, whereas mask is fantastic for talking to completely unmodified backends. So that's like the first divergence point. Then, yes, the other property here is with OHI, we're able to do, you know, it's per message decorrelation and separation as opposed to, you know, per TLS session or whatever else the end-to-end connection is. Yes, yes, yes, yes, chunk is different, but like oblivious HTTP message, which could include binary HTTP or something else. But it's when you want to be able to, so the privacy guarantees are stronger with oblivious HTTP, unless for a mask session, you had to do kind of like the end-to-end TLS, send one message and then tear it down, which is much less efficient. So if you want to have the complete decorrelation properties and are able to coordinate with the end server and you care about performance, you really should use OHI. And streaming here just means I may have a large message, but it's still always one request and one response. I see. So just to summarize, make sure I get what you're saying, you could solve this with mask. However, you would need for each streaming request to do a full TLS handshake, and that would be cryptographically, computationally more expensive than doing one HPTE. So this is a performance optimization. That's a good justification. Yes, yes. Right. Like in order to get the same privacy guarantees as this, you would have to do, you would lose way more bytes. You would have way more extra encapsulation, other things like it just would not be worth it. Policy signatures and everything. Okay. That makes perfect sense to me. Thank you. Great. Also, when you are working with backend servers that are trying to build out this privacy system, forwarding along a post request works really nicely through various reverse proxies. And it's a lighter weight than them all having to, you know, open up TCP sockets and worry about allocating an IP address to forward your traffic. It works well in a server environment. Okay. So, oh, Mark. Mark Nottingham. Where is Mark? Mark, you're up. What? Where'd you go? You're virtual now. Is that working? There we go. Yep. Go ahead. Okay. Not coming into the last day of meetings, enthusiast. I think, Tommy, what you were just talking about should probably go into the current Ohio draft or some abstract of it. That was a pretty good explanation of some of the differences between mask and what we're doing here. But I got into the queue to ask what your intentions are regarding non-final responses in HTTP. I missed that. What do you want to do about informational responses, non-final responses? Oh. Binary HTTP already handles those. Right. So, actually, that makes this work. Like, the streaming makes it work pretty well. Like, if you want to do that, you kind of need this. Right. Because they were excluded last time, but this one, it should be okay. Yeah. Yeah. That's actually another good reason for this. Yeah. Yeah. We'll mention that. Thank you for bringing that up. Okay. All right. So, I think next we're going to talk about the actual... Yes. Okay. So, for chunk encapsulation, the things that we need to additionally protect... Oh. Dennis, did you want to... I forgot this slide. So, you get to the end. Okay, great. These are what I'm positing is the things that we additionally want to protect. We need to make sure that if we are chunking things, that those chunks cannot be reordered without the receiver knowing about it. And we also need to make sure that chunks cannot be dropped either in the middle or at the end. We need to know that we got to the final chunk within a single chunk of the cryptographic properties we have from HPKE and the AAD on the way back, is that we don't have to worry about a single chunk being truncated that should be detected. So, these, I think, are the two primary new requirements tell me why I'm wrong. No, I think you're right. Sorry, Dennis Jackson, Mozilla. But I also think it's not enough by itself. So, if the client or the server are going to start processing this stream earlier on, they don't necessarily know what's in the next chunk. So, without some additional guidance about where applications cut those chunks, you're back to the same kind of truncation attack. So, an example might be in a situation where you can introduce extra content, like, into that stream to move the chunk boundaries, which turns out to be quite common in a lot of applications. And then, potentially, you get some kind of confusion at the application layer, because you start processing the chunk early, without having seen what's in the next chunk, which is going to change what you would have done, had you seen them both at the same time. Does that make sense? Are you talking about this at the, essentially, the application layer? Where, like, once I've decapsulated and I have the binary HTTP, is that fundamentally different from the fact that when I'm reading from an H2 stream, I don't know what's going to come up in the future, because I'm downloading an enormous file, and it's going to take several minutes. Yeah, no, you're absolutely right there. But in this circumstance, because you've got that middle relay, which is trusted to know identities, but not content, you've got an attacker that's much more capable of observing those chunks and delaying chunks, or potentially, you know, fiddling around with what's going on. So, you've got a stronger privacy property here than what TLS ordinarily is striving to. I see, I see. So, absolutely something we should note, do you believe there's anything beyond privacy security considerations to be done there? I mean, I can't think of anything. If it's going to be general, you can't do anything better, but getting that careful wording in the draft, I think, is really important. Yeah, that's a good point. And it does, what I'm getting out of this overall is that, you know, we need to have clear text on what is suitable content for these and what is unsuitable. Yeah, and maybe a very clear example of how it might go wrong, because that is often more compelling than unless it's not. Thanks. Richard. Richard, Brian, speaking logically from the floor, but too late to actually get on there. Yeah, yeah, yeah. It seems like maybe one additional requirement you actually have here that I don't see on the slide is guaranteeing that all of the chunks arrive at the same server or at the same ultimate destination, right? Because if there's something processing this and, you know, there are multiple instances of the same, as Ted was saying, for ECMP, right? With the existing uncorrelated requests, different OHI messages can land on different servers and there's no problem. And that's not the case for different chunks here. True. True. I think the requirements, which I did not clearly state who these requirements are for, I was thinking of these as for the receiver in order to verify the integrity of the data. Yeah. Nothing about the set. And I think there may not be anything to do in the protocol to assure that the property I mentioned. That's a great point. But it may be something you mentioned in deployment considerations, operation considerations. Well, one of the failure modes here is your chunks get misrouted and you have a very sad day. Yeah. But I had been musing on your last slide as to whether you had mentioned the server beginning to process the request while it was still inbound. I had been musing on the question of whether the server was allowed to begin responding to the request while the request was still arriving. And it sounds like... As a property of HTTP, it is. Yeah, no, I realize that's legal. But if you're concerned about these properties, then maybe you want to have some sort of cryptographic interlock between the request and the response that would prevent that. I don't... My initial response is, I don't believe we need it, but that's something we should discuss more. Okay. Who's after Richard? Jonathan, I'm next. Jonathan Hoyland. Jonathan Hoyland, Cloud Player. There's also the trivial one, i.e. you shouldn't allow insertions. Thank you. Yeah, like I think that's already... It's already handled, but that's one of the requirements. Yes, that's fantastic. Thank you so much. Yeah. Yeah, cool, but sorry, one hail of a question while I'm listening to the discussion. And maybe I don't understand the use case well enough, and maybe I don't understand the non-existing solution you're proposing well enough. But it sounds like... It's just here. Yes. It sounds like there are applications who actually can provide small and bigger chunks on their own, right? So I'm wondering, is that actually the right layer to provide this functionality? Is this something that the upper layer can do anyway in a lot of cases already? I mean, I think the fundamental problem with the existing OHDP is that it allows exactly one chunk entirely. So it means you must have the complete request or the complete response before you can process any one byte of it. And so as long as that's not a problem, we should continue to use the existing one. But if I can just put my requests and replies into really small chunks, I can process them one by one on the application layer if I can. Well, no, but with the layer going inside OHDP, you could chunk them up very, very small, but then the other side would have to wait for everything to arrive before it could even decrypt a single byte. No, I'm talking about chunking them up in different requests. In separate requests, but the separate requests are completely non-correlatable. Yeah. Right. So I mean, I need some logic on the higher. Yes, I mean, I guess if I need to download a giant file, I could have, you know, what is the patch request? I mean, like to say, like, you know, give me bytes one through 10, then bytes 10 through 20 as like separate requests. But that seems a bit. I mean, what I'm saying at the beginning, it's like, I'm not sure I understand the use case well enough to understand if that is the right solution or if there's a different solution. Okay. Who's next? Hector. Hector. Hector. Hector. Hector. So we have like some handshake and then that we like start with and then like we start streaming this data and that seems like we're kind of triumphantly reinventing TLS. It's a little concerning, frankly. So like the rationale behind HTTP was that you wanted to have things that were like too expensive to like tee up the TLS handshake and you want anyone to correlate across things. And so I'm like a little trying to figure out like why this isn't called mask. Right. So you missed the earlier discussion. I did. Which we have for several minutes on this topic. Okay. I'll be happy to take it to the list. Okay. Sure. Yeah. The TLDR, I mean, essentially is, this is a performance improvement on that. Like, yes, you could do a full TLS handshake and then do exactly, and set up an HTTP session and then do exactly one request over it and then tear the whole thing down in order to get the same privacy properties as this through a forward proxy. Um, but that's far more back and forth than just saying, I, I'm just going to use HPK's existing ability to, you know, chunk up the data into two chunks if it's being slow and driving it. Yeah. Two. Yes. Now five, 50, a million. Like, I guess what I'm observing is you're like verging into recreating a TLS in pieces. I mean, fundamentally, like what we're doing with HPK and OHCP to begin with is, you know, just choosing to send one chunk. Um, I, I don't see why going, just expanding that to use several more chunks means we would want to then entirely switch over to a different protocol here where we need to now set up a stream state. And this is the way that you gradually reproduce every feature of TLS by one feature at a time. Like, I, I guess, I guess I understand what you're saying and like that marginal, that argument doesn't make sense in the margins. And I'm trying to ask you to look at the general equilibrium rather than the margin. Right. But I mean, also like, you know, in, in the case of mass, let's say I have a forward proxy. So, uh, you know, that would require that forward proxy to, you know, set up what, you know, some TCP or UDP forwarding state to the next hop. And if I'm, you know, bouncing this around, like that is a lot of infrastructure and IP allocation. Whereas here it's, you know, just more of like a reverse proxy. Like I just send a post request with my message. It just forwards it along through whatever chain it needs to and sends it back to a different layer here. And to repeat myself, yes, that sounds like a sensible argument at the margin. And yet the end game is you've reproduced the game protocol. Um, so, um, okay, well, I guess, is the end game of this a call for adoption? Because I guess I just talked about it. No, no, the end game is talk about what should go into a zero, zero draft, individual draft to even have that discussion further. Okay. Yeah. Yeah. We want to see if there's interest and see if folks are interested in working on it. And then. And see if there are obvious holes. Right. Um, I guess one question I had Tommy was, um, um, do you have like a list of use cases that, um, that need this right now? Or, uh, I've described the properties of the use cases, right? But did you have like, um, so is there any, I guess right now, are there use cases that, um, you think would benefit from this? Because we just had some chat about a chatter about that in the chat. Yes. Okay. Um, yeah, I'm not sure. I can't get into the details of them. Sure. Absolutely. Yes. Jonathan. So is one property of this, okay. And I really should have read the draft already. Uh, is one property of this that the entire. Okay. Excellent. Then I, uh, the, the entire request has arrived before the first bite of the response has been sent. That is an artificial limitation. You could add. There is no reason to necessarily do that unless we say that we want a particular. Security privacy property that otherwise would not be achievable, but there's no requirement for that from the protocol, nor from like binary HTTP within that. So if I'm being a psychopath, I could just do like a full, like, this is TLS, ha, ha, ha. Um, and just like continuously send a bunch of different requests, get a bunch of different responses, send a bunch of different requests. Sure. And if you wanted to essentially make this, like, this is not TLS, you could just add the artificial requirement that you must send all of the requests and then get all the response. Like that is a thing you could artificially put on this if you want. It, it, if we thought that was valuable. It seems that that is a property that is in the original HTTP. Yes. So if we wanted to preserve that property. And I think that does have an impact on the security property you actually get. I'm not sure that's necessarily the one you want, but it is something that it makes a difference. Yes. You're right. You're absolutely correct. And if I'm thinking about the use cases that we would have for this, um, I believe, you know, a lot of them, yeah, I think they'd be fine with that separation. Or even just a commitment to the full request in the first message, right? Like, just part of the thing is like, when you think about indeterminate length binary HTTP, like it does not necessarily know the entire length of a response. So what I'm, what I'm worried about is an adaptive, like based on something I get in the response, I change what I'm sending in my request. Um, and not just like, right. So we can prohibit that. And that, that does like prohibiting that would make this, you know, true, truly only a performance optimization and not change any other property. And that would make it easier to reason about for analysis purposes. I agree with you there. Thank you. Thank you. Uh, David Skenazi is me again. Um, so thinking about this more, um, this, so this is a question. So this is an optimization, but it isn't a free optimization. Um, uh, um, OHTP has weaker security properties than layered TLS or, you know, like mask in the multiple nested mask proxies. What are the weaker properties? I does not have forward secrecy. Um, if the, uh, HPK private key leaks, then information can be decrypted as opposed to, if you do an end to end TLS and the session keys get thrown out, um, you have perfect forward secrecy. Um, so there are times when it is worthwhile to reduce your security for improved, um, performance. And that's what the original HTTP is for. Um, when you have small, many, many small requests that are decorrelated, it makes perfect sense to do that instead of the heavyweight approach of nested TLS. Um, but in this case, it's not guaranteed. So one of your use cases is a giant file download. Um, I think the correct answer there is to spend the slightly bigger performance hit, which will be negligible in the end of the day to get the improved security. Um, I totally understand that, uh, you might not be at liberty to talk about the, uh, uh, all your use cases, but I think before I consider wanting to work on this and adoption, I would need to know more why this use case needs this. It's still unclear to me. Oh, Eric. Yeah, just based on all the feedback of, oh, it seems like TLS in these cases, oh, you should use TLS in that case, stuff like that. I think any draft on this needs a pretty good guidance section on you should use TLS if these things are true and talk about if, if you need back and forth, if you need more than so many chunks in your response, you need more, more than so much time in your response. And that also means we kind of need to figure out where these borders are for when it's useful to use oh, high versus TLS before we have this draft. So I think that's stuff we need to figure out. And I think stuff we need to write down before we can really have a reasonable draft that can go far. Eric, I mean, just sort of like follow up on what I was saying earlier when Eric and David were just saying, I mean, like one of the advantages of using a transport layer, another thing when you were downloading a big thing is that the rate control works and here the rate control, you have two decoupled transport sessions and I don't understand the rate control works in any meaningful way. And if there's rate control from like this rate control from the center to the rate control from like, from like the center of the proxy and the proxy there and it's like, correct. And it's like extraordinarily goofy. Well, I mean, essentially the, the relay or any relays or other servers in between are responsible for propagating back pressure, which would be the case for any reverse proxy as well. Well, I mean, like the, the, the, the way that certainly is the way that, and that is not one reason why when we're just running quick over mask rather than, rather than running to TCP termination, turning each side. Because the rate control is end to end. But I think, I mean, I think in the case here, because remember like a large OHDP relay is going to have essentially like normally just one giant, you know, H2 session with the gateway or, you know, a handful of those. Which overall for, it is a better performance thing. And it is just having to deal with the rate for the streams. I, I, I'm not sure I'm following, but maybe that's a separate, I mean, why, why, like, why is that better? I mean, the, so in the case of like mask, right, we are like, what we want ultimately is, you know, this end to end session that we are doing lots of stuff, you know, we're doing H3, H2 to the end servers, where we want that end to end flow control by pressure, et cetera. In this case, I mean, clients will have some session to the relay, that relay will have essentially, you know, one logical large session to the gateway and they just independently deal with their own. No, no, I'm saying, but I'm saying it's a defect. I'm saying that, like, that, like the rate control, like, that, like, that the, that, like, you have totally decoupled rate control between the data in each direction. And it's just weird. But I think for single request response pairs. Is the, is the, the scenario you're envisioning, like, the relay to gateway link gets stopped up for some reason, but the inbound to the relay from the client doesn't know about that. It feels no back pressure. And so the relay ends up with basically unbounded buffering. But it should just have back pressure on its, whatever units like H2 or H3 streams are on either side. Yeah. I'm not saying one can't build that. What I'm saying is that you took a situation where you had, like, automatic back pressure, both in the transfer protocol, and you're placing it by, by back pressure at the application layer. Which is, I mean, extremely, like, it's a pattern that we already have to deal with for OHDP and essentially every reverse proxy case. Like, it's, it's also a pretty well-established thing here when we are just doing an end-to-end HP request response. Um, yes, but there's, they're self-clocking because they're, because, because, because they, they're one-to-one. They're what? One-to-one. The part before that. It's self-clocking. Self-clocking? Yes. Anyway, like, it may require an offline discussion. I'm just saying, like, you're creating a series of new problems. I think we have to, I think that, like, demotivate this. I mean, like, I think it's one thing to say, like, I wanted to send, I wanted to send two chunks, but when you're like, I want to send, like, a, like, effective HL stream is, like, a different story. So, I think, at least, you know, the way I've been thinking about this is the alternative that you're starting with is you just have a very, like, you are using a OHDP, and you have a very large response that is being slow to generate. And the question is just, are you waiting for that or not? And in that case, you know, whatever backpressure there is, those same number of bytes are going to go through that relay. Yeah. No matter what. I mean, but no, they'll, I mean, I mean, David sort of, and David indicated, like, that the price, the price you paid for this was for foreign secrecy. There's a bigger price you pay, which is it doesn't work with unmodified servers. Oh, no, absolutely. And the predicate for using OHDP in the first place is that you have a server system where you, they are cooperating to do this, that they are trying to do this, so that you can get better decoupling on a per-message basis than you would if you had to use mask or if you have to do a TLS session per every single day. No, I understand that. Is that performance privacy tradeoff? No, I understand that. What I'm saying is that when, that, the reason that tradeoff made sense was because you're sending one message. And when you're sending a crap ton of messages, a tradeoff no longer makes any sense. Right. There are also, well, there's also cases, again, kind of going into the parameters of the use case here. Well, I think you need to. Yes, I know. Where upon making a request, you do not necessarily know the size of the response. And it may be essentially an extremely short thing that's very appropriate for OHDP, but it may have some slower chunks. And having to commit to every single request being a full, you know, back and forth TLS handshake to then send the request in your HTTP session when it may end up being something that comes back in a single RTT is also expensive. I mean, I suppose that might be the case, but I guess, like, I'm waiting to hear a motivating use case for why that would be the case. And, like, we have some internal thing I'm not going to talk about. It doesn't really do it for me. So, like, I guess this isn't the web. Like, this is not the web. These are corporate. Because the endpoint is effectively controlled by both sides, right? Yes. And so, like, you ought to have a fair amount of intelligence about what's going on, about what characters' response are likely to be. So, anyway, I guess I think I'll step back for a bit. Okay. All right. Let's see. All right. So, going further on the chunk encapsulation, the proposal is essentially what Martin had written up in his PR, like, a year ago with slight modification. So, to preserve the integrity of chunks, HPKE, the sequence of it, HPKE already supports this. It has its own sequence numbers that needs to be added. You essentially need to add a counter to the AAD message nonce for responses. That is essentially just cribbing what HPKE did. And then for the final chunk integrity, the proposal here is to, you know, modify the AAD to say, yes, this is some sort of sentinel for marking that this is the final chunk. And then with those two things, I believe that it should achieve the requirements on the previous slide. The request response format is also what Martin had proposed, where you can have just a length var int that precedes the different chunks. And then the final chunk is indicated by a length of zero to say that this chunk extends to the end of the stream and that one also would need to have the final sentinel in its AAD. So, that seems to be a very simple, obvious thing to do. One note I'll make is that you don't strictly need to use this particular request response format with the internal crypto, that the length fields here are not covered and they don't necessarily need to be in order to achieve those properties. And so, it does allow you to potentially have other media types or other formats where you, ways to move the chunks around that don't change the properties of the actual cryptographic stream here. So, if you'll have concerns with that, we can talk about it. But I think we also talk about more of the overall use case going forward. And if we do this, we have different media types, we could call it this, and we don't want to call it streamed, we call it chunked, but this is a bike shed to be had. And I think, oh yes, also Mark Nottingham had brought up several good points about just like the ways you hold this thing apart from the actual wire protocol, such as, you know, do you have to negotiate or indicate support? It is generally OHDP is done kind of with some a priori configuration that is out of band that could indicate whether or not you want to do chunked-ness. How you do this for discovered cases, I'm not sure. I'm also not aware of use cases for that, particularly. And then there's a question of, you know, can you have asymmetrical cases where the server could reply with chunks and the client did not request with chunks? Stuff to be discussed. Anyway, so the next steps are for me to use this discussion to actually write up the 0, 0. I've been playing around with implementation of this. I know Chris Wood was also working on this. Martin Thompson had built a version of this a long time ago. So there's, if people are interested, we can, you know, play with Interop on that. And then we should have more discussion once we have a 0, 0. So thank you for the input, everyone. Thanks, Tommy. So, yeah, I think we are looking for use cases that folks might have, which they feel would benefit from this work, you know, as opposed to, like, masks or something. So, yeah, so if folks have use cases right now that they think would benefit from this, please speak up. And also, yeah, is there, I guess we can go to that question first, yeah. Hi, I wanted to put in a question for Tommy. So, we, we heard some discussion about the lack of forward secrecy, basically because the client is constantly encrypting to a fixed public key for the gateway. It's somewhat orthogonal to this specific proposal, but I wonder if we could enable some amount of forward secrecy with oblivious HTTP by, for example, providing the gateway a way to rapidly update, provide clients with fresh public keys. In a, in a chunked world specifically? I, I think it's orthogonal, except that it would go to one of the objections that's been raised just in this discussion. It feels like then you go down the route of, you're reinventing TLS? Oh, oh, you go, right. So, it might be simpler than that. For example, we could simply note that the gateway has the ability to create short-lived gateway key configurations and very frequently rotated to public key, and then that touches on our consistency discussion. So, how fast could you rotate your key config while having a working consistency system? Given that, generically for OHDP, the mechanisms for key distribution are left as an exercise to the reader, or, I think stuff like that would need to really be specific to some as yet unspecified protocol for distributing those, because there aren't assumptions you can make generically on OHDP about how quickly they're rotating anyway. You know, it's very possible for existing deployments to rotate those keys every 15 minutes if they want. Sure. In that case, it could just be, for example, a BCP to OHDP implementers, and that might also help people feel more comfortable that this is likely to be used in a way that's not as weak as some have been concerned. That makes sense. Jonathan. Jonathan Neul and Kravler. You could put the... Wow, this mic is really loud. You could put pre-keys, pre-shared DFY Almond keys in DNS, right? And just change them, every single DNS request. And just mix that into your key, right? Just do, effectively, a DFY-Helman handshake. Yeah, I'm a little worried about the targeting there, but, yes, this is also going beyond the scope of the force. I think reinventing MLS is probably out of the scope of the training. Hey, hey, hey, I wasn't going to use MLS. I was going to use art. We got rid of that in MLS. David Skenazi. Since we're designing the crypto at the microphone, let's do the name, too, and call this Oblivious Mask. But, no, more seriously, David Benjamin pointed out to me that another security property that this lacks is replay protection. So, for the same reasons that OHDP does. The OHDP specs has all of this, but it's not marketed very well. Not everyone in the room knows this or knew this, maybe, initially. It's easy to forget. And this sounds like it's building something that has these weaker properties that people might accidentally use. So, there's a risk there. And another point that Ben made is the security properties of OHDP and this look a lot like 0RTT. And, actually, it's exactly the same construct as Google Quick 0RTT. And Ben's proposal of then you inject another key is exactly how GQuick switches from 0RTT to 1RTT. But, again, this is reinventing TLS 1.3, which is not the point. Yeah. But, so, because of all these things, it really feels like this is building a weaker version of TLS, which is TLS that only sends early data, kind of. And that's not a good thing. So, maybe let's chat offline about these use cases because I think that is the bottom line here. They will have to be incredibly compelling in order to propose something like this with weaker security that people might use and accidentally, in cases where they shouldn't be. Like the big file transfer, for example. One other conclusion that will come from this is that I am very glad that when this was originally just a PR to be like, oh, yeah, we're just going to have all of OHDP be streamed, that I was like, maybe we should not do that. But, so, this conversation bears that out that that was the right call because this does warrant much more discussion and thought. I do think it is still useful for use cases, so, like, you know, we'll work on that. But it is not yet nearly as obvious, I think, as the simple OHDP case. All right, so, Tommy, you're welcome to send in a 0-0 draft. I think you've gotten a lot of good feedback here, and so we'll look forward to that and handle questions on the list. All right, that brings us to the end of our planned agenda for the day. We have, oh, a little while left in the session in case anyone has any other business. But otherwise, we'll conclude. Is there any other business for the good of the order? All right, then I declare us concluded. Thank you. Woo! Thanks, Ben, for the notes. Thanks. Thanks. Thanks.",
  "segments": [
    {
      "id": 0,
      "seek": 0,
      "start": 0,
      "end": 8.5,
      "text": " Your seats if you're interested in talking about oblivious HTTP.",
      "tokens": [
        50365,
        2260,
        11069,
        498,
        291,
        434,
        3102,
        294,
        1417,
        466,
        47039,
        851,
        33283,
        13,
        50790
      ],
      "temperature": 0,
      "avg_logprob": -0.6065842509269714,
      "compression_ratio": 0.9142857142857143,
      "no_speech_prob": 2.131737494859287e-12,
      "speaker_id": 6
    },
    {
      "id": 1,
      "seek": 3000,
      "start": 30,
      "end": 59.980000000000004,
      "text": " Thank you.",
      "tokens": [
        50365,
        1044,
        291,
        13,
        51864
      ],
      "temperature": 0,
      "avg_logprob": -0.657621701558431,
      "compression_ratio": 0.5555555555555556,
      "no_speech_prob": 7.546966523941379e-12
    },
    {
      "id": 2,
      "seek": 6000,
      "start": 60,
      "end": 67.16,
      "text": " All right, folks, we're going to go ahead and get started here.",
      "tokens": [
        50365,
        1057,
        558,
        11,
        4024,
        11,
        321,
        434,
        516,
        281,
        352,
        2286,
        293,
        483,
        1409,
        510,
        13,
        50723
      ],
      "temperature": 0,
      "avg_logprob": -0.2716371181399323,
      "compression_ratio": 1.427906976744186,
      "no_speech_prob": 8.90980106534045e-13,
      "speaker_id": 6
    },
    {
      "id": 3,
      "seek": 6000,
      "start": 67.24,
      "end": 68.96000000000001,
      "text": " Could someone close the doors in the back, please?",
      "tokens": [
        50727,
        7497,
        1580,
        1998,
        264,
        8077,
        294,
        264,
        646,
        11,
        1767,
        30,
        50813
      ],
      "temperature": 0,
      "avg_logprob": -0.2716371181399323,
      "compression_ratio": 1.427906976744186,
      "no_speech_prob": 8.90980106534045e-13,
      "speaker_id": 6
    },
    {
      "id": 4,
      "seek": 6000,
      "start": 70.38,
      "end": 71.16,
      "text": " Thank you.",
      "tokens": [
        50884,
        1044,
        291,
        13,
        50923
      ],
      "temperature": 0,
      "avg_logprob": -0.2716371181399323,
      "compression_ratio": 1.427906976744186,
      "no_speech_prob": 8.90980106534045e-13,
      "speaker_id": 6
    },
    {
      "id": 5,
      "seek": 6000,
      "start": 76.52,
      "end": 79.38,
      "text": " Hear ye, hear ye, Ohio Working Group, now starting.",
      "tokens": [
        51191,
        30685,
        606,
        11,
        1568,
        606,
        11,
        14469,
        18337,
        10500,
        11,
        586,
        2891,
        13,
        51334
      ],
      "temperature": 0,
      "avg_logprob": -0.2716371181399323,
      "compression_ratio": 1.427906976744186,
      "no_speech_prob": 8.90980106534045e-13,
      "speaker_id": 6
    },
    {
      "id": 6,
      "seek": 6000,
      "start": 79.96000000000001,
      "end": 84.96000000000001,
      "text": " The Oblivious HTTP Application Intermediation Group, folks who like to care about the privacy",
      "tokens": [
        51363,
        440,
        4075,
        45997,
        851,
        33283,
        39512,
        5751,
        3130,
        399,
        10500,
        11,
        4024,
        567,
        411,
        281,
        1127,
        466,
        264,
        11427,
        51613
      ],
      "temperature": 0,
      "avg_logprob": -0.2716371181399323,
      "compression_ratio": 1.427906976744186,
      "no_speech_prob": 8.90980106534045e-13,
      "speaker_id": 6
    },
    {
      "id": 7,
      "seek": 6000,
      "start": 84.96000000000001,
      "end": 85.88,
      "text": " of their IP addresses.",
      "tokens": [
        51613,
        295,
        641,
        8671,
        16862,
        13,
        51659
      ],
      "temperature": 0,
      "avg_logprob": -0.2716371181399323,
      "compression_ratio": 1.427906976744186,
      "no_speech_prob": 8.90980106534045e-13,
      "speaker_id": 6
    },
    {
      "id": 8,
      "seek": 6000,
      "start": 87.02,
      "end": 88.48,
      "text": " So, welcome.",
      "tokens": [
        51716,
        407,
        11,
        2928,
        13,
        51789
      ],
      "temperature": 0,
      "avg_logprob": -0.2716371181399323,
      "compression_ratio": 1.427906976744186,
      "no_speech_prob": 8.90980106534045e-13,
      "speaker_id": 6
    },
    {
      "id": 9,
      "seek": 9000,
      "start": 90.48,
      "end": 92.2,
      "text": " This is our meeting at IETF 17.",
      "tokens": [
        50389,
        639,
        307,
        527,
        3440,
        412,
        286,
        4850,
        37,
        3282,
        13,
        50475
      ],
      "temperature": 0,
      "avg_logprob": -0.2199476288586128,
      "compression_ratio": 1.51,
      "no_speech_prob": 1.304196659522583e-12,
      "speaker_id": 6
    },
    {
      "id": 10,
      "seek": 9000,
      "start": 92.42,
      "end": 93.84,
      "text": " I think the next slide is probably the note well.",
      "tokens": [
        50486,
        286,
        519,
        264,
        958,
        4137,
        307,
        1391,
        264,
        3637,
        731,
        13,
        50557
      ],
      "temperature": 0,
      "avg_logprob": -0.2199476288586128,
      "compression_ratio": 1.51,
      "no_speech_prob": 1.304196659522583e-12,
      "speaker_id": 6
    },
    {
      "id": 11,
      "seek": 9000,
      "start": 95.38,
      "end": 96.76,
      "text": " Please note the note well.",
      "tokens": [
        50634,
        2555,
        3637,
        264,
        3637,
        731,
        13,
        50703
      ],
      "temperature": 0,
      "avg_logprob": -0.2199476288586128,
      "compression_ratio": 1.51,
      "no_speech_prob": 1.304196659522583e-12,
      "speaker_id": 6
    },
    {
      "id": 12,
      "seek": 9000,
      "start": 97.62,
      "end": 103.04,
      "text": " We all, in our participation here, have certain obligations as regards things like IPR and",
      "tokens": [
        50746,
        492,
        439,
        11,
        294,
        527,
        13487,
        510,
        11,
        362,
        1629,
        26234,
        382,
        14258,
        721,
        411,
        8671,
        49,
        293,
        51017
      ],
      "temperature": 0,
      "avg_logprob": -0.2199476288586128,
      "compression_ratio": 1.51,
      "no_speech_prob": 1.304196659522583e-12,
      "speaker_id": 6
    },
    {
      "id": 13,
      "seek": 9000,
      "start": 103.04,
      "end": 104,
      "text": " our code of conduct.",
      "tokens": [
        51017,
        527,
        3089,
        295,
        6018,
        13,
        51065
      ],
      "temperature": 0,
      "avg_logprob": -0.2199476288586128,
      "compression_ratio": 1.51,
      "no_speech_prob": 1.304196659522583e-12,
      "speaker_id": 6
    },
    {
      "id": 14,
      "seek": 9000,
      "start": 104.1,
      "end": 105.16,
      "text": " We take those very seriously.",
      "tokens": [
        51070,
        492,
        747,
        729,
        588,
        6638,
        13,
        51123
      ],
      "temperature": 0,
      "avg_logprob": -0.2199476288586128,
      "compression_ratio": 1.51,
      "no_speech_prob": 1.304196659522583e-12,
      "speaker_id": 6
    },
    {
      "id": 15,
      "seek": 9000,
      "start": 105.42,
      "end": 109.42,
      "text": " And please know your obligations and abide by them.",
      "tokens": [
        51136,
        400,
        1767,
        458,
        428,
        26234,
        293,
        39663,
        538,
        552,
        13,
        51336
      ],
      "temperature": 0,
      "avg_logprob": -0.2199476288586128,
      "compression_ratio": 1.51,
      "no_speech_prob": 1.304196659522583e-12,
      "speaker_id": 6
    },
    {
      "id": 16,
      "seek": 10942,
      "start": 109.42,
      "end": 119.14,
      "text": " In case this is the first session at IETF you are participating in, you missed a lot of good",
      "tokens": [
        50365,
        682,
        1389,
        341,
        307,
        264,
        700,
        5481,
        412,
        286,
        4850,
        37,
        291,
        366,
        13950,
        294,
        11,
        291,
        6721,
        257,
        688,
        295,
        665,
        50851
      ],
      "temperature": 0,
      "avg_logprob": -0.20977948506673177,
      "compression_ratio": 1.125,
      "no_speech_prob": 1.6463157876886458e-12,
      "speaker_id": 6
    },
    {
      "id": 17,
      "seek": 10942,
      "start": 119.14,
      "end": 119.58,
      "text": " stuff.",
      "tokens": [
        50851,
        1507,
        13,
        50873
      ],
      "temperature": 0,
      "avg_logprob": -0.20977948506673177,
      "compression_ratio": 1.125,
      "no_speech_prob": 1.6463157876886458e-12,
      "speaker_id": 6
    },
    {
      "id": 18,
      "seek": 11958,
      "start": 119.58,
      "end": 125.25999999999999,
      "text": " But there's still some tips to pay attention to.",
      "tokens": [
        50365,
        583,
        456,
        311,
        920,
        512,
        6082,
        281,
        1689,
        3202,
        281,
        13,
        50649
      ],
      "temperature": 0,
      "avg_logprob": -0.2186199218507797,
      "compression_ratio": 1.3532934131736527,
      "no_speech_prob": 1.9035612929679813e-12,
      "speaker_id": 6
    },
    {
      "id": 19,
      "seek": 11958,
      "start": 125.74,
      "end": 129.54,
      "text": " Please keep your mic off if you're remote, unless you're going to be speaking.",
      "tokens": [
        50673,
        2555,
        1066,
        428,
        3123,
        766,
        498,
        291,
        434,
        8607,
        11,
        5969,
        291,
        434,
        516,
        281,
        312,
        4124,
        13,
        50863
      ],
      "temperature": 0,
      "avg_logprob": -0.2186199218507797,
      "compression_ratio": 1.3532934131736527,
      "no_speech_prob": 1.9035612929679813e-12,
      "speaker_id": 6
    },
    {
      "id": 20,
      "seek": 11958,
      "start": 129.84,
      "end": 131.34,
      "text": " And headsets are obviously strongly recommended.",
      "tokens": [
        50878,
        400,
        8050,
        1385,
        366,
        2745,
        10613,
        9628,
        13,
        50953
      ],
      "temperature": 0,
      "avg_logprob": -0.2186199218507797,
      "compression_ratio": 1.3532934131736527,
      "no_speech_prob": 1.9035612929679813e-12,
      "speaker_id": 6
    },
    {
      "id": 21,
      "seek": 11958,
      "start": 131.8,
      "end": 133.57999999999998,
      "text": " On-site folks, please check in.",
      "tokens": [
        50976,
        1282,
        12,
        30417,
        4024,
        11,
        1767,
        1520,
        294,
        13,
        51065
      ],
      "temperature": 0,
      "avg_logprob": -0.2186199218507797,
      "compression_ratio": 1.3532934131736527,
      "no_speech_prob": 1.9035612929679813e-12,
      "speaker_id": 6
    },
    {
      "id": 22,
      "seek": 11958,
      "start": 133.72,
      "end": 134.88,
      "text": " Scan the QR code.",
      "tokens": [
        51072,
        41177,
        264,
        32784,
        3089,
        13,
        51130
      ],
      "temperature": 0,
      "avg_logprob": -0.2186199218507797,
      "compression_ratio": 1.3532934131736527,
      "no_speech_prob": 1.9035612929679813e-12,
      "speaker_id": 6
    },
    {
      "id": 23,
      "seek": 13488,
      "start": 134.88,
      "end": 140.46,
      "text": " It's not as good as Foursquare, but it will get us a better room and ensure that we get",
      "tokens": [
        50365,
        467,
        311,
        406,
        382,
        665,
        382,
        479,
        5067,
        358,
        543,
        11,
        457,
        309,
        486,
        483,
        505,
        257,
        1101,
        1808,
        293,
        5586,
        300,
        321,
        483,
        50644
      ],
      "temperature": 0,
      "avg_logprob": -0.15350168401544745,
      "compression_ratio": 1.3558282208588956,
      "no_speech_prob": 2.433345408850429e-12,
      "speaker_id": 6
    },
    {
      "id": 24,
      "seek": 13488,
      "start": 140.46,
      "end": 143.85999999999999,
      "text": " a similarly capacious room at the next IETF.",
      "tokens": [
        50644,
        257,
        14138,
        4637,
        851,
        1808,
        412,
        264,
        958,
        286,
        4850,
        37,
        13,
        50814
      ],
      "temperature": 0,
      "avg_logprob": -0.15350168401544745,
      "compression_ratio": 1.3558282208588956,
      "no_speech_prob": 2.433345408850429e-12,
      "speaker_id": 6
    },
    {
      "id": 25,
      "seek": 13488,
      "start": 144.1,
      "end": 145.88,
      "text": " So, much appreciated to check in there.",
      "tokens": [
        50826,
        407,
        11,
        709,
        17169,
        281,
        1520,
        294,
        456,
        13,
        50915
      ],
      "temperature": 0,
      "avg_logprob": -0.15350168401544745,
      "compression_ratio": 1.3558282208588956,
      "no_speech_prob": 2.433345408850429e-12,
      "speaker_id": 6
    },
    {
      "id": 26,
      "seek": 13488,
      "start": 145.96,
      "end": 147.78,
      "text": " And we'll use that for queue management as well.",
      "tokens": [
        50919,
        400,
        321,
        603,
        764,
        300,
        337,
        18639,
        4592,
        382,
        731,
        13,
        51010
      ],
      "temperature": 0,
      "avg_logprob": -0.15350168401544745,
      "compression_ratio": 1.3558282208588956,
      "no_speech_prob": 2.433345408850429e-12,
      "speaker_id": 6
    },
    {
      "id": 27,
      "seek": 14778,
      "start": 147.78,
      "end": 151.98,
      "text": " All right, here's our agenda.",
      "tokens": [
        50365,
        1057,
        558,
        11,
        510,
        311,
        527,
        9829,
        13,
        50575
      ],
      "temperature": 0,
      "avg_logprob": -0.2493615746498108,
      "compression_ratio": 1.5238095238095237,
      "no_speech_prob": 1.893376080919218e-12,
      "speaker_id": 6
    },
    {
      "id": 28,
      "seek": 14778,
      "start": 152.6,
      "end": 154.88,
      "text": " We have already welcomed you.",
      "tokens": [
        50606,
        492,
        362,
        1217,
        23668,
        291,
        13,
        50720
      ],
      "temperature": 0,
      "avg_logprob": -0.2493615746498108,
      "compression_ratio": 1.5238095238095237,
      "no_speech_prob": 1.893376080919218e-12,
      "speaker_id": 6
    },
    {
      "id": 29,
      "seek": 14778,
      "start": 156.06,
      "end": 158.46,
      "text": " We will need a note taker for this session.",
      "tokens": [
        50779,
        492,
        486,
        643,
        257,
        3637,
        991,
        260,
        337,
        341,
        5481,
        13,
        50899
      ],
      "temperature": 0,
      "avg_logprob": -0.2493615746498108,
      "compression_ratio": 1.5238095238095237,
      "no_speech_prob": 1.893376080919218e-12,
      "speaker_id": 6
    },
    {
      "id": 30,
      "seek": 14778,
      "start": 158.98,
      "end": 161.1,
      "text": " Could I have a volunteer for that, please?",
      "tokens": [
        50925,
        7497,
        286,
        362,
        257,
        13835,
        337,
        300,
        11,
        1767,
        30,
        51031
      ],
      "temperature": 0,
      "avg_logprob": -0.2493615746498108,
      "compression_ratio": 1.5238095238095237,
      "no_speech_prob": 1.893376080919218e-12,
      "speaker_id": 6
    },
    {
      "id": 31,
      "seek": 14778,
      "start": 163.3,
      "end": 164.4,
      "text": " Thank you, Ben Schwartz.",
      "tokens": [
        51141,
        1044,
        291,
        11,
        3964,
        17576,
        45929,
        13,
        51196
      ],
      "temperature": 0,
      "avg_logprob": -0.2493615746498108,
      "compression_ratio": 1.5238095238095237,
      "no_speech_prob": 1.893376080919218e-12,
      "speaker_id": 6
    },
    {
      "id": 32,
      "seek": 14778,
      "start": 166.94,
      "end": 169.4,
      "text": " Blue sheets are no longer blue.",
      "tokens": [
        51323,
        8510,
        15421,
        366,
        572,
        2854,
        3344,
        13,
        51446
      ],
      "temperature": 0,
      "avg_logprob": -0.2493615746498108,
      "compression_ratio": 1.5238095238095237,
      "no_speech_prob": 1.893376080919218e-12,
      "speaker_id": 6
    },
    {
      "id": 33,
      "seek": 14778,
      "start": 169.48,
      "end": 170.9,
      "text": " We were just using the QR code.",
      "tokens": [
        51450,
        492,
        645,
        445,
        1228,
        264,
        32784,
        3089,
        13,
        51521
      ],
      "temperature": 0,
      "avg_logprob": -0.2493615746498108,
      "compression_ratio": 1.5238095238095237,
      "no_speech_prob": 1.893376080919218e-12,
      "speaker_id": 6
    },
    {
      "id": 34,
      "seek": 14778,
      "start": 171.32,
      "end": 172.52,
      "text": " And we already covered the note well.",
      "tokens": [
        51542,
        400,
        321,
        1217,
        5343,
        264,
        3637,
        731,
        13,
        51602
      ],
      "temperature": 0,
      "avg_logprob": -0.2493615746498108,
      "compression_ratio": 1.5238095238095237,
      "no_speech_prob": 1.893376080919218e-12,
      "speaker_id": 6
    },
    {
      "id": 35,
      "seek": 14778,
      "start": 172.74,
      "end": 175.36,
      "text": " So, thank you to Ben for covering that aspect.",
      "tokens": [
        51613,
        407,
        11,
        1309,
        291,
        281,
        3964,
        337,
        10322,
        300,
        4171,
        13,
        51744
      ],
      "temperature": 0,
      "avg_logprob": -0.2493615746498108,
      "compression_ratio": 1.5238095238095237,
      "no_speech_prob": 1.893376080919218e-12,
      "speaker_id": 6
    },
    {
      "id": 36,
      "seek": 17536,
      "start": 175.36,
      "end": 181.36,
      "text": " All right, I'm going to pass it over to Siobhan to do an update on the working group drafts",
      "tokens": [
        50365,
        1057,
        558,
        11,
        286,
        478,
        516,
        281,
        1320,
        309,
        670,
        281,
        4909,
        996,
        3451,
        281,
        360,
        364,
        5623,
        322,
        264,
        1364,
        1594,
        11206,
        82,
        50665
      ],
      "temperature": 0,
      "avg_logprob": -0.17650277526290328,
      "compression_ratio": 1.589430894308943,
      "no_speech_prob": 2.0007962300838678e-12,
      "speaker_id": 6
    },
    {
      "id": 37,
      "seek": 17536,
      "start": 181.36,
      "end": 182.82000000000002,
      "text": " and a potential working group draft.",
      "tokens": [
        50665,
        293,
        257,
        3995,
        1364,
        1594,
        11206,
        13,
        50738
      ],
      "temperature": 0,
      "avg_logprob": -0.17650277526290328,
      "compression_ratio": 1.589430894308943,
      "no_speech_prob": 2.0007962300838678e-12,
      "speaker_id": 6
    },
    {
      "id": 38,
      "seek": 17536,
      "start": 183,
      "end": 189.68,
      "text": " And then the main content we have today for new stuff is a new proposal from Tommy on doing",
      "tokens": [
        50747,
        400,
        550,
        264,
        2135,
        2701,
        321,
        362,
        965,
        337,
        777,
        1507,
        307,
        257,
        777,
        11494,
        490,
        19448,
        322,
        884,
        51081
      ],
      "temperature": 0,
      "avg_logprob": -0.17650277526290328,
      "compression_ratio": 1.589430894308943,
      "no_speech_prob": 2.0007962300838678e-12,
      "speaker_id": 6
    },
    {
      "id": 39,
      "seek": 17536,
      "start": 189.68,
      "end": 191.18,
      "text": " OHGTP, but streaming.",
      "tokens": [
        51081,
        13931,
        38,
        16804,
        11,
        457,
        11791,
        13,
        51156
      ],
      "temperature": 0,
      "avg_logprob": -0.17650277526290328,
      "compression_ratio": 1.589430894308943,
      "no_speech_prob": 2.0007962300838678e-12,
      "speaker_id": 6
    },
    {
      "id": 40,
      "seek": 17536,
      "start": 192.3,
      "end": 193.98000000000002,
      "text": " That is our proposed agenda.",
      "tokens": [
        51212,
        663,
        307,
        527,
        10348,
        9829,
        13,
        51296
      ],
      "temperature": 0,
      "avg_logprob": -0.17650277526290328,
      "compression_ratio": 1.589430894308943,
      "no_speech_prob": 2.0007962300838678e-12,
      "speaker_id": 6
    },
    {
      "id": 41,
      "seek": 17536,
      "start": 194.88000000000002,
      "end": 200.06,
      "text": " Do we have any modifications, bashes to that agenda before we launch into it?",
      "tokens": [
        51341,
        1144,
        321,
        362,
        604,
        26881,
        11,
        987,
        8076,
        281,
        300,
        9829,
        949,
        321,
        4025,
        666,
        309,
        30,
        51600
      ],
      "temperature": 0,
      "avg_logprob": -0.17650277526290328,
      "compression_ratio": 1.589430894308943,
      "no_speech_prob": 2.0007962300838678e-12,
      "speaker_id": 6
    },
    {
      "id": 42,
      "seek": 17536,
      "start": 204.12,
      "end": 205.34,
      "text": " All right, Siobhan, would you like to go?",
      "tokens": [
        51803,
        1057,
        558,
        11,
        4909,
        996,
        3451,
        11,
        576,
        291,
        411,
        281,
        352,
        30,
        51864
      ],
      "temperature": 0,
      "avg_logprob": -0.17650277526290328,
      "compression_ratio": 1.589430894308943,
      "no_speech_prob": 2.0007962300838678e-12,
      "speaker_id": 6
    },
    {
      "id": 43,
      "seek": 20536,
      "start": 205.36,
      "end": 206.44000000000003,
      "text": " It's a review of the status.",
      "tokens": [
        50365,
        467,
        311,
        257,
        3131,
        295,
        264,
        6558,
        13,
        50419
      ],
      "temperature": 0,
      "avg_logprob": -0.1919517015155993,
      "compression_ratio": 1.556390977443609,
      "no_speech_prob": 2.0838554406349408e-12,
      "speaker_id": 6
    },
    {
      "id": 44,
      "seek": 20536,
      "start": 207.56,
      "end": 208.18,
      "text": " Thanks, Richard.",
      "tokens": [
        50475,
        2561,
        11,
        9809,
        13,
        50506
      ],
      "temperature": 0,
      "avg_logprob": -0.1919517015155993,
      "compression_ratio": 1.556390977443609,
      "no_speech_prob": 2.0838554406349408e-12,
      "speaker_id": 7
    },
    {
      "id": 45,
      "seek": 20536,
      "start": 208.52,
      "end": 209.20000000000002,
      "text": " Welcome all.",
      "tokens": [
        50523,
        4027,
        439,
        13,
        50557
      ],
      "temperature": 0,
      "avg_logprob": -0.1919517015155993,
      "compression_ratio": 1.556390977443609,
      "no_speech_prob": 2.0838554406349408e-12,
      "speaker_id": 7
    },
    {
      "id": 46,
      "seek": 20536,
      "start": 210.14000000000001,
      "end": 216.52,
      "text": " Yeah, so our main protocol draft was in the RFC editor queue, but there was a late-breaking",
      "tokens": [
        50604,
        865,
        11,
        370,
        527,
        2135,
        10336,
        11206,
        390,
        294,
        264,
        497,
        18671,
        9839,
        18639,
        11,
        457,
        456,
        390,
        257,
        3469,
        12,
        20602,
        50923
      ],
      "temperature": 0,
      "avg_logprob": -0.1919517015155993,
      "compression_ratio": 1.556390977443609,
      "no_speech_prob": 2.0838554406349408e-12,
      "speaker_id": 7
    },
    {
      "id": 47,
      "seek": 20536,
      "start": 216.52,
      "end": 218.32000000000002,
      "text": " change that the authors found.",
      "tokens": [
        50923,
        1319,
        300,
        264,
        16552,
        1352,
        13,
        51013
      ],
      "temperature": 0,
      "avg_logprob": -0.1919517015155993,
      "compression_ratio": 1.556390977443609,
      "no_speech_prob": 2.0838554406349408e-12,
      "speaker_id": 7
    },
    {
      "id": 48,
      "seek": 20536,
      "start": 218.94000000000003,
      "end": 225.10000000000002,
      "text": " So, just to make sure that we are crossing our T's and dotting our I's, we brought it back",
      "tokens": [
        51044,
        407,
        11,
        445,
        281,
        652,
        988,
        300,
        321,
        366,
        14712,
        527,
        314,
        311,
        293,
        5893,
        783,
        527,
        286,
        311,
        11,
        321,
        3038,
        309,
        646,
        51352
      ],
      "temperature": 0,
      "avg_logprob": -0.1919517015155993,
      "compression_ratio": 1.556390977443609,
      "no_speech_prob": 2.0838554406349408e-12,
      "speaker_id": 7
    },
    {
      "id": 49,
      "seek": 20536,
      "start": 225.10000000000002,
      "end": 227.58,
      "text": " for a second working group last call that passed.",
      "tokens": [
        51352,
        337,
        257,
        1150,
        1364,
        1594,
        1036,
        818,
        300,
        4678,
        13,
        51476
      ],
      "temperature": 0,
      "avg_logprob": -0.1919517015155993,
      "compression_ratio": 1.556390977443609,
      "no_speech_prob": 2.0838554406349408e-12,
      "speaker_id": 7
    },
    {
      "id": 50,
      "seek": 20536,
      "start": 227.86,
      "end": 234.32000000000002,
      "text": " We got some plus ones on a couple of different email threads and some discussion on the PR.",
      "tokens": [
        51490,
        492,
        658,
        512,
        1804,
        2306,
        322,
        257,
        1916,
        295,
        819,
        3796,
        19314,
        293,
        512,
        5017,
        322,
        264,
        11568,
        13,
        51813
      ],
      "temperature": 0,
      "avg_logprob": -0.1919517015155993,
      "compression_ratio": 1.556390977443609,
      "no_speech_prob": 2.0838554406349408e-12,
      "speaker_id": 7
    },
    {
      "id": 51,
      "seek": 23432,
      "start": 234.32,
      "end": 239.06,
      "text": " So, we merged that in and published a new version and sent it on again.",
      "tokens": [
        50365,
        407,
        11,
        321,
        36427,
        300,
        294,
        293,
        6572,
        257,
        777,
        3037,
        293,
        2279,
        309,
        322,
        797,
        13,
        50602
      ],
      "temperature": 0,
      "avg_logprob": -0.2457716464996338,
      "compression_ratio": 1.4,
      "no_speech_prob": 2.2380975111396806e-12,
      "speaker_id": 7
    },
    {
      "id": 52,
      "seek": 23432,
      "start": 240.38,
      "end": 241.79999999999998,
      "text": " Second time's the charm, I think.",
      "tokens": [
        50668,
        5736,
        565,
        311,
        264,
        18904,
        11,
        286,
        519,
        13,
        50739
      ],
      "temperature": 0,
      "avg_logprob": -0.2457716464996338,
      "compression_ratio": 1.4,
      "no_speech_prob": 2.2380975111396806e-12,
      "speaker_id": 7
    },
    {
      "id": 53,
      "seek": 23432,
      "start": 242.42,
      "end": 242.72,
      "text": " I hope.",
      "tokens": [
        50770,
        286,
        1454,
        13,
        50785
      ],
      "temperature": 0,
      "avg_logprob": -0.2457716464996338,
      "compression_ratio": 1.4,
      "no_speech_prob": 2.2380975111396806e-12,
      "speaker_id": 7
    },
    {
      "id": 54,
      "seek": 23432,
      "start": 244.12,
      "end": 244.48,
      "text": " And then, yeah.",
      "tokens": [
        50855,
        400,
        550,
        11,
        1338,
        13,
        50873
      ],
      "temperature": 0,
      "avg_logprob": -0.2457716464996338,
      "compression_ratio": 1.4,
      "no_speech_prob": 2.2380975111396806e-12,
      "speaker_id": 7
    },
    {
      "id": 55,
      "seek": 23432,
      "start": 244.54,
      "end": 250.35999999999999,
      "text": " And then for this, for the SVCB config draft, we submitted a publication.",
      "tokens": [
        50876,
        400,
        550,
        337,
        341,
        11,
        337,
        264,
        31910,
        34,
        33,
        6662,
        11206,
        11,
        321,
        14405,
        257,
        19953,
        13,
        51167
      ],
      "temperature": 0,
      "avg_logprob": -0.2457716464996338,
      "compression_ratio": 1.4,
      "no_speech_prob": 2.2380975111396806e-12,
      "speaker_id": 7
    },
    {
      "id": 56,
      "seek": 25036,
      "start": 250.36,
      "end": 254.48000000000002,
      "text": " So, both this one and the main protocol draft are being handled by Murray.",
      "tokens": [
        50365,
        407,
        11,
        1293,
        341,
        472,
        293,
        264,
        2135,
        10336,
        11206,
        366,
        885,
        18033,
        538,
        27291,
        13,
        50571
      ],
      "temperature": 0,
      "avg_logprob": -0.19248199462890625,
      "compression_ratio": 1.3310344827586207,
      "no_speech_prob": 4.085724380348266e-12,
      "speaker_id": 7
    },
    {
      "id": 57,
      "seek": 25036,
      "start": 254.84,
      "end": 256.38,
      "text": " So, thanks to our AD.",
      "tokens": [
        50589,
        407,
        11,
        3231,
        281,
        527,
        9135,
        13,
        50666
      ],
      "temperature": 0,
      "avg_logprob": -0.19248199462890625,
      "compression_ratio": 1.3310344827586207,
      "no_speech_prob": 4.085724380348266e-12,
      "speaker_id": 7
    },
    {
      "id": 58,
      "seek": 25036,
      "start": 257.72,
      "end": 261.26,
      "text": " Yeah, and then there's the third draft, which is an individual draft.",
      "tokens": [
        50733,
        865,
        11,
        293,
        550,
        456,
        311,
        264,
        2636,
        11206,
        11,
        597,
        307,
        364,
        2609,
        11206,
        13,
        50910
      ],
      "temperature": 0,
      "avg_logprob": -0.19248199462890625,
      "compression_ratio": 1.3310344827586207,
      "no_speech_prob": 4.085724380348266e-12,
      "speaker_id": 7
    },
    {
      "id": 59,
      "seek": 25036,
      "start": 262.42,
      "end": 264.68,
      "text": " The feedback to proxy one.",
      "tokens": [
        50968,
        440,
        5824,
        281,
        29690,
        472,
        13,
        51081
      ],
      "temperature": 0,
      "avg_logprob": -0.19248199462890625,
      "compression_ratio": 1.3310344827586207,
      "no_speech_prob": 4.085724380348266e-12,
      "speaker_id": 7
    },
    {
      "id": 60,
      "seek": 26468,
      "start": 264.68,
      "end": 268.34000000000003,
      "text": " And we talked about that at the previous meeting.",
      "tokens": [
        50365,
        400,
        321,
        2825,
        466,
        300,
        412,
        264,
        3894,
        3440,
        13,
        50548
      ],
      "temperature": 0,
      "avg_logprob": -0.15690263436765087,
      "compression_ratio": 1.3973509933774835,
      "no_speech_prob": 3.306214026513321e-12,
      "speaker_id": 7
    },
    {
      "id": 61,
      "seek": 26468,
      "start": 269.44,
      "end": 274.08,
      "text": " Folks generally seemed satisfied with the content of the draft.",
      "tokens": [
        50603,
        39275,
        5101,
        6576,
        11239,
        365,
        264,
        2701,
        295,
        264,
        11206,
        13,
        50835
      ],
      "temperature": 0,
      "avg_logprob": -0.15690263436765087,
      "compression_ratio": 1.3973509933774835,
      "no_speech_prob": 3.306214026513321e-12,
      "speaker_id": 7
    },
    {
      "id": 62,
      "seek": 26468,
      "start": 274.2,
      "end": 281.5,
      "text": " There was some apprehension about whether there's enough interest to justify taking that item",
      "tokens": [
        50841,
        821,
        390,
        512,
        38675,
        3378,
        466,
        1968,
        456,
        311,
        1547,
        1179,
        281,
        20833,
        1940,
        300,
        3174,
        51206
      ],
      "temperature": 0,
      "avg_logprob": -0.15690263436765087,
      "compression_ratio": 1.3973509933774835,
      "no_speech_prob": 3.306214026513321e-12,
      "speaker_id": 7
    },
    {
      "id": 63,
      "seek": 26468,
      "start": 281.5,
      "end": 281.74,
      "text": " on.",
      "tokens": [
        51206,
        322,
        13,
        51218
      ],
      "temperature": 0,
      "avg_logprob": -0.15690263436765087,
      "compression_ratio": 1.3973509933774835,
      "no_speech_prob": 3.306214026513321e-12,
      "speaker_id": 7
    },
    {
      "id": 64,
      "seek": 28174,
      "start": 281.74,
      "end": 289.72,
      "text": " So, we have, the chairs have a task to do a call for adoption to solicit that interest.",
      "tokens": [
        50365,
        407,
        11,
        321,
        362,
        11,
        264,
        18299,
        362,
        257,
        5633,
        281,
        360,
        257,
        818,
        337,
        19215,
        281,
        23665,
        270,
        300,
        1179,
        13,
        50764
      ],
      "temperature": 0,
      "avg_logprob": -0.1620915034045912,
      "compression_ratio": 1.502824858757062,
      "no_speech_prob": 3.0881252416348204e-12,
      "speaker_id": 7
    },
    {
      "id": 65,
      "seek": 28174,
      "start": 290.04,
      "end": 293.76,
      "text": " And we're looking for strong, like, interest and folks speaking up.",
      "tokens": [
        50780,
        400,
        321,
        434,
        1237,
        337,
        2068,
        11,
        411,
        11,
        1179,
        293,
        4024,
        4124,
        493,
        13,
        50966
      ],
      "temperature": 0,
      "avg_logprob": -0.1620915034045912,
      "compression_ratio": 1.502824858757062,
      "no_speech_prob": 3.0881252416348204e-12,
      "speaker_id": 7
    },
    {
      "id": 66,
      "seek": 28174,
      "start": 294.5,
      "end": 296.76,
      "text": " Does that plan still sound okay to folks?",
      "tokens": [
        51003,
        4402,
        300,
        1393,
        920,
        1626,
        1392,
        281,
        4024,
        30,
        51116
      ],
      "temperature": 0,
      "avg_logprob": -0.1620915034045912,
      "compression_ratio": 1.502824858757062,
      "no_speech_prob": 3.0881252416348204e-12,
      "speaker_id": 7
    },
    {
      "id": 67,
      "seek": 28174,
      "start": 296.88,
      "end": 300.64,
      "text": " Or if you don't agree with that plan of action, please speak up now.",
      "tokens": [
        51122,
        1610,
        498,
        291,
        500,
        380,
        3986,
        365,
        300,
        1393,
        295,
        3069,
        11,
        1767,
        1710,
        493,
        586,
        13,
        51310
      ],
      "temperature": 0,
      "avg_logprob": -0.1620915034045912,
      "compression_ratio": 1.502824858757062,
      "no_speech_prob": 3.0881252416348204e-12,
      "speaker_id": 7
    },
    {
      "id": 68,
      "seek": 30064,
      "start": 300.64,
      "end": 304.09999999999997,
      "text": " All right.",
      "tokens": [
        50365,
        1057,
        558,
        13,
        50538
      ],
      "temperature": 0,
      "avg_logprob": -0.528089481851329,
      "compression_ratio": 1.1855670103092784,
      "no_speech_prob": 4.5445353209772055e-12,
      "speaker_id": 7
    },
    {
      "id": 69,
      "seek": 30064,
      "start": 304.38,
      "end": 307,
      "text": " So, we'll be doing that call for adoption very soon.",
      "tokens": [
        50552,
        407,
        11,
        321,
        603,
        312,
        884,
        300,
        818,
        337,
        19215,
        588,
        2321,
        13,
        50683
      ],
      "temperature": 0,
      "avg_logprob": -0.528089481851329,
      "compression_ratio": 1.1855670103092784,
      "no_speech_prob": 4.5445353209772055e-12,
      "speaker_id": 7
    },
    {
      "id": 70,
      "seek": 30064,
      "start": 309.18,
      "end": 309.65999999999997,
      "text": " Yeah.",
      "tokens": [
        50792,
        865,
        13,
        50816
      ],
      "temperature": 0,
      "avg_logprob": -0.528089481851329,
      "compression_ratio": 1.1855670103092784,
      "no_speech_prob": 4.5445353209772055e-12,
      "speaker_id": 7
    },
    {
      "id": 71,
      "seek": 30064,
      "start": 309.86,
      "end": 310.91999999999996,
      "text": " I think that's everything.",
      "tokens": [
        50826,
        286,
        519,
        300,
        311,
        1203,
        13,
        50879
      ],
      "temperature": 0,
      "avg_logprob": -0.528089481851329,
      "compression_ratio": 1.1855670103092784,
      "no_speech_prob": 4.5445353209772055e-12,
      "speaker_id": 7
    },
    {
      "id": 72,
      "seek": 30064,
      "start": 311.76,
      "end": 312.2,
      "text": " Tommy.",
      "tokens": [
        50921,
        19448,
        13,
        50943
      ],
      "temperature": 0,
      "avg_logprob": -0.528089481851329,
      "compression_ratio": 1.1855670103092784,
      "no_speech_prob": 4.5445353209772055e-12,
      "speaker_id": 7
    },
    {
      "id": 73,
      "seek": 30064,
      "start": 323.71999999999997,
      "end": 324.15999999999997,
      "text": " Yeah.",
      "tokens": [
        51519,
        865,
        13,
        51541
      ],
      "temperature": 0,
      "avg_logprob": -0.528089481851329,
      "compression_ratio": 1.1855670103092784,
      "no_speech_prob": 4.5445353209772055e-12
    },
    {
      "id": 74,
      "seek": 30064,
      "start": 324.15999999999997,
      "end": 324.2,
      "text": " Yeah.",
      "tokens": [
        51541,
        865,
        13,
        51543
      ],
      "temperature": 0,
      "avg_logprob": -0.528089481851329,
      "compression_ratio": 1.1855670103092784,
      "no_speech_prob": 4.5445353209772055e-12
    },
    {
      "id": 75,
      "seek": 32420,
      "start": 324.2,
      "end": 347.84,
      "text": " All right.",
      "tokens": [
        50365,
        1057,
        558,
        13,
        51547
      ],
      "temperature": 0,
      "avg_logprob": -0.6118300878084623,
      "compression_ratio": 0.7777777777777778,
      "no_speech_prob": 1.6893701064793487e-12,
      "speaker_id": 1
    },
    {
      "id": 76,
      "seek": 32420,
      "start": 349.34,
      "end": 350.32,
      "text": " Hello, everybody.",
      "tokens": [
        51622,
        2425,
        11,
        2201,
        13,
        51671
      ],
      "temperature": 0,
      "avg_logprob": -0.6118300878084623,
      "compression_ratio": 0.7777777777777778,
      "no_speech_prob": 1.6893701064793487e-12,
      "speaker_id": 1
    },
    {
      "id": 77,
      "seek": 35032,
      "start": 350.32,
      "end": 354.78,
      "text": " I hope you've had a good IETF week.",
      "tokens": [
        50365,
        286,
        1454,
        291,
        600,
        632,
        257,
        665,
        286,
        4850,
        37,
        1243,
        13,
        50588
      ],
      "temperature": 0,
      "avg_logprob": -0.23072167660327667,
      "compression_ratio": 1.4210526315789473,
      "no_speech_prob": 2.1294296621149345e-12,
      "speaker_id": 1
    },
    {
      "id": 78,
      "seek": 35032,
      "start": 355.12,
      "end": 356.28,
      "text": " We're almost done.",
      "tokens": [
        50605,
        492,
        434,
        1920,
        1096,
        13,
        50663
      ],
      "temperature": 0,
      "avg_logprob": -0.23072167660327667,
      "compression_ratio": 1.4210526315789473,
      "no_speech_prob": 2.1294296621149345e-12,
      "speaker_id": 1
    },
    {
      "id": 79,
      "seek": 35032,
      "start": 357.5,
      "end": 358.84,
      "text": " So, I'm Tommy Pauley from Apple.",
      "tokens": [
        50724,
        407,
        11,
        286,
        478,
        19448,
        430,
        1459,
        3420,
        490,
        6373,
        13,
        50791
      ],
      "temperature": 0,
      "avg_logprob": -0.23072167660327667,
      "compression_ratio": 1.4210526315789473,
      "no_speech_prob": 2.1294296621149345e-12,
      "speaker_id": 1
    },
    {
      "id": 80,
      "seek": 35032,
      "start": 359.24,
      "end": 367,
      "text": " And this is a discussion on adding streaming capabilities to ObliviousHTP.",
      "tokens": [
        50811,
        400,
        341,
        307,
        257,
        5017,
        322,
        5127,
        11791,
        10862,
        281,
        4075,
        45997,
        851,
        39,
        16804,
        13,
        51199
      ],
      "temperature": 0,
      "avg_logprob": -0.23072167660327667,
      "compression_ratio": 1.4210526315789473,
      "no_speech_prob": 2.1294296621149345e-12,
      "speaker_id": 1
    },
    {
      "id": 81,
      "seek": 35032,
      "start": 367.65999999999997,
      "end": 372.4,
      "text": " I apologize for not having had a draft out before the deadlines.",
      "tokens": [
        51232,
        286,
        12328,
        337,
        406,
        1419,
        632,
        257,
        11206,
        484,
        949,
        264,
        37548,
        13,
        51469
      ],
      "temperature": 0,
      "avg_logprob": -0.23072167660327667,
      "compression_ratio": 1.4210526315789473,
      "no_speech_prob": 2.1294296621149345e-12,
      "speaker_id": 1
    },
    {
      "id": 82,
      "seek": 35032,
      "start": 372.62,
      "end": 375.64,
      "text": " But I expect to have something written up soon.",
      "tokens": [
        51480,
        583,
        286,
        2066,
        281,
        362,
        746,
        3720,
        493,
        2321,
        13,
        51631
      ],
      "temperature": 0,
      "avg_logprob": -0.23072167660327667,
      "compression_ratio": 1.4210526315789473,
      "no_speech_prob": 2.1294296621149345e-12,
      "speaker_id": 1
    },
    {
      "id": 83,
      "seek": 35032,
      "start": 376.38,
      "end": 378.3,
      "text": " The purpose here is to talk about the use cases.",
      "tokens": [
        51668,
        440,
        4334,
        510,
        307,
        281,
        751,
        466,
        264,
        764,
        3331,
        13,
        51764
      ],
      "temperature": 0,
      "avg_logprob": -0.23072167660327667,
      "compression_ratio": 1.4210526315789473,
      "no_speech_prob": 2.1294296621149345e-12,
      "speaker_id": 1
    },
    {
      "id": 84,
      "seek": 37830,
      "start": 378.3,
      "end": 380.74,
      "text": " See how we feel about that.",
      "tokens": [
        50365,
        3008,
        577,
        321,
        841,
        466,
        300,
        13,
        50487
      ],
      "temperature": 0,
      "avg_logprob": -0.24633684020111526,
      "compression_ratio": 1.384180790960452,
      "no_speech_prob": 1.3909867181693958e-12,
      "speaker_id": 1
    },
    {
      "id": 85,
      "seek": 37830,
      "start": 381.02000000000004,
      "end": 386.78000000000003,
      "text": " And then go into the proposed details of what would be in this 0, 0 draft.",
      "tokens": [
        50501,
        400,
        550,
        352,
        666,
        264,
        10348,
        4365,
        295,
        437,
        576,
        312,
        294,
        341,
        1958,
        11,
        1958,
        11206,
        13,
        50789
      ],
      "temperature": 0,
      "avg_logprob": -0.24633684020111526,
      "compression_ratio": 1.384180790960452,
      "no_speech_prob": 1.3909867181693958e-12,
      "speaker_id": 1
    },
    {
      "id": 86,
      "seek": 37830,
      "start": 390.28000000000003,
      "end": 391.02000000000004,
      "text": " All right.",
      "tokens": [
        50964,
        1057,
        558,
        13,
        51001
      ],
      "temperature": 0,
      "avg_logprob": -0.24633684020111526,
      "compression_ratio": 1.384180790960452,
      "no_speech_prob": 1.3909867181693958e-12,
      "speaker_id": 1
    },
    {
      "id": 87,
      "seek": 37830,
      "start": 391.1,
      "end": 394.54,
      "text": " So, first to review what our status quo is for OETP.",
      "tokens": [
        51005,
        407,
        11,
        700,
        281,
        3131,
        437,
        527,
        6558,
        28425,
        307,
        337,
        422,
        4850,
        47,
        13,
        51177
      ],
      "temperature": 0,
      "avg_logprob": -0.24633684020111526,
      "compression_ratio": 1.384180790960452,
      "no_speech_prob": 1.3909867181693958e-12,
      "speaker_id": 1
    },
    {
      "id": 88,
      "seek": 37830,
      "start": 395.3,
      "end": 401.1,
      "text": " We currently have a single message in each direction for request and response.",
      "tokens": [
        51215,
        492,
        4362,
        362,
        257,
        2167,
        3636,
        294,
        1184,
        3513,
        337,
        5308,
        293,
        4134,
        13,
        51505
      ],
      "temperature": 0,
      "avg_logprob": -0.24633684020111526,
      "compression_ratio": 1.384180790960452,
      "no_speech_prob": 1.3909867181693958e-12,
      "speaker_id": 1
    },
    {
      "id": 89,
      "seek": 40110,
      "start": 401.1,
      "end": 406.5,
      "text": " That must be encrypted and decrypted in a single chunk each.",
      "tokens": [
        50365,
        663,
        1633,
        312,
        36663,
        293,
        979,
        627,
        25383,
        294,
        257,
        2167,
        16635,
        1184,
        13,
        50635
      ],
      "temperature": 0,
      "avg_logprob": -0.1657974950728878,
      "compression_ratio": 1.44,
      "no_speech_prob": 1.8394955697553783e-12,
      "speaker_id": 1
    },
    {
      "id": 90,
      "seek": 40110,
      "start": 407.78000000000003,
      "end": 413.26000000000005,
      "text": " And that fit very, very well for many of the use cases that we have.",
      "tokens": [
        50699,
        400,
        300,
        3318,
        588,
        11,
        588,
        731,
        337,
        867,
        295,
        264,
        764,
        3331,
        300,
        321,
        362,
        13,
        50973
      ],
      "temperature": 0,
      "avg_logprob": -0.1657974950728878,
      "compression_ratio": 1.44,
      "no_speech_prob": 1.8394955697553783e-12,
      "speaker_id": 1
    },
    {
      "id": 91,
      "seek": 40110,
      "start": 413.40000000000003,
      "end": 415.38,
      "text": " And it's still a very, very good thing.",
      "tokens": [
        50980,
        400,
        309,
        311,
        920,
        257,
        588,
        11,
        588,
        665,
        551,
        13,
        51079
      ],
      "temperature": 0,
      "avg_logprob": -0.1657974950728878,
      "compression_ratio": 1.44,
      "no_speech_prob": 1.8394955697553783e-12,
      "speaker_id": 1
    },
    {
      "id": 92,
      "seek": 40110,
      "start": 415.54,
      "end": 418.64000000000004,
      "text": " I'm glad we are publishing this document soon.",
      "tokens": [
        51087,
        286,
        478,
        5404,
        321,
        366,
        17832,
        341,
        4166,
        2321,
        13,
        51242
      ],
      "temperature": 0,
      "avg_logprob": -0.1657974950728878,
      "compression_ratio": 1.44,
      "no_speech_prob": 1.8394955697553783e-12,
      "speaker_id": 1
    },
    {
      "id": 93,
      "seek": 41864,
      "start": 418.64,
      "end": 423.32,
      "text": " So, things like DNS, et cetera, are small messages.",
      "tokens": [
        50365,
        407,
        11,
        721,
        411,
        35153,
        11,
        1030,
        11458,
        11,
        366,
        1359,
        7897,
        13,
        50599
      ],
      "temperature": 0,
      "avg_logprob": -0.12478365056654986,
      "compression_ratio": 1.52803738317757,
      "no_speech_prob": 1.706614884133817e-12,
      "speaker_id": 1
    },
    {
      "id": 94,
      "seek": 41864,
      "start": 424.14,
      "end": 432,
      "text": " And really, there's essentially nothing you can meaningfully do with the data in the request or response without having the complete thing.",
      "tokens": [
        50640,
        400,
        534,
        11,
        456,
        311,
        4476,
        1825,
        291,
        393,
        3620,
        2277,
        360,
        365,
        264,
        1412,
        294,
        264,
        5308,
        420,
        4134,
        1553,
        1419,
        264,
        3566,
        551,
        13,
        51033
      ],
      "temperature": 0,
      "avg_logprob": -0.12478365056654986,
      "compression_ratio": 1.52803738317757,
      "no_speech_prob": 1.706614884133817e-12,
      "speaker_id": 1
    },
    {
      "id": 95,
      "seek": 41864,
      "start": 432.08,
      "end": 436.15999999999997,
      "text": " You can't just be like, all right, I read the first half of your DNS query.",
      "tokens": [
        51037,
        509,
        393,
        380,
        445,
        312,
        411,
        11,
        439,
        558,
        11,
        286,
        1401,
        264,
        700,
        1922,
        295,
        428,
        35153,
        14581,
        13,
        51241
      ],
      "temperature": 0,
      "avg_logprob": -0.12478365056654986,
      "compression_ratio": 1.52803738317757,
      "no_speech_prob": 1.706614884133817e-12,
      "speaker_id": 1
    },
    {
      "id": 96,
      "seek": 41864,
      "start": 436.5,
      "end": 438.03999999999996,
      "text": " That's enough for me to start responding.",
      "tokens": [
        51258,
        663,
        311,
        1547,
        337,
        385,
        281,
        722,
        16670,
        13,
        51335
      ],
      "temperature": 0,
      "avg_logprob": -0.12478365056654986,
      "compression_ratio": 1.52803738317757,
      "no_speech_prob": 1.706614884133817e-12,
      "speaker_id": 1
    },
    {
      "id": 97,
      "seek": 41864,
      "start": 438.21999999999997,
      "end": 438.91999999999996,
      "text": " Like, not useful.",
      "tokens": [
        51344,
        1743,
        11,
        406,
        4420,
        13,
        51379
      ],
      "temperature": 0,
      "avg_logprob": -0.12478365056654986,
      "compression_ratio": 1.52803738317757,
      "no_speech_prob": 1.706614884133817e-12,
      "speaker_id": 1
    },
    {
      "id": 98,
      "seek": 43892,
      "start": 438.92,
      "end": 447.16,
      "text": " And OETP is communicating binary HTTP as its content by default.",
      "tokens": [
        50365,
        400,
        422,
        4850,
        47,
        307,
        17559,
        17434,
        33283,
        382,
        1080,
        2701,
        538,
        7576,
        13,
        50777
      ],
      "temperature": 0,
      "avg_logprob": -0.22374255006963556,
      "compression_ratio": 1.2826086956521738,
      "no_speech_prob": 1.6964253352763636e-12,
      "speaker_id": 1
    },
    {
      "id": 99,
      "seek": 43892,
      "start": 448.14000000000004,
      "end": 453.98,
      "text": " And very helpfully, binary HTTP does support effectively streaming.",
      "tokens": [
        50826,
        400,
        588,
        854,
        2277,
        11,
        17434,
        33283,
        775,
        1406,
        8659,
        11791,
        13,
        51118
      ],
      "temperature": 0,
      "avg_logprob": -0.22374255006963556,
      "compression_ratio": 1.2826086956521738,
      "no_speech_prob": 1.6964253352763636e-12,
      "speaker_id": 1
    },
    {
      "id": 100,
      "seek": 43892,
      "start": 454.1,
      "end": 455.92,
      "text": " It has this mode for indeterminate messages.",
      "tokens": [
        51124,
        467,
        575,
        341,
        4391,
        337,
        1016,
        35344,
        13923,
        7897,
        13,
        51215
      ],
      "temperature": 0,
      "avg_logprob": -0.22374255006963556,
      "compression_ratio": 1.2826086956521738,
      "no_speech_prob": 1.6964253352763636e-12,
      "speaker_id": 1
    },
    {
      "id": 101,
      "seek": 45592,
      "start": 455.92,
      "end": 467.52000000000004,
      "text": " So, either you have a message where you know all of the, like, the length of the fields and how many fields you'll have and the body up front.",
      "tokens": [
        50365,
        407,
        11,
        2139,
        291,
        362,
        257,
        3636,
        689,
        291,
        458,
        439,
        295,
        264,
        11,
        411,
        11,
        264,
        4641,
        295,
        264,
        7909,
        293,
        577,
        867,
        7909,
        291,
        603,
        362,
        293,
        264,
        1772,
        493,
        1868,
        13,
        50945
      ],
      "temperature": 0,
      "avg_logprob": -0.1296389719073692,
      "compression_ratio": 1.5922330097087378,
      "no_speech_prob": 2.508237324436169e-12,
      "speaker_id": 1
    },
    {
      "id": 102,
      "seek": 45592,
      "start": 467.8,
      "end": 476.36,
      "text": " Or you don't know up front and you do, instead of length value, you have a delimiter at the end of these areas.",
      "tokens": [
        50959,
        1610,
        291,
        500,
        380,
        458,
        493,
        1868,
        293,
        291,
        360,
        11,
        2602,
        295,
        4641,
        2158,
        11,
        291,
        362,
        257,
        1103,
        332,
        1681,
        412,
        264,
        917,
        295,
        613,
        3179,
        13,
        51387
      ],
      "temperature": 0,
      "avg_logprob": -0.1296389719073692,
      "compression_ratio": 1.5922330097087378,
      "no_speech_prob": 2.508237324436169e-12,
      "speaker_id": 1
    },
    {
      "id": 103,
      "seek": 45592,
      "start": 477.28000000000003,
      "end": 482.88,
      "text": " So, binary HTTP is capable of doing more than what we have in OHDP today.",
      "tokens": [
        51433,
        407,
        11,
        17434,
        33283,
        307,
        8189,
        295,
        884,
        544,
        813,
        437,
        321,
        362,
        294,
        13931,
        11373,
        965,
        13,
        51713
      ],
      "temperature": 0,
      "avg_logprob": -0.1296389719073692,
      "compression_ratio": 1.5922330097087378,
      "no_speech_prob": 2.508237324436169e-12,
      "speaker_id": 1
    },
    {
      "id": 104,
      "seek": 48288,
      "start": 482.88,
      "end": 487.9,
      "text": " And actually, also, HPKE that OHDP is based on also allows you to have multiple chunks.",
      "tokens": [
        50365,
        400,
        767,
        11,
        611,
        11,
        12557,
        8522,
        300,
        13931,
        11373,
        307,
        2361,
        322,
        611,
        4045,
        291,
        281,
        362,
        3866,
        24004,
        13,
        50616
      ],
      "temperature": 0,
      "avg_logprob": -0.11858029166857402,
      "compression_ratio": 1.4173913043478261,
      "no_speech_prob": 2.3596628123678798e-12,
      "speaker_id": 1
    },
    {
      "id": 105,
      "seek": 48288,
      "start": 488,
      "end": 489.18,
      "text": " We just don't exercise that today.",
      "tokens": [
        50621,
        492,
        445,
        500,
        380,
        5380,
        300,
        965,
        13,
        50680
      ],
      "temperature": 0,
      "avg_logprob": -0.11858029166857402,
      "compression_ratio": 1.4173913043478261,
      "no_speech_prob": 2.3596628123678798e-12,
      "speaker_id": 1
    },
    {
      "id": 106,
      "seek": 48288,
      "start": 491.88,
      "end": 495.26,
      "text": " So, there's, it's definitely possible to do streamed OHDP.",
      "tokens": [
        50815,
        407,
        11,
        456,
        311,
        11,
        309,
        311,
        2138,
        1944,
        281,
        360,
        4309,
        292,
        13931,
        11373,
        13,
        50984
      ],
      "temperature": 0,
      "avg_logprob": -0.11858029166857402,
      "compression_ratio": 1.4173913043478261,
      "no_speech_prob": 2.3596628123678798e-12,
      "speaker_id": 1
    },
    {
      "id": 107,
      "seek": 48288,
      "start": 496.12,
      "end": 498.04,
      "text": " And why would we want to do this?",
      "tokens": [
        51027,
        400,
        983,
        576,
        321,
        528,
        281,
        360,
        341,
        30,
        51123
      ],
      "temperature": 0,
      "avg_logprob": -0.11858029166857402,
      "compression_ratio": 1.4173913043478261,
      "no_speech_prob": 2.3596628123678798e-12,
      "speaker_id": 1
    },
    {
      "id": 108,
      "seek": 48288,
      "start": 498.71999999999997,
      "end": 504.08,
      "text": " And I was a skeptic when Martin originally brought this up.",
      "tokens": [
        51157,
        400,
        286,
        390,
        257,
        19128,
        299,
        562,
        9184,
        7993,
        3038,
        341,
        493,
        13,
        51425
      ],
      "temperature": 0,
      "avg_logprob": -0.11858029166857402,
      "compression_ratio": 1.4173913043478261,
      "no_speech_prob": 2.3596628123678798e-12,
      "speaker_id": 1
    },
    {
      "id": 109,
      "seek": 48288,
      "start": 504.84,
      "end": 508.02,
      "text": " And, you know, for use cases like DNS, I still am.",
      "tokens": [
        51463,
        400,
        11,
        291,
        458,
        11,
        337,
        764,
        3331,
        411,
        35153,
        11,
        286,
        920,
        669,
        13,
        51622
      ],
      "temperature": 0,
      "avg_logprob": -0.11858029166857402,
      "compression_ratio": 1.4173913043478261,
      "no_speech_prob": 2.3596628123678798e-12,
      "speaker_id": 1
    },
    {
      "id": 110,
      "seek": 50802,
      "start": 508.02,
      "end": 513.4,
      "text": " But there are definitely usage patterns that can benefit from streaming.",
      "tokens": [
        50365,
        583,
        456,
        366,
        2138,
        14924,
        8294,
        300,
        393,
        5121,
        490,
        11791,
        13,
        50634
      ],
      "temperature": 0,
      "avg_logprob": -0.14876653529979564,
      "compression_ratio": 1.5,
      "no_speech_prob": 2.648508148905826e-12,
      "speaker_id": 1
    },
    {
      "id": 111,
      "seek": 50802,
      "start": 514.96,
      "end": 516.96,
      "text": " We've talked about this on the list a bit.",
      "tokens": [
        50712,
        492,
        600,
        2825,
        466,
        341,
        322,
        264,
        1329,
        257,
        857,
        13,
        50812
      ],
      "temperature": 0,
      "avg_logprob": -0.14876653529979564,
      "compression_ratio": 1.5,
      "no_speech_prob": 2.648508148905826e-12,
      "speaker_id": 1
    },
    {
      "id": 112,
      "seek": 50802,
      "start": 517.22,
      "end": 525.52,
      "text": " You can have very long messages or messages that take a long time to generate that can be processed in multiple parts.",
      "tokens": [
        50825,
        509,
        393,
        362,
        588,
        938,
        7897,
        420,
        7897,
        300,
        747,
        257,
        938,
        565,
        281,
        8460,
        300,
        393,
        312,
        18846,
        294,
        3866,
        3166,
        13,
        51240
      ],
      "temperature": 0,
      "avg_logprob": -0.14876653529979564,
      "compression_ratio": 1.5,
      "no_speech_prob": 2.648508148905826e-12,
      "speaker_id": 1
    },
    {
      "id": 113,
      "seek": 52552,
      "start": 525.52,
      "end": 540.42,
      "text": " Maybe you have some database lookup or some other thing that's being interactively generated that you can start displaying something to the user or doing local processing on while you're waiting for more content to be generated.",
      "tokens": [
        50365,
        2704,
        291,
        362,
        512,
        8149,
        574,
        1010,
        420,
        512,
        661,
        551,
        300,
        311,
        885,
        4648,
        3413,
        10833,
        300,
        291,
        393,
        722,
        36834,
        746,
        281,
        264,
        4195,
        420,
        884,
        2654,
        9007,
        322,
        1339,
        291,
        434,
        3806,
        337,
        544,
        2701,
        281,
        312,
        10833,
        13,
        51110
      ],
      "temperature": 0,
      "avg_logprob": -0.0829885316931683,
      "compression_ratio": 1.509933774834437,
      "no_speech_prob": 1.8690084202521717e-12,
      "speaker_id": 1
    },
    {
      "id": 114,
      "seek": 54042,
      "start": 540.42,
      "end": 555.24,
      "text": " And you can also have cases where you have a more interactive workflow in which someone makes the initial request and sends, you know, no body or some of the body.",
      "tokens": [
        50365,
        400,
        291,
        393,
        611,
        362,
        3331,
        689,
        291,
        362,
        257,
        544,
        15141,
        20993,
        294,
        597,
        1580,
        1669,
        264,
        5883,
        5308,
        293,
        14790,
        11,
        291,
        458,
        11,
        572,
        1772,
        420,
        512,
        295,
        264,
        1772,
        13,
        51106
      ],
      "temperature": 0,
      "avg_logprob": -0.13221055108147697,
      "compression_ratio": 1.6237623762376239,
      "no_speech_prob": 1.6244257458261635e-12,
      "speaker_id": 1
    },
    {
      "id": 115,
      "seek": 54042,
      "start": 555.5999999999999,
      "end": 557.66,
      "text": " And then the server sends to the response.",
      "tokens": [
        51124,
        400,
        550,
        264,
        7154,
        14790,
        281,
        264,
        4134,
        13,
        51227
      ],
      "temperature": 0,
      "avg_logprob": -0.13221055108147697,
      "compression_ratio": 1.6237623762376239,
      "no_speech_prob": 1.6244257458261635e-12,
      "speaker_id": 1
    },
    {
      "id": 116,
      "seek": 54042,
      "start": 557.78,
      "end": 562.8399999999999,
      "text": " But then you can keep sending more body data in both directions and actually have a little interactive session over OHDP.",
      "tokens": [
        51233,
        583,
        550,
        291,
        393,
        1066,
        7750,
        544,
        1772,
        1412,
        294,
        1293,
        11095,
        293,
        767,
        362,
        257,
        707,
        15141,
        5481,
        670,
        13931,
        11373,
        13,
        51486
      ],
      "temperature": 0,
      "avg_logprob": -0.13221055108147697,
      "compression_ratio": 1.6237623762376239,
      "no_speech_prob": 1.6244257458261635e-12,
      "speaker_id": 1
    },
    {
      "id": 117,
      "seek": 56284,
      "start": 562.84,
      "end": 565.94,
      "text": " And you could do that if you had chunks and streaming.",
      "tokens": [
        50365,
        400,
        291,
        727,
        360,
        300,
        498,
        291,
        632,
        24004,
        293,
        11791,
        13,
        50520
      ],
      "temperature": 0,
      "avg_logprob": -0.15040059523148971,
      "compression_ratio": 1.4887640449438202,
      "no_speech_prob": 1.7333050142745443e-12,
      "speaker_id": 1
    },
    {
      "id": 118,
      "seek": 56284,
      "start": 569.5400000000001,
      "end": 577.5400000000001,
      "text": " So assuming we actually want this and we think that these are good use cases, what would we need to change to make the streaming work?",
      "tokens": [
        50700,
        407,
        11926,
        321,
        767,
        528,
        341,
        293,
        321,
        519,
        300,
        613,
        366,
        665,
        764,
        3331,
        11,
        437,
        576,
        321,
        643,
        281,
        1319,
        281,
        652,
        264,
        11791,
        589,
        30,
        51100
      ],
      "temperature": 0,
      "avg_logprob": -0.15040059523148971,
      "compression_ratio": 1.4887640449438202,
      "no_speech_prob": 1.7333050142745443e-12,
      "speaker_id": 1
    },
    {
      "id": 119,
      "seek": 56284,
      "start": 580.58,
      "end": 583.9,
      "text": " Here I've identified three primary things.",
      "tokens": [
        51252,
        1692,
        286,
        600,
        9234,
        1045,
        6194,
        721,
        13,
        51418
      ],
      "temperature": 0,
      "avg_logprob": -0.15040059523148971,
      "compression_ratio": 1.4887640449438202,
      "no_speech_prob": 1.7333050142745443e-12,
      "speaker_id": 1
    },
    {
      "id": 120,
      "seek": 56284,
      "start": 584.1600000000001,
      "end": 584.98,
      "text": " Happy to hear if there are more.",
      "tokens": [
        51431,
        8277,
        281,
        1568,
        498,
        456,
        366,
        544,
        13,
        51472
      ],
      "temperature": 0,
      "avg_logprob": -0.15040059523148971,
      "compression_ratio": 1.4887640449438202,
      "no_speech_prob": 1.7333050142745443e-12,
      "speaker_id": 1
    },
    {
      "id": 121,
      "seek": 58498,
      "start": 584.98,
      "end": 599.12,
      "text": " First, there is just kind of the cryptographic mechanisms about how we're doing chunk encapsulation, how we do the HPKE for the requests and how we do the AADs for the responses to make them chunkable.",
      "tokens": [
        50365,
        2386,
        11,
        456,
        307,
        445,
        733,
        295,
        264,
        9844,
        12295,
        15902,
        466,
        577,
        321,
        434,
        884,
        16635,
        38745,
        2776,
        11,
        577,
        321,
        360,
        264,
        12557,
        8522,
        337,
        264,
        12475,
        293,
        577,
        321,
        360,
        264,
        316,
        6112,
        82,
        337,
        264,
        13019,
        281,
        652,
        552,
        16635,
        712,
        13,
        51072
      ],
      "temperature": 0,
      "avg_logprob": -0.08891611221509102,
      "compression_ratio": 1.6485148514851484,
      "no_speech_prob": 1.8193374324432243e-12,
      "speaker_id": 1
    },
    {
      "id": 122,
      "seek": 58498,
      "start": 600.4,
      "end": 608.28,
      "text": " We have the request and response format for how we actually send these chunks and delimit the chunks and everything else like that.",
      "tokens": [
        51136,
        492,
        362,
        264,
        5308,
        293,
        4134,
        7877,
        337,
        577,
        321,
        767,
        2845,
        613,
        24004,
        293,
        1103,
        332,
        270,
        264,
        24004,
        293,
        1203,
        1646,
        411,
        300,
        13,
        51530
      ],
      "temperature": 0,
      "avg_logprob": -0.08891611221509102,
      "compression_ratio": 1.6485148514851484,
      "no_speech_prob": 1.8193374324432243e-12,
      "speaker_id": 1
    },
    {
      "id": 123,
      "seek": 60828,
      "start": 608.28,
      "end": 619.5799999999999,
      "text": " And then, as we talked about, when this was originally proposed, it makes sense to have a new media type for this, since if we are changing our request response format, we need to know that we're doing that.",
      "tokens": [
        50365,
        400,
        550,
        11,
        382,
        321,
        2825,
        466,
        11,
        562,
        341,
        390,
        7993,
        10348,
        11,
        309,
        1669,
        2020,
        281,
        362,
        257,
        777,
        3021,
        2010,
        337,
        341,
        11,
        1670,
        498,
        321,
        366,
        4473,
        527,
        5308,
        4134,
        7877,
        11,
        321,
        643,
        281,
        458,
        300,
        321,
        434,
        884,
        300,
        13,
        50930
      ],
      "temperature": 0,
      "avg_logprob": -0.1697126093918715,
      "compression_ratio": 1.6340579710144927,
      "no_speech_prob": 1.8858848944286466e-12,
      "speaker_id": 1
    },
    {
      "id": 124,
      "seek": 60828,
      "start": 619.8199999999999,
      "end": 620.6,
      "text": " Jonathan Hoyland.",
      "tokens": [
        50942,
        15471,
        28664,
        1661,
        13,
        50981
      ],
      "temperature": 0,
      "avg_logprob": -0.1697126093918715,
      "compression_ratio": 1.6340579710144927,
      "no_speech_prob": 1.8858848944286466e-12,
      "speaker_id": 1
    },
    {
      "id": 125,
      "seek": 60828,
      "start": 622.22,
      "end": 623.5,
      "text": " Chunk enthusiast.",
      "tokens": [
        51062,
        761,
        3197,
        18076,
        525,
        13,
        51126
      ],
      "temperature": 0,
      "avg_logprob": -0.1697126093918715,
      "compression_ratio": 1.6340579710144927,
      "no_speech_prob": 1.8858848944286466e-12,
      "speaker_id": 1
    },
    {
      "id": 126,
      "seek": 60828,
      "start": 624.78,
      "end": 626.22,
      "text": " Jonathan Hoyland, Cloudflare.",
      "tokens": [
        51190,
        15471,
        28664,
        1661,
        11,
        8061,
        3423,
        543,
        13,
        51262
      ],
      "temperature": 0,
      "avg_logprob": -0.1697126093918715,
      "compression_ratio": 1.6340579710144927,
      "no_speech_prob": 1.8858848944286466e-12,
      "speaker_id": 2
    },
    {
      "id": 127,
      "seek": 60828,
      "start": 626.88,
      "end": 630,
      "text": " Does this not change the security properties at all?",
      "tokens": [
        51295,
        4402,
        341,
        406,
        1319,
        264,
        3825,
        7221,
        412,
        439,
        30,
        51451
      ],
      "temperature": 0,
      "avg_logprob": -0.1697126093918715,
      "compression_ratio": 1.6340579710144927,
      "no_speech_prob": 1.8858848944286466e-12,
      "speaker_id": 2
    },
    {
      "id": 128,
      "seek": 60828,
      "start": 630.1,
      "end": 633.8399999999999,
      "text": " Like, I haven't had a chance to think about this at all, but I'm surprised it's not on the list.",
      "tokens": [
        51456,
        1743,
        11,
        286,
        2378,
        380,
        632,
        257,
        2931,
        281,
        519,
        466,
        341,
        412,
        439,
        11,
        457,
        286,
        478,
        6100,
        309,
        311,
        406,
        322,
        264,
        1329,
        13,
        51643
      ],
      "temperature": 0,
      "avg_logprob": -0.1697126093918715,
      "compression_ratio": 1.6340579710144927,
      "no_speech_prob": 1.8858848944286466e-12,
      "speaker_id": 2
    },
    {
      "id": 129,
      "seek": 60828,
      "start": 634.72,
      "end": 637.06,
      "text": " Oh, that's why you're here.",
      "tokens": [
        51687,
        876,
        11,
        300,
        311,
        983,
        291,
        434,
        510,
        13,
        51804
      ],
      "temperature": 0,
      "avg_logprob": -0.1697126093918715,
      "compression_ratio": 1.6340579710144927,
      "no_speech_prob": 1.8858848944286466e-12,
      "speaker_id": 1
    },
    {
      "id": 130,
      "seek": 63828,
      "start": 638.28,
      "end": 642.24,
      "text": " No, no, it's certainly, I think it does.",
      "tokens": [
        50365,
        883,
        11,
        572,
        11,
        309,
        311,
        3297,
        11,
        286,
        519,
        309,
        775,
        13,
        50563
      ],
      "temperature": 0,
      "avg_logprob": -0.13641254485599577,
      "compression_ratio": 1.65,
      "no_speech_prob": 1.8063148633090664e-12,
      "speaker_id": 1
    },
    {
      "id": 131,
      "seek": 63828,
      "start": 642.3399999999999,
      "end": 647.12,
      "text": " I mean, these, sorry, I was trying to say, like, you know, what are the technical, like, on the wire changes that need to happen?",
      "tokens": [
        50568,
        286,
        914,
        11,
        613,
        11,
        2597,
        11,
        286,
        390,
        1382,
        281,
        584,
        11,
        411,
        11,
        291,
        458,
        11,
        437,
        366,
        264,
        6191,
        11,
        411,
        11,
        322,
        264,
        6234,
        2962,
        300,
        643,
        281,
        1051,
        30,
        50807
      ],
      "temperature": 0,
      "avg_logprob": -0.13641254485599577,
      "compression_ratio": 1.65,
      "no_speech_prob": 1.8063148633090664e-12,
      "speaker_id": 1
    },
    {
      "id": 132,
      "seek": 63828,
      "start": 647.76,
      "end": 653.8199999999999,
      "text": " I think we need to make sure that we do correctly analyze the security properties and any implications it has.",
      "tokens": [
        50839,
        286,
        519,
        321,
        643,
        281,
        652,
        988,
        300,
        321,
        360,
        8944,
        12477,
        264,
        3825,
        7221,
        293,
        604,
        16602,
        309,
        575,
        13,
        51142
      ],
      "temperature": 0,
      "avg_logprob": -0.13641254485599577,
      "compression_ratio": 1.65,
      "no_speech_prob": 1.8063148633090664e-12,
      "speaker_id": 1
    },
    {
      "id": 133,
      "seek": 63828,
      "start": 655.1,
      "end": 659.8399999999999,
      "text": " For things like HPKE that do support chunks, like, I believe that analysis was done.",
      "tokens": [
        51206,
        1171,
        721,
        411,
        12557,
        8522,
        300,
        360,
        1406,
        24004,
        11,
        411,
        11,
        286,
        1697,
        300,
        5215,
        390,
        1096,
        13,
        51443
      ],
      "temperature": 0,
      "avg_logprob": -0.13641254485599577,
      "compression_ratio": 1.65,
      "no_speech_prob": 1.8063148633090664e-12,
      "speaker_id": 1
    },
    {
      "id": 134,
      "seek": 63828,
      "start": 659.8399999999999,
      "end": 661.66,
      "text": " Yeah, that bit's fine.",
      "tokens": [
        51443,
        865,
        11,
        300,
        857,
        311,
        2489,
        13,
        51534
      ],
      "temperature": 0,
      "avg_logprob": -0.13641254485599577,
      "compression_ratio": 1.65,
      "no_speech_prob": 1.8063148633090664e-12,
      "speaker_id": 2
    },
    {
      "id": 135,
      "seek": 63828,
      "start": 661.8,
      "end": 663.68,
      "text": " It's the doing it in multiple shots.",
      "tokens": [
        51541,
        467,
        311,
        264,
        884,
        309,
        294,
        3866,
        8305,
        13,
        51635
      ],
      "temperature": 0,
      "avg_logprob": -0.13641254485599577,
      "compression_ratio": 1.65,
      "no_speech_prob": 1.8063148633090664e-12,
      "speaker_id": 2
    },
    {
      "id": 136,
      "seek": 63828,
      "start": 663.76,
      "end": 664.8399999999999,
      "text": " Does that change the obliviousness?",
      "tokens": [
        51639,
        4402,
        300,
        1319,
        264,
        47039,
        851,
        1287,
        30,
        51693
      ],
      "temperature": 0,
      "avg_logprob": -0.13641254485599577,
      "compression_ratio": 1.65,
      "no_speech_prob": 1.8063148633090664e-12,
      "speaker_id": 2
    },
    {
      "id": 137,
      "seek": 66828,
      "start": 668.28,
      "end": 670.9399999999999,
      "text": " Oh, for the privacy aspects of it.",
      "tokens": [
        50365,
        876,
        11,
        337,
        264,
        11427,
        7270,
        295,
        309,
        13,
        50498
      ],
      "temperature": 0,
      "avg_logprob": -0.2376992400263397,
      "compression_ratio": 1.3837209302325582,
      "no_speech_prob": 1.6050670991960003e-12,
      "speaker_id": 1
    },
    {
      "id": 138,
      "seek": 66828,
      "start": 671.02,
      "end": 671.14,
      "text": " Yeah.",
      "tokens": [
        50502,
        865,
        13,
        50508
      ],
      "temperature": 0,
      "avg_logprob": -0.2376992400263397,
      "compression_ratio": 1.3837209302325582,
      "no_speech_prob": 1.6050670991960003e-12,
      "speaker_id": 2
    },
    {
      "id": 139,
      "seek": 66828,
      "start": 674.5799999999999,
      "end": 675.9399999999999,
      "text": " Yes, do a security analysis.",
      "tokens": [
        50680,
        1079,
        11,
        360,
        257,
        3825,
        5215,
        13,
        50748
      ],
      "temperature": 0,
      "avg_logprob": -0.2376992400263397,
      "compression_ratio": 1.3837209302325582,
      "no_speech_prob": 1.6050670991960003e-12,
      "speaker_id": 2
    },
    {
      "id": 140,
      "seek": 66828,
      "start": 676.1,
      "end": 677.3,
      "text": " Standing on one foot, it's fine.",
      "tokens": [
        50756,
        33655,
        322,
        472,
        2671,
        11,
        309,
        311,
        2489,
        13,
        50816
      ],
      "temperature": 0,
      "avg_logprob": -0.2376992400263397,
      "compression_ratio": 1.3837209302325582,
      "no_speech_prob": 1.6050670991960003e-12,
      "speaker_id": 2
    },
    {
      "id": 141,
      "seek": 66828,
      "start": 677.64,
      "end": 677.98,
      "text": " Yes.",
      "tokens": [
        50833,
        1079,
        13,
        50850
      ],
      "temperature": 0,
      "avg_logprob": -0.2376992400263397,
      "compression_ratio": 1.3837209302325582,
      "no_speech_prob": 1.6050670991960003e-12,
      "speaker_id": 1
    },
    {
      "id": 142,
      "seek": 66828,
      "start": 678.4,
      "end": 688.42,
      "text": " I mean, certainly, like, it's not going as far as saying that you are being stateful between multiple HTTP requests and responses.",
      "tokens": [
        50871,
        286,
        914,
        11,
        3297,
        11,
        411,
        11,
        309,
        311,
        406,
        516,
        382,
        1400,
        382,
        1566,
        300,
        291,
        366,
        885,
        1785,
        906,
        1296,
        3866,
        33283,
        12475,
        293,
        13019,
        13,
        51372
      ],
      "temperature": 0,
      "avg_logprob": -0.2376992400263397,
      "compression_ratio": 1.3837209302325582,
      "no_speech_prob": 1.6050670991960003e-12,
      "speaker_id": 1
    },
    {
      "id": 143,
      "seek": 68842,
      "start": 688.42,
      "end": 692.66,
      "text": " You know, so I think there are two cases here, right?",
      "tokens": [
        50365,
        509,
        458,
        11,
        370,
        286,
        519,
        456,
        366,
        732,
        3331,
        510,
        11,
        558,
        30,
        50577
      ],
      "temperature": 0,
      "avg_logprob": -0.09051580898097304,
      "compression_ratio": 1.4294871794871795,
      "no_speech_prob": 1.2244132574154576e-12,
      "speaker_id": 1
    },
    {
      "id": 144,
      "seek": 68842,
      "start": 692.66,
      "end": 697.7199999999999,
      "text": " There's the case where you just have, like, I'm just really slow at sending the response I would have done anyway.",
      "tokens": [
        50577,
        821,
        311,
        264,
        1389,
        689,
        291,
        445,
        362,
        11,
        411,
        11,
        286,
        478,
        445,
        534,
        2964,
        412,
        7750,
        264,
        4134,
        286,
        576,
        362,
        1096,
        4033,
        13,
        50830
      ],
      "temperature": 0,
      "avg_logprob": -0.09051580898097304,
      "compression_ratio": 1.4294871794871795,
      "no_speech_prob": 1.2244132574154576e-12,
      "speaker_id": 1
    },
    {
      "id": 145,
      "seek": 68842,
      "start": 697.8199999999999,
      "end": 700.3399999999999,
      "text": " In this case, I think it's purely a performance thing.",
      "tokens": [
        50835,
        682,
        341,
        1389,
        11,
        286,
        519,
        309,
        311,
        17491,
        257,
        3389,
        551,
        13,
        50961
      ],
      "temperature": 0,
      "avg_logprob": -0.09051580898097304,
      "compression_ratio": 1.4294871794871795,
      "no_speech_prob": 1.2244132574154576e-12,
      "speaker_id": 1
    },
    {
      "id": 146,
      "seek": 70034,
      "start": 700.34,
      "end": 720.86,
      "text": " If we are going back and forth, then that does allow you to be stateful within that one session, and I think it's then, to some degree, a use case, an application analysis decision about how are we using this, and are we okay with that, whatever state we are building up within that one session?",
      "tokens": [
        50365,
        759,
        321,
        366,
        516,
        646,
        293,
        5220,
        11,
        550,
        300,
        775,
        2089,
        291,
        281,
        312,
        1785,
        906,
        1951,
        300,
        472,
        5481,
        11,
        293,
        286,
        519,
        309,
        311,
        550,
        11,
        281,
        512,
        4314,
        11,
        257,
        764,
        1389,
        11,
        364,
        3861,
        5215,
        3537,
        466,
        577,
        366,
        321,
        1228,
        341,
        11,
        293,
        366,
        321,
        1392,
        365,
        300,
        11,
        2035,
        1785,
        321,
        366,
        2390,
        493,
        1951,
        300,
        472,
        5481,
        30,
        51391
      ],
      "temperature": 0,
      "avg_logprob": -0.08393412317548479,
      "compression_ratio": 1.6761363636363635,
      "no_speech_prob": 1.74647622752655e-12,
      "speaker_id": 1
    },
    {
      "id": 147,
      "seek": 72086,
      "start": 720.86,
      "end": 726.46,
      "text": " Like, if you had an extremely long-lived session, and you tunneled all of your web browsing over this, then, like, yes, that's worse for privacy.",
      "tokens": [
        50365,
        1743,
        11,
        498,
        291,
        632,
        364,
        4664,
        938,
        12,
        46554,
        5481,
        11,
        293,
        291,
        13186,
        292,
        439,
        295,
        428,
        3670,
        38602,
        670,
        341,
        11,
        550,
        11,
        411,
        11,
        2086,
        11,
        300,
        311,
        5324,
        337,
        11427,
        13,
        50645
      ],
      "temperature": 0,
      "avg_logprob": -0.2540183832615982,
      "compression_ratio": 1.3892215568862276,
      "no_speech_prob": 1.847629471293799e-12,
      "speaker_id": 1
    },
    {
      "id": 148,
      "seek": 72086,
      "start": 729.66,
      "end": 730.16,
      "text": " Thank you.",
      "tokens": [
        50805,
        1044,
        291,
        13,
        50830
      ],
      "temperature": 0,
      "avg_logprob": -0.2540183832615982,
      "compression_ratio": 1.3892215568862276,
      "no_speech_prob": 1.847629471293799e-12,
      "speaker_id": 2
    },
    {
      "id": 149,
      "seek": 72086,
      "start": 730.78,
      "end": 730.9,
      "text": " Yeah.",
      "tokens": [
        50861,
        865,
        13,
        50867
      ],
      "temperature": 0,
      "avg_logprob": -0.2540183832615982,
      "compression_ratio": 1.3892215568862276,
      "no_speech_prob": 1.847629471293799e-12,
      "speaker_id": 1
    },
    {
      "id": 150,
      "seek": 72086,
      "start": 731.0600000000001,
      "end": 731.3000000000001,
      "text": " All right.",
      "tokens": [
        50875,
        1057,
        558,
        13,
        50887
      ],
      "temperature": 0,
      "avg_logprob": -0.2540183832615982,
      "compression_ratio": 1.3892215568862276,
      "no_speech_prob": 1.847629471293799e-12,
      "speaker_id": 1
    },
    {
      "id": 151,
      "seek": 72086,
      "start": 731.74,
      "end": 732.54,
      "text": " There's more of a queue.",
      "tokens": [
        50909,
        821,
        311,
        544,
        295,
        257,
        18639,
        13,
        50949
      ],
      "temperature": 0,
      "avg_logprob": -0.2540183832615982,
      "compression_ratio": 1.3892215568862276,
      "no_speech_prob": 1.847629471293799e-12,
      "speaker_id": 1
    },
    {
      "id": 152,
      "seek": 72086,
      "start": 732.64,
      "end": 733.52,
      "text": " I can't see who's next.",
      "tokens": [
        50954,
        286,
        393,
        380,
        536,
        567,
        311,
        958,
        13,
        50998
      ],
      "temperature": 0,
      "avg_logprob": -0.2540183832615982,
      "compression_ratio": 1.3892215568862276,
      "no_speech_prob": 1.847629471293799e-12,
      "speaker_id": 1
    },
    {
      "id": 153,
      "seek": 72086,
      "start": 734.02,
      "end": 734.16,
      "text": " Ted.",
      "tokens": [
        51023,
        14985,
        13,
        51030
      ],
      "temperature": 0,
      "avg_logprob": -0.2540183832615982,
      "compression_ratio": 1.3892215568862276,
      "no_speech_prob": 1.847629471293799e-12,
      "speaker_id": 1
    },
    {
      "id": 154,
      "seek": 72086,
      "start": 735.72,
      "end": 736.08,
      "text": " Ted.",
      "tokens": [
        51108,
        14985,
        13,
        51126
      ],
      "temperature": 0,
      "avg_logprob": -0.2540183832615982,
      "compression_ratio": 1.3892215568862276,
      "no_speech_prob": 1.847629471293799e-12,
      "speaker_id": 1
    },
    {
      "id": 155,
      "seek": 73608,
      "start": 736.08,
      "end": 743.58,
      "text": " Ted, Friday enthusiast.",
      "tokens": [
        50365,
        14985,
        11,
        6984,
        18076,
        525,
        13,
        50740
      ],
      "temperature": 0,
      "avg_logprob": -0.19816315174102783,
      "compression_ratio": 1.5393939393939393,
      "no_speech_prob": 2.1675792671177474e-12,
      "speaker_id": 3
    },
    {
      "id": 156,
      "seek": 73608,
      "start": 744.46,
      "end": 760.82,
      "text": " So I believe the way the charter is currently written, it focuses a good bit on the use cases for this protocol being those which do not require correlation, where the requests and responses do not require any form of correlation.",
      "tokens": [
        50784,
        407,
        286,
        1697,
        264,
        636,
        264,
        27472,
        307,
        4362,
        3720,
        11,
        309,
        16109,
        257,
        665,
        857,
        322,
        264,
        764,
        3331,
        337,
        341,
        10336,
        885,
        729,
        597,
        360,
        406,
        3651,
        20009,
        11,
        689,
        264,
        12475,
        293,
        13019,
        360,
        406,
        3651,
        604,
        1254,
        295,
        20009,
        13,
        51602
      ],
      "temperature": 0,
      "avg_logprob": -0.19816315174102783,
      "compression_ratio": 1.5393939393939393,
      "no_speech_prob": 2.1675792671177474e-12,
      "speaker_id": 3
    },
    {
      "id": 157,
      "seek": 76082,
      "start": 760.82,
      "end": 767.1400000000001,
      "text": " And so when we went out with the charter, it was like, hey, you're not going to use this to go talk to Gmail, right?",
      "tokens": [
        50365,
        400,
        370,
        562,
        321,
        1437,
        484,
        365,
        264,
        27472,
        11,
        309,
        390,
        411,
        11,
        4177,
        11,
        291,
        434,
        406,
        516,
        281,
        764,
        341,
        281,
        352,
        751,
        281,
        36732,
        11,
        558,
        30,
        50681
      ],
      "temperature": 0,
      "avg_logprob": -0.1936300902807412,
      "compression_ratio": 1.688715953307393,
      "no_speech_prob": 1.6242092306523181e-12,
      "speaker_id": 3
    },
    {
      "id": 158,
      "seek": 76082,
      "start": 767.24,
      "end": 768.7600000000001,
      "text": " Because you're not going to do that.",
      "tokens": [
        50686,
        1436,
        291,
        434,
        406,
        516,
        281,
        360,
        300,
        13,
        50762
      ],
      "temperature": 0,
      "avg_logprob": -0.1936300902807412,
      "compression_ratio": 1.688715953307393,
      "no_speech_prob": 1.6242092306523181e-12,
      "speaker_id": 3
    },
    {
      "id": 159,
      "seek": 76082,
      "start": 769.24,
      "end": 774.9200000000001,
      "text": " And specifically here, like, we're talking about correlation across different requests?",
      "tokens": [
        50786,
        400,
        4682,
        510,
        11,
        411,
        11,
        321,
        434,
        1417,
        466,
        20009,
        2108,
        819,
        12475,
        30,
        51070
      ],
      "temperature": 0,
      "avg_logprob": -0.1936300902807412,
      "compression_ratio": 1.688715953307393,
      "no_speech_prob": 1.6242092306523181e-12,
      "speaker_id": 1
    },
    {
      "id": 160,
      "seek": 76082,
      "start": 775.4000000000001,
      "end": 775.58,
      "text": " Yes.",
      "tokens": [
        51094,
        1079,
        13,
        51103
      ],
      "temperature": 0,
      "avg_logprob": -0.1936300902807412,
      "compression_ratio": 1.688715953307393,
      "no_speech_prob": 1.6242092306523181e-12,
      "speaker_id": 3
    },
    {
      "id": 161,
      "seek": 76082,
      "start": 775.84,
      "end": 775.98,
      "text": " Yeah.",
      "tokens": [
        51116,
        865,
        13,
        51123
      ],
      "temperature": 0,
      "avg_logprob": -0.1936300902807412,
      "compression_ratio": 1.688715953307393,
      "no_speech_prob": 1.6242092306523181e-12,
      "speaker_id": 1
    },
    {
      "id": 162,
      "seek": 76082,
      "start": 776.5400000000001,
      "end": 776.7600000000001,
      "text": " Totally.",
      "tokens": [
        51151,
        22837,
        13,
        51162
      ],
      "temperature": 0,
      "avg_logprob": -0.1936300902807412,
      "compression_ratio": 1.688715953307393,
      "no_speech_prob": 1.6242092306523181e-12,
      "speaker_id": 1
    },
    {
      "id": 163,
      "seek": 76082,
      "start": 776.7600000000001,
      "end": 787.82,
      "text": " So that if you went to Ojai server one for request one and Ojai server two for request two, and they talked to two different backends because of ECMP, it's all good, right?",
      "tokens": [
        51162,
        407,
        300,
        498,
        291,
        1437,
        281,
        47100,
        1301,
        7154,
        472,
        337,
        5308,
        472,
        293,
        47100,
        1301,
        7154,
        732,
        337,
        5308,
        732,
        11,
        293,
        436,
        2825,
        281,
        732,
        819,
        646,
        2581,
        570,
        295,
        19081,
        12224,
        11,
        309,
        311,
        439,
        665,
        11,
        558,
        30,
        51715
      ],
      "temperature": 0,
      "avg_logprob": -0.1936300902807412,
      "compression_ratio": 1.688715953307393,
      "no_speech_prob": 1.6242092306523181e-12,
      "speaker_id": 3
    },
    {
      "id": 164,
      "seek": 78782,
      "start": 787.82,
      "end": 793.7800000000001,
      "text": " But so when I first saw this and I thought and saw streaming, there was like a little moment of panic and like, what?",
      "tokens": [
        50365,
        583,
        370,
        562,
        286,
        700,
        1866,
        341,
        293,
        286,
        1194,
        293,
        1866,
        11791,
        11,
        456,
        390,
        411,
        257,
        707,
        1623,
        295,
        14783,
        293,
        411,
        11,
        437,
        30,
        50663
      ],
      "temperature": 0,
      "avg_logprob": -0.22002319596771502,
      "compression_ratio": 1.5780590717299579,
      "no_speech_prob": 1.8298162464402967e-12,
      "speaker_id": 3
    },
    {
      "id": 165,
      "seek": 78782,
      "start": 795.2800000000001,
      "end": 799.12,
      "text": " Speaking as mock chair, I was like, am I going to have to have an Ojai for mock?",
      "tokens": [
        50738,
        13069,
        382,
        17362,
        6090,
        11,
        286,
        390,
        411,
        11,
        669,
        286,
        516,
        281,
        362,
        281,
        362,
        364,
        47100,
        1301,
        337,
        17362,
        30,
        50930
      ],
      "temperature": 0,
      "avg_logprob": -0.22002319596771502,
      "compression_ratio": 1.5780590717299579,
      "no_speech_prob": 1.8298162464402967e-12,
      "speaker_id": 3
    },
    {
      "id": 166,
      "seek": 78782,
      "start": 799.22,
      "end": 800.36,
      "text": " This is going to be really bad.",
      "tokens": [
        50935,
        639,
        307,
        516,
        281,
        312,
        534,
        1578,
        13,
        50992
      ],
      "temperature": 0,
      "avg_logprob": -0.22002319596771502,
      "compression_ratio": 1.5780590717299579,
      "no_speech_prob": 1.8298162464402967e-12,
      "speaker_id": 3
    },
    {
      "id": 167,
      "seek": 78782,
      "start": 800.5,
      "end": 801.1800000000001,
      "text": " Don't do this to me.",
      "tokens": [
        50999,
        1468,
        380,
        360,
        341,
        281,
        385,
        13,
        51033
      ],
      "temperature": 0,
      "avg_logprob": -0.22002319596771502,
      "compression_ratio": 1.5780590717299579,
      "no_speech_prob": 1.8298162464402967e-12,
      "speaker_id": 3
    },
    {
      "id": 168,
      "seek": 78782,
      "start": 801.38,
      "end": 801.7800000000001,
      "text": " Please, no.",
      "tokens": [
        51043,
        2555,
        11,
        572,
        13,
        51063
      ],
      "temperature": 0,
      "avg_logprob": -0.22002319596771502,
      "compression_ratio": 1.5780590717299579,
      "no_speech_prob": 1.8298162464402967e-12,
      "speaker_id": 1
    },
    {
      "id": 169,
      "seek": 78782,
      "start": 801.7800000000001,
      "end": 805.6800000000001,
      "text": " So thank you for the actual know that you calm down.",
      "tokens": [
        51063,
        407,
        1309,
        291,
        337,
        264,
        3539,
        458,
        300,
        291,
        7151,
        760,
        13,
        51258
      ],
      "temperature": 0,
      "avg_logprob": -0.22002319596771502,
      "compression_ratio": 1.5780590717299579,
      "no_speech_prob": 1.8298162464402967e-12,
      "speaker_id": 3
    },
    {
      "id": 170,
      "seek": 78782,
      "start": 805.7600000000001,
      "end": 806.1400000000001,
      "text": " It's Friday.",
      "tokens": [
        51262,
        467,
        311,
        6984,
        13,
        51281
      ],
      "temperature": 0,
      "avg_logprob": -0.22002319596771502,
      "compression_ratio": 1.5780590717299579,
      "no_speech_prob": 1.8298162464402967e-12,
      "speaker_id": 3
    },
    {
      "id": 171,
      "seek": 78782,
      "start": 806.2800000000001,
      "end": 807.2800000000001,
      "text": " You don't have to panic now.",
      "tokens": [
        51288,
        509,
        500,
        380,
        362,
        281,
        14783,
        586,
        13,
        51338
      ],
      "temperature": 0,
      "avg_logprob": -0.22002319596771502,
      "compression_ratio": 1.5780590717299579,
      "no_speech_prob": 1.8298162464402967e-12,
      "speaker_id": 3
    },
    {
      "id": 172,
      "seek": 78782,
      "start": 807.44,
      "end": 808.2,
      "text": " Pieces of this.",
      "tokens": [
        51346,
        22914,
        887,
        295,
        341,
        13,
        51384
      ],
      "temperature": 0,
      "avg_logprob": -0.22002319596771502,
      "compression_ratio": 1.5780590717299579,
      "no_speech_prob": 1.8298162464402967e-12,
      "speaker_id": 3
    },
    {
      "id": 173,
      "seek": 80820,
      "start": 808.2,
      "end": 830.5,
      "text": " But I do think it's going to be very hard for you to describe in simple terms, and I encourage you in the zero, zero to do it in very simple terms, which kinds of flows will need this sort of chunked indeterminate behavior, but will retain the lack of correlation property?",
      "tokens": [
        50365,
        583,
        286,
        360,
        519,
        309,
        311,
        516,
        281,
        312,
        588,
        1152,
        337,
        291,
        281,
        6786,
        294,
        2199,
        2115,
        11,
        293,
        286,
        5373,
        291,
        294,
        264,
        4018,
        11,
        4018,
        281,
        360,
        309,
        294,
        588,
        2199,
        2115,
        11,
        597,
        3685,
        295,
        12867,
        486,
        643,
        341,
        1333,
        295,
        16635,
        292,
        1016,
        35344,
        13923,
        5223,
        11,
        457,
        486,
        18340,
        264,
        5011,
        295,
        20009,
        4707,
        30,
        51480
      ],
      "temperature": 0,
      "avg_logprob": -0.11650962578622918,
      "compression_ratio": 1.527363184079602,
      "no_speech_prob": 1.8070901762826108e-12,
      "speaker_id": 3
    },
    {
      "id": 174,
      "seek": 80820,
      "start": 831.08,
      "end": 832.0600000000001,
      "text": " Yeah, that's a really good point.",
      "tokens": [
        51509,
        865,
        11,
        300,
        311,
        257,
        534,
        665,
        935,
        13,
        51558
      ],
      "temperature": 0,
      "avg_logprob": -0.11650962578622918,
      "compression_ratio": 1.527363184079602,
      "no_speech_prob": 1.8070901762826108e-12,
      "speaker_id": 1
    },
    {
      "id": 175,
      "seek": 83206,
      "start": 832.06,
      "end": 848.2199999999999,
      "text": " And so please ditch the phrase streaming at some point along the way, if I may suggest, and pick something else that highlights that particular pairing, because I think that's the core of what you're proposing, and it's a little bit different from what leaps to mind.",
      "tokens": [
        50365,
        400,
        370,
        1767,
        25325,
        264,
        9535,
        11791,
        412,
        512,
        935,
        2051,
        264,
        636,
        11,
        498,
        286,
        815,
        3402,
        11,
        293,
        1888,
        746,
        1646,
        300,
        14254,
        300,
        1729,
        32735,
        11,
        570,
        286,
        519,
        300,
        311,
        264,
        4965,
        295,
        437,
        291,
        434,
        29939,
        11,
        293,
        309,
        311,
        257,
        707,
        857,
        819,
        490,
        437,
        476,
        2382,
        281,
        1575,
        13,
        51173
      ],
      "temperature": 0,
      "avg_logprob": -0.1308238949395914,
      "compression_ratio": 1.6063829787234043,
      "no_speech_prob": 1.7608866838617065e-12,
      "speaker_id": 3
    },
    {
      "id": 176,
      "seek": 83206,
      "start": 848.56,
      "end": 848.78,
      "text": " Thanks.",
      "tokens": [
        51190,
        2561,
        13,
        51201
      ],
      "temperature": 0,
      "avg_logprob": -0.1308238949395914,
      "compression_ratio": 1.6063829787234043,
      "no_speech_prob": 1.7608866838617065e-12,
      "speaker_id": 3
    },
    {
      "id": 177,
      "seek": 83206,
      "start": 849.2199999999999,
      "end": 849.3399999999999,
      "text": " Yeah.",
      "tokens": [
        51223,
        865,
        13,
        51229
      ],
      "temperature": 0,
      "avg_logprob": -0.1308238949395914,
      "compression_ratio": 1.6063829787234043,
      "no_speech_prob": 1.7608866838617065e-12,
      "speaker_id": 1
    },
    {
      "id": 178,
      "seek": 83206,
      "start": 849.66,
      "end": 854.7199999999999,
      "text": " And here, streaming is inherited from the PR that was abandoned previously.",
      "tokens": [
        51245,
        400,
        510,
        11,
        11791,
        307,
        27091,
        490,
        264,
        11568,
        300,
        390,
        13732,
        8046,
        13,
        51498
      ],
      "temperature": 0,
      "avg_logprob": -0.1308238949395914,
      "compression_ratio": 1.6063829787234043,
      "no_speech_prob": 1.7608866838617065e-12,
      "speaker_id": 1
    },
    {
      "id": 179,
      "seek": 83206,
      "start": 854.8599999999999,
      "end": 857.4399999999999,
      "text": " I would love to hear people's suggestions.",
      "tokens": [
        51505,
        286,
        576,
        959,
        281,
        1568,
        561,
        311,
        13396,
        13,
        51634
      ],
      "temperature": 0,
      "avg_logprob": -0.1308238949395914,
      "compression_ratio": 1.6063829787234043,
      "no_speech_prob": 1.7608866838617065e-12,
      "speaker_id": 1
    },
    {
      "id": 180,
      "seek": 83206,
      "start": 858.4399999999999,
      "end": 861.2199999999999,
      "text": " Maybe it's just chunked, or I don't know what it is.",
      "tokens": [
        51684,
        2704,
        309,
        311,
        445,
        16635,
        292,
        11,
        420,
        286,
        500,
        380,
        458,
        437,
        309,
        307,
        13,
        51823
      ],
      "temperature": 0,
      "avg_logprob": -0.1308238949395914,
      "compression_ratio": 1.6063829787234043,
      "no_speech_prob": 1.7608866838617065e-12,
      "speaker_id": 1
    },
    {
      "id": 181,
      "seek": 86122,
      "start": 861.22,
      "end": 861.6600000000001,
      "text": " Yeah.",
      "tokens": [
        50365,
        865,
        13,
        50387
      ],
      "temperature": 0,
      "avg_logprob": -0.25219526635595113,
      "compression_ratio": 1.5317073170731708,
      "no_speech_prob": 1.8835401988104294e-12,
      "speaker_id": 3
    },
    {
      "id": 182,
      "seek": 86122,
      "start": 863.94,
      "end": 864.1800000000001,
      "text": " Okay.",
      "tokens": [
        50501,
        1033,
        13,
        50513
      ],
      "temperature": 0,
      "avg_logprob": -0.25219526635595113,
      "compression_ratio": 1.5317073170731708,
      "no_speech_prob": 1.8835401988104294e-12,
      "speaker_id": 1
    },
    {
      "id": 183,
      "seek": 86122,
      "start": 864.46,
      "end": 864.6800000000001,
      "text": " Yeah.",
      "tokens": [
        50527,
        865,
        13,
        50538
      ],
      "temperature": 0,
      "avg_logprob": -0.25219526635595113,
      "compression_ratio": 1.5317073170731708,
      "no_speech_prob": 1.8835401988104294e-12,
      "speaker_id": 1
    },
    {
      "id": 184,
      "seek": 86122,
      "start": 865.6600000000001,
      "end": 867.0600000000001,
      "text": " Send me your bad ideas.",
      "tokens": [
        50587,
        17908,
        385,
        428,
        1578,
        3487,
        13,
        50657
      ],
      "temperature": 0,
      "avg_logprob": -0.25219526635595113,
      "compression_ratio": 1.5317073170731708,
      "no_speech_prob": 1.8835401988104294e-12,
      "speaker_id": 1
    },
    {
      "id": 185,
      "seek": 86122,
      "start": 868.74,
      "end": 870.1600000000001,
      "text": " Dennis Jackson, Mozilla.",
      "tokens": [
        50741,
        23376,
        10647,
        11,
        3335,
        26403,
        13,
        50812
      ],
      "temperature": 0,
      "avg_logprob": -0.25219526635595113,
      "compression_ratio": 1.5317073170731708,
      "no_speech_prob": 1.8835401988104294e-12,
      "speaker_id": 4
    },
    {
      "id": 186,
      "seek": 86122,
      "start": 871.12,
      "end": 885.88,
      "text": " Just to echo the two previous comments and questions and understand better, the intent here is that the server is going to start processing this before it reaches the end of the stream, or just that the client is able to send this a bit at a time?",
      "tokens": [
        50860,
        1449,
        281,
        14300,
        264,
        732,
        3894,
        3053,
        293,
        1651,
        293,
        1223,
        1101,
        11,
        264,
        8446,
        510,
        307,
        300,
        264,
        7154,
        307,
        516,
        281,
        722,
        9007,
        341,
        949,
        309,
        14235,
        264,
        917,
        295,
        264,
        4309,
        11,
        420,
        445,
        300,
        264,
        6423,
        307,
        1075,
        281,
        2845,
        341,
        257,
        857,
        412,
        257,
        565,
        30,
        51598
      ],
      "temperature": 0,
      "avg_logprob": -0.25219526635595113,
      "compression_ratio": 1.5317073170731708,
      "no_speech_prob": 1.8835401988104294e-12,
      "speaker_id": 4
    },
    {
      "id": 187,
      "seek": 88588,
      "start": 885.88,
      "end": 900.26,
      "text": " One of the main use cases I have in my head is that actually, you know, probably your request is small and simple, and then you get a large response that may take a while to look up or to generate.",
      "tokens": [
        50365,
        1485,
        295,
        264,
        2135,
        764,
        3331,
        286,
        362,
        294,
        452,
        1378,
        307,
        300,
        767,
        11,
        291,
        458,
        11,
        1391,
        428,
        5308,
        307,
        1359,
        293,
        2199,
        11,
        293,
        550,
        291,
        483,
        257,
        2416,
        4134,
        300,
        815,
        747,
        257,
        1339,
        281,
        574,
        493,
        420,
        281,
        8460,
        13,
        51084
      ],
      "temperature": 0,
      "avg_logprob": -0.13150934297211317,
      "compression_ratio": 1.3873239436619718,
      "no_speech_prob": 1.5351986175360377e-12,
      "speaker_id": 1
    },
    {
      "id": 188,
      "seek": 90026,
      "start": 900.26,
      "end": 918.26,
      "text": " And rather than saying, I need to wait for this giant multi-megabyte response to all come in to be able to decrypt it at all, I can start using it in chunks and processing it while I'm waiting for this very slow server or slow network to keep giving me the rest of the data.",
      "tokens": [
        50365,
        400,
        2831,
        813,
        1566,
        11,
        286,
        643,
        281,
        1699,
        337,
        341,
        7410,
        4825,
        12,
        42800,
        34529,
        4134,
        281,
        439,
        808,
        294,
        281,
        312,
        1075,
        281,
        979,
        627,
        662,
        309,
        412,
        439,
        11,
        286,
        393,
        722,
        1228,
        309,
        294,
        24004,
        293,
        9007,
        309,
        1339,
        286,
        478,
        3806,
        337,
        341,
        588,
        2964,
        7154,
        420,
        2964,
        3209,
        281,
        1066,
        2902,
        385,
        264,
        1472,
        295,
        264,
        1412,
        13,
        51265
      ],
      "temperature": 0,
      "avg_logprob": -0.10221632789163028,
      "compression_ratio": 1.5393258426966292,
      "no_speech_prob": 1.8457845928770977e-12,
      "speaker_id": 1
    },
    {
      "id": 189,
      "seek": 91826,
      "start": 918.26,
      "end": 923.34,
      "text": " So at least on one side, you're going to start processing the data in the stream before the end of the stream.",
      "tokens": [
        50365,
        407,
        412,
        1935,
        322,
        472,
        1252,
        11,
        291,
        434,
        516,
        281,
        722,
        9007,
        264,
        1412,
        294,
        264,
        4309,
        949,
        264,
        917,
        295,
        264,
        4309,
        13,
        50619
      ],
      "temperature": 0,
      "avg_logprob": -0.25414757650406633,
      "compression_ratio": 1.352112676056338,
      "no_speech_prob": 2.016125981441075e-12,
      "speaker_id": 4
    },
    {
      "id": 190,
      "seek": 91826,
      "start": 923.72,
      "end": 923.8199999999999,
      "text": " Yeah.",
      "tokens": [
        50638,
        865,
        13,
        50643
      ],
      "temperature": 0,
      "avg_logprob": -0.25414757650406633,
      "compression_ratio": 1.352112676056338,
      "no_speech_prob": 2.016125981441075e-12,
      "speaker_id": 1
    },
    {
      "id": 191,
      "seek": 91826,
      "start": 923.8199999999999,
      "end": 923.92,
      "text": " Okay.",
      "tokens": [
        50643,
        1033,
        13,
        50648
      ],
      "temperature": 0,
      "avg_logprob": -0.25414757650406633,
      "compression_ratio": 1.352112676056338,
      "no_speech_prob": 2.016125981441075e-12,
      "speaker_id": 4
    },
    {
      "id": 192,
      "seek": 91826,
      "start": 924.16,
      "end": 924.36,
      "text": " Thanks.",
      "tokens": [
        50660,
        2561,
        13,
        50670
      ],
      "temperature": 0,
      "avg_logprob": -0.25414757650406633,
      "compression_ratio": 1.352112676056338,
      "no_speech_prob": 2.016125981441075e-12,
      "speaker_id": 4
    },
    {
      "id": 193,
      "seek": 91826,
      "start": 924.5,
      "end": 924.84,
      "text": " That's right.",
      "tokens": [
        50677,
        663,
        311,
        558,
        13,
        50694
      ],
      "temperature": 0,
      "avg_logprob": -0.25414757650406633,
      "compression_ratio": 1.352112676056338,
      "no_speech_prob": 2.016125981441075e-12,
      "speaker_id": 1
    },
    {
      "id": 194,
      "seek": 91826,
      "start": 928.52,
      "end": 932.38,
      "text": " David Skenazi, privacy enthusiast in this case.",
      "tokens": [
        50878,
        4389,
        318,
        2653,
        26637,
        11,
        11427,
        18076,
        525,
        294,
        341,
        1389,
        13,
        51071
      ],
      "temperature": 0,
      "avg_logprob": -0.25414757650406633,
      "compression_ratio": 1.352112676056338,
      "no_speech_prob": 2.016125981441075e-12,
      "speaker_id": 5
    },
    {
      "id": 195,
      "seek": 93238,
      "start": 932.38,
      "end": 942.12,
      "text": " So in our world where we're, you know, working to improve privacy, OHI is one of my favorite tools, but it's not the only one.",
      "tokens": [
        50365,
        407,
        294,
        527,
        1002,
        689,
        321,
        434,
        11,
        291,
        458,
        11,
        1364,
        281,
        3470,
        11427,
        11,
        13931,
        40,
        307,
        472,
        295,
        452,
        2954,
        3873,
        11,
        457,
        309,
        311,
        406,
        264,
        787,
        472,
        13,
        50852
      ],
      "temperature": 0,
      "avg_logprob": -0.22691556458832116,
      "compression_ratio": 1.5318181818181817,
      "no_speech_prob": 2.3572294290119533e-12,
      "speaker_id": 5
    },
    {
      "id": 196,
      "seek": 93238,
      "start": 942.4,
      "end": 945.62,
      "text": " And you can guess which one is my really, really favorite one.",
      "tokens": [
        50866,
        400,
        291,
        393,
        2041,
        597,
        472,
        307,
        452,
        534,
        11,
        534,
        2954,
        472,
        13,
        51027
      ],
      "temperature": 0,
      "avg_logprob": -0.22691556458832116,
      "compression_ratio": 1.5318181818181817,
      "no_speech_prob": 2.3572294290119533e-12,
      "speaker_id": 5
    },
    {
      "id": 197,
      "seek": 93238,
      "start": 947.72,
      "end": 961.72,
      "text": " So, so mask, if it wasn't obvious, but no, but so here I regularly answer the question at work of, wait, why are we implementing both mask and OHI?",
      "tokens": [
        51132,
        407,
        11,
        370,
        6094,
        11,
        498,
        309,
        2067,
        380,
        6322,
        11,
        457,
        572,
        11,
        457,
        370,
        510,
        286,
        11672,
        1867,
        264,
        1168,
        412,
        589,
        295,
        11,
        1699,
        11,
        983,
        366,
        321,
        18114,
        1293,
        6094,
        293,
        13931,
        40,
        30,
        51832
      ],
      "temperature": 0,
      "avg_logprob": -0.22691556458832116,
      "compression_ratio": 1.5318181818181817,
      "no_speech_prob": 2.3572294290119533e-12,
      "speaker_id": 5
    },
    {
      "id": 198,
      "seek": 96172,
      "start": 961.72,
      "end": 962.98,
      "text": " I also do this.",
      "tokens": [
        50365,
        286,
        611,
        360,
        341,
        13,
        50428
      ],
      "temperature": 0,
      "avg_logprob": -0.20201245060673467,
      "compression_ratio": 1.6851063829787234,
      "no_speech_prob": 1.5606783443714023e-12,
      "speaker_id": 1
    },
    {
      "id": 199,
      "seek": 96172,
      "start": 963.08,
      "end": 963.44,
      "text": " Yeah, right.",
      "tokens": [
        50433,
        865,
        11,
        558,
        13,
        50451
      ],
      "temperature": 0,
      "avg_logprob": -0.20201245060673467,
      "compression_ratio": 1.6851063829787234,
      "no_speech_prob": 1.5606783443714023e-12,
      "speaker_id": 5
    },
    {
      "id": 200,
      "seek": 96172,
      "start": 964.28,
      "end": 968.1800000000001,
      "text": " I kind of wrote a draft explaining that, so I don't need to repeat it anymore.",
      "tokens": [
        50493,
        286,
        733,
        295,
        4114,
        257,
        11206,
        13468,
        300,
        11,
        370,
        286,
        500,
        380,
        643,
        281,
        7149,
        309,
        3602,
        13,
        50688
      ],
      "temperature": 0,
      "avg_logprob": -0.20201245060673467,
      "compression_ratio": 1.6851063829787234,
      "no_speech_prob": 1.5606783443714023e-12,
      "speaker_id": 5
    },
    {
      "id": 201,
      "seek": 96172,
      "start": 968.4,
      "end": 973.1,
      "text": " But anyway, there's, there, you scope for different things.",
      "tokens": [
        50699,
        583,
        4033,
        11,
        456,
        311,
        11,
        456,
        11,
        291,
        11923,
        337,
        819,
        721,
        13,
        50934
      ],
      "temperature": 0,
      "avg_logprob": -0.20201245060673467,
      "compression_ratio": 1.6851063829787234,
      "no_speech_prob": 1.5606783443714023e-12,
      "speaker_id": 5
    },
    {
      "id": 202,
      "seek": 96172,
      "start": 973.1,
      "end": 980.26,
      "text": " And my answer when people ask is when you have multiple requests that are de-correlated, you reach for OHI.",
      "tokens": [
        50934,
        400,
        452,
        1867,
        562,
        561,
        1029,
        307,
        562,
        291,
        362,
        3866,
        12475,
        300,
        366,
        368,
        12,
        19558,
        12004,
        11,
        291,
        2524,
        337,
        13931,
        40,
        13,
        51292
      ],
      "temperature": 0,
      "avg_logprob": -0.20201245060673467,
      "compression_ratio": 1.6851063829787234,
      "no_speech_prob": 1.5606783443714023e-12,
      "speaker_id": 5
    },
    {
      "id": 203,
      "seek": 96172,
      "start": 980.58,
      "end": 980.88,
      "text": " Yes.",
      "tokens": [
        51308,
        1079,
        13,
        51323
      ],
      "temperature": 0,
      "avg_logprob": -0.20201245060673467,
      "compression_ratio": 1.6851063829787234,
      "no_speech_prob": 1.5606783443714023e-12,
      "speaker_id": 1
    },
    {
      "id": 204,
      "seek": 96172,
      "start": 981.02,
      "end": 986.74,
      "text": " And then if you have a case like a web browsing context where multiple requests are correlated, you reach for mask.",
      "tokens": [
        51330,
        400,
        550,
        498,
        291,
        362,
        257,
        1389,
        411,
        257,
        3670,
        38602,
        4319,
        689,
        3866,
        12475,
        366,
        38574,
        11,
        291,
        2524,
        337,
        6094,
        13,
        51616
      ],
      "temperature": 0,
      "avg_logprob": -0.20201245060673467,
      "compression_ratio": 1.6851063829787234,
      "no_speech_prob": 1.5606783443714023e-12,
      "speaker_id": 5
    },
    {
      "id": 205,
      "seek": 98674,
      "start": 987.42,
      "end": 998.82,
      "text": " And so my question is, this almost, like my naive impression is why don't you use mask and I hate to be that guy, but can you explain why that's not the right solution here?",
      "tokens": [
        50399,
        400,
        370,
        452,
        1168,
        307,
        11,
        341,
        1920,
        11,
        411,
        452,
        29052,
        9995,
        307,
        983,
        500,
        380,
        291,
        764,
        6094,
        293,
        286,
        4700,
        281,
        312,
        300,
        2146,
        11,
        457,
        393,
        291,
        2903,
        983,
        300,
        311,
        406,
        264,
        558,
        3827,
        510,
        30,
        50969
      ],
      "temperature": 0,
      "avg_logprob": -0.10672767427232531,
      "compression_ratio": 1.3307692307692307,
      "no_speech_prob": 1.529459068075334e-12,
      "speaker_id": 5
    },
    {
      "id": 206,
      "seek": 99882,
      "start": 998.82,
      "end": 999.1,
      "text": " Yeah.",
      "tokens": [
        50365,
        865,
        13,
        50379
      ],
      "temperature": 0,
      "avg_logprob": -0.13098721755178352,
      "compression_ratio": 1.5072463768115942,
      "no_speech_prob": 1.5024827085610704e-12,
      "speaker_id": 1
    },
    {
      "id": 207,
      "seek": 99882,
      "start": 999.6400000000001,
      "end": 1008.8000000000001,
      "text": " And having also answered this question a multitude of times, I think there are a couple other dimensions to why you would choose OHI over mask.",
      "tokens": [
        50406,
        400,
        1419,
        611,
        10103,
        341,
        1168,
        257,
        36358,
        295,
        1413,
        11,
        286,
        519,
        456,
        366,
        257,
        1916,
        661,
        12819,
        281,
        983,
        291,
        576,
        2826,
        13931,
        40,
        670,
        6094,
        13,
        50864
      ],
      "temperature": 0,
      "avg_logprob": -0.13098721755178352,
      "compression_ratio": 1.5072463768115942,
      "no_speech_prob": 1.5024827085610704e-12,
      "speaker_id": 1
    },
    {
      "id": 208,
      "seek": 99882,
      "start": 1009.1,
      "end": 1011.1,
      "text": " And it's certainly not the generic one.",
      "tokens": [
        50879,
        400,
        309,
        311,
        3297,
        406,
        264,
        19577,
        472,
        13,
        50979
      ],
      "temperature": 0,
      "avg_logprob": -0.13098721755178352,
      "compression_ratio": 1.5072463768115942,
      "no_speech_prob": 1.5024827085610704e-12,
      "speaker_id": 1
    },
    {
      "id": 209,
      "seek": 99882,
      "start": 1011.62,
      "end": 1020.6400000000001,
      "text": " But so first of all, I think the big one is OHI does require modification or coordination by the servers along the system.",
      "tokens": [
        51005,
        583,
        370,
        700,
        295,
        439,
        11,
        286,
        519,
        264,
        955,
        472,
        307,
        13931,
        40,
        775,
        3651,
        26747,
        420,
        21252,
        538,
        264,
        15909,
        2051,
        264,
        1185,
        13,
        51456
      ],
      "temperature": 0,
      "avg_logprob": -0.13098721755178352,
      "compression_ratio": 1.5072463768115942,
      "no_speech_prob": 1.5024827085610704e-12,
      "speaker_id": 1
    },
    {
      "id": 210,
      "seek": 102064,
      "start": 1020.64,
      "end": 1031.8,
      "text": " I think that's a really big one that you need to intentionally be using this and working with the server that is trying to preserve your privacy, whereas mask is fantastic for talking to completely unmodified backends.",
      "tokens": [
        50365,
        286,
        519,
        300,
        311,
        257,
        534,
        955,
        472,
        300,
        291,
        643,
        281,
        22062,
        312,
        1228,
        341,
        293,
        1364,
        365,
        264,
        7154,
        300,
        307,
        1382,
        281,
        15665,
        428,
        11427,
        11,
        9735,
        6094,
        307,
        5456,
        337,
        1417,
        281,
        2584,
        517,
        8014,
        2587,
        646,
        2581,
        13,
        50923
      ],
      "temperature": 0,
      "avg_logprob": -0.1428966686643403,
      "compression_ratio": 1.5,
      "no_speech_prob": 1.720840592418782e-12,
      "speaker_id": 1
    },
    {
      "id": 211,
      "seek": 102064,
      "start": 1032.62,
      "end": 1034.1399999999999,
      "text": " So that's like the first divergence point.",
      "tokens": [
        50964,
        407,
        300,
        311,
        411,
        264,
        700,
        47387,
        935,
        13,
        51040
      ],
      "temperature": 0,
      "avg_logprob": -0.1428966686643403,
      "compression_ratio": 1.5,
      "no_speech_prob": 1.720840592418782e-12,
      "speaker_id": 1
    },
    {
      "id": 212,
      "seek": 103414,
      "start": 1034.14,
      "end": 1055.16,
      "text": " Then, yes, the other property here is with OHI, we're able to do, you know, it's per message decorrelation and separation as opposed to, you know, per TLS session or whatever else the end-to-end connection is.",
      "tokens": [
        50365,
        1396,
        11,
        2086,
        11,
        264,
        661,
        4707,
        510,
        307,
        365,
        13931,
        40,
        11,
        321,
        434,
        1075,
        281,
        360,
        11,
        291,
        458,
        11,
        309,
        311,
        680,
        3636,
        979,
        284,
        4419,
        399,
        293,
        14634,
        382,
        8851,
        281,
        11,
        291,
        458,
        11,
        680,
        314,
        19198,
        5481,
        420,
        2035,
        1646,
        264,
        917,
        12,
        1353,
        12,
        521,
        4984,
        307,
        13,
        51416
      ],
      "temperature": 0,
      "avg_logprob": -0.10436297271211269,
      "compression_ratio": 1.3933333333333333,
      "no_speech_prob": 1.9498272354340207e-12,
      "speaker_id": 1
    },
    {
      "id": 213,
      "seek": 105516,
      "start": 1055.16,
      "end": 1067.0600000000002,
      "text": " Yes, yes, yes, yes, chunk is different, but like oblivious HTTP message, which could include binary HTTP or something else.",
      "tokens": [
        50365,
        1079,
        11,
        2086,
        11,
        2086,
        11,
        2086,
        11,
        16635,
        307,
        819,
        11,
        457,
        411,
        47039,
        851,
        33283,
        3636,
        11,
        597,
        727,
        4090,
        17434,
        33283,
        420,
        746,
        1646,
        13,
        50960
      ],
      "temperature": 0,
      "avg_logprob": -0.0936765875867618,
      "compression_ratio": 1.5732758620689655,
      "no_speech_prob": 1.8311487309102814e-12,
      "speaker_id": 1
    },
    {
      "id": 214,
      "seek": 105516,
      "start": 1069.0600000000002,
      "end": 1084.48,
      "text": " But it's when you want to be able to, so the privacy guarantees are stronger with oblivious HTTP, unless for a mask session, you had to do kind of like the end-to-end TLS, send one message and then tear it down, which is much less efficient.",
      "tokens": [
        51060,
        583,
        309,
        311,
        562,
        291,
        528,
        281,
        312,
        1075,
        281,
        11,
        370,
        264,
        11427,
        32567,
        366,
        7249,
        365,
        47039,
        851,
        33283,
        11,
        5969,
        337,
        257,
        6094,
        5481,
        11,
        291,
        632,
        281,
        360,
        733,
        295,
        411,
        264,
        917,
        12,
        1353,
        12,
        521,
        314,
        19198,
        11,
        2845,
        472,
        3636,
        293,
        550,
        12556,
        309,
        760,
        11,
        597,
        307,
        709,
        1570,
        7148,
        13,
        51831
      ],
      "temperature": 0,
      "avg_logprob": -0.0936765875867618,
      "compression_ratio": 1.5732758620689655,
      "no_speech_prob": 1.8311487309102814e-12,
      "speaker_id": 1
    },
    {
      "id": 215,
      "seek": 108448,
      "start": 1084.48,
      "end": 1094.8600000000001,
      "text": " So if you want to have the complete decorrelation properties and are able to coordinate with the end server and you care about performance, you really should use OHI.",
      "tokens": [
        50365,
        407,
        498,
        291,
        528,
        281,
        362,
        264,
        3566,
        979,
        284,
        4419,
        399,
        7221,
        293,
        366,
        1075,
        281,
        15670,
        365,
        264,
        917,
        7154,
        293,
        291,
        1127,
        466,
        3389,
        11,
        291,
        534,
        820,
        764,
        13931,
        40,
        13,
        50884
      ],
      "temperature": 0,
      "avg_logprob": -0.13400726733000382,
      "compression_ratio": 1.5606694560669456,
      "no_speech_prob": 1.6451878921686092e-12,
      "speaker_id": 1
    },
    {
      "id": 216,
      "seek": 108448,
      "start": 1095.72,
      "end": 1103.18,
      "text": " And streaming here just means I may have a large message, but it's still always one request and one response.",
      "tokens": [
        50927,
        400,
        11791,
        510,
        445,
        1355,
        286,
        815,
        362,
        257,
        2416,
        3636,
        11,
        457,
        309,
        311,
        920,
        1009,
        472,
        5308,
        293,
        472,
        4134,
        13,
        51300
      ],
      "temperature": 0,
      "avg_logprob": -0.13400726733000382,
      "compression_ratio": 1.5606694560669456,
      "no_speech_prob": 1.6451878921686092e-12,
      "speaker_id": 1
    },
    {
      "id": 217,
      "seek": 108448,
      "start": 1104.84,
      "end": 1105.8600000000001,
      "text": " I see.",
      "tokens": [
        51383,
        286,
        536,
        13,
        51434
      ],
      "temperature": 0,
      "avg_logprob": -0.13400726733000382,
      "compression_ratio": 1.5606694560669456,
      "no_speech_prob": 1.6451878921686092e-12,
      "speaker_id": 5
    },
    {
      "id": 218,
      "seek": 108448,
      "start": 1106,
      "end": 1111.48,
      "text": " So just to summarize, make sure I get what you're saying, you could solve this with mask.",
      "tokens": [
        51441,
        407,
        445,
        281,
        20858,
        11,
        652,
        988,
        286,
        483,
        437,
        291,
        434,
        1566,
        11,
        291,
        727,
        5039,
        341,
        365,
        6094,
        13,
        51715
      ],
      "temperature": 0,
      "avg_logprob": -0.13400726733000382,
      "compression_ratio": 1.5606694560669456,
      "no_speech_prob": 1.6451878921686092e-12,
      "speaker_id": 5
    },
    {
      "id": 219,
      "seek": 111148,
      "start": 1111.48,
      "end": 1123.64,
      "text": " However, you would need for each streaming request to do a full TLS handshake, and that would be cryptographically, computationally more expensive than doing one HPTE.",
      "tokens": [
        50365,
        2908,
        11,
        291,
        576,
        643,
        337,
        1184,
        11791,
        5308,
        281,
        360,
        257,
        1577,
        314,
        19198,
        2377,
        34593,
        11,
        293,
        300,
        576,
        312,
        9844,
        3108,
        984,
        11,
        24903,
        379,
        544,
        5124,
        813,
        884,
        472,
        12557,
        13639,
        13,
        50973
      ],
      "temperature": 0,
      "avg_logprob": -0.1944249251793171,
      "compression_ratio": 1.6321428571428571,
      "no_speech_prob": 1.861591176349764e-12,
      "speaker_id": 5
    },
    {
      "id": 220,
      "seek": 111148,
      "start": 1123.8,
      "end": 1125.18,
      "text": " So this is a performance optimization.",
      "tokens": [
        50981,
        407,
        341,
        307,
        257,
        3389,
        19618,
        13,
        51050
      ],
      "temperature": 0,
      "avg_logprob": -0.1944249251793171,
      "compression_ratio": 1.6321428571428571,
      "no_speech_prob": 1.861591176349764e-12,
      "speaker_id": 5
    },
    {
      "id": 221,
      "seek": 111148,
      "start": 1125.32,
      "end": 1126.02,
      "text": " That's a good justification.",
      "tokens": [
        51057,
        663,
        311,
        257,
        665,
        31591,
        13,
        51092
      ],
      "temperature": 0,
      "avg_logprob": -0.1944249251793171,
      "compression_ratio": 1.6321428571428571,
      "no_speech_prob": 1.861591176349764e-12,
      "speaker_id": 5
    },
    {
      "id": 222,
      "seek": 111148,
      "start": 1126.02,
      "end": 1126.42,
      "text": " Yes, yes.",
      "tokens": [
        51092,
        1079,
        11,
        2086,
        13,
        51112
      ],
      "temperature": 0,
      "avg_logprob": -0.1944249251793171,
      "compression_ratio": 1.6321428571428571,
      "no_speech_prob": 1.861591176349764e-12,
      "speaker_id": 1
    },
    {
      "id": 223,
      "seek": 111148,
      "start": 1126.64,
      "end": 1127.04,
      "text": " Right.",
      "tokens": [
        51123,
        1779,
        13,
        51143
      ],
      "temperature": 0,
      "avg_logprob": -0.1944249251793171,
      "compression_ratio": 1.6321428571428571,
      "no_speech_prob": 1.861591176349764e-12,
      "speaker_id": 1
    },
    {
      "id": 224,
      "seek": 111148,
      "start": 1127.22,
      "end": 1135.8600000000001,
      "text": " Like in order to get the same privacy guarantees as this, you would have to do, you would lose way more bytes.",
      "tokens": [
        51152,
        1743,
        294,
        1668,
        281,
        483,
        264,
        912,
        11427,
        32567,
        382,
        341,
        11,
        291,
        576,
        362,
        281,
        360,
        11,
        291,
        576,
        3624,
        636,
        544,
        36088,
        13,
        51584
      ],
      "temperature": 0,
      "avg_logprob": -0.1944249251793171,
      "compression_ratio": 1.6321428571428571,
      "no_speech_prob": 1.861591176349764e-12,
      "speaker_id": 1
    },
    {
      "id": 225,
      "seek": 111148,
      "start": 1136.06,
      "end": 1139.8600000000001,
      "text": " You would have way more extra encapsulation, other things like it just would not be worth it.",
      "tokens": [
        51594,
        509,
        576,
        362,
        636,
        544,
        2857,
        38745,
        2776,
        11,
        661,
        721,
        411,
        309,
        445,
        576,
        406,
        312,
        3163,
        309,
        13,
        51784
      ],
      "temperature": 0,
      "avg_logprob": -0.1944249251793171,
      "compression_ratio": 1.6321428571428571,
      "no_speech_prob": 1.861591176349764e-12,
      "speaker_id": 1
    },
    {
      "id": 226,
      "seek": 113986,
      "start": 1139.86,
      "end": 1141.62,
      "text": " Policy signatures and everything.",
      "tokens": [
        50365,
        21708,
        32322,
        293,
        1203,
        13,
        50453
      ],
      "temperature": 0,
      "avg_logprob": -0.24809727949254654,
      "compression_ratio": 1.446808510638298,
      "no_speech_prob": 2.1643617887506794e-12,
      "speaker_id": 5
    },
    {
      "id": 227,
      "seek": 113986,
      "start": 1141.74,
      "end": 1141.8799999999999,
      "text": " Okay.",
      "tokens": [
        50459,
        1033,
        13,
        50466
      ],
      "temperature": 0,
      "avg_logprob": -0.24809727949254654,
      "compression_ratio": 1.446808510638298,
      "no_speech_prob": 2.1643617887506794e-12,
      "speaker_id": 5
    },
    {
      "id": 228,
      "seek": 113986,
      "start": 1142,
      "end": 1143.1399999999999,
      "text": " That makes perfect sense to me.",
      "tokens": [
        50472,
        663,
        1669,
        2176,
        2020,
        281,
        385,
        13,
        50529
      ],
      "temperature": 0,
      "avg_logprob": -0.24809727949254654,
      "compression_ratio": 1.446808510638298,
      "no_speech_prob": 2.1643617887506794e-12,
      "speaker_id": 5
    },
    {
      "id": 229,
      "seek": 113986,
      "start": 1143.36,
      "end": 1143.9599999999998,
      "text": " Thank you.",
      "tokens": [
        50540,
        1044,
        291,
        13,
        50570
      ],
      "temperature": 0,
      "avg_logprob": -0.24809727949254654,
      "compression_ratio": 1.446808510638298,
      "no_speech_prob": 2.1643617887506794e-12,
      "speaker_id": 5
    },
    {
      "id": 230,
      "seek": 113986,
      "start": 1144.08,
      "end": 1144.1799999999998,
      "text": " Great.",
      "tokens": [
        50576,
        3769,
        13,
        50581
      ],
      "temperature": 0,
      "avg_logprob": -0.24809727949254654,
      "compression_ratio": 1.446808510638298,
      "no_speech_prob": 2.1643617887506794e-12,
      "speaker_id": 1
    },
    {
      "id": 231,
      "seek": 113986,
      "start": 1146.26,
      "end": 1160.4199999999998,
      "text": " Also, when you are working with backend servers that are trying to build out this privacy system, forwarding along a post request works really nicely through various reverse proxies.",
      "tokens": [
        50685,
        2743,
        11,
        562,
        291,
        366,
        1364,
        365,
        38087,
        15909,
        300,
        366,
        1382,
        281,
        1322,
        484,
        341,
        11427,
        1185,
        11,
        2128,
        278,
        2051,
        257,
        2183,
        5308,
        1985,
        534,
        9594,
        807,
        3683,
        9943,
        447,
        87,
        530,
        13,
        51393
      ],
      "temperature": 0,
      "avg_logprob": -0.24809727949254654,
      "compression_ratio": 1.446808510638298,
      "no_speech_prob": 2.1643617887506794e-12,
      "speaker_id": 1
    },
    {
      "id": 232,
      "seek": 116042,
      "start": 1160.42,
      "end": 1167.04,
      "text": " And it's a lighter weight than them all having to, you know, open up TCP sockets and worry about allocating an IP address to forward your traffic.",
      "tokens": [
        50365,
        400,
        309,
        311,
        257,
        11546,
        3364,
        813,
        552,
        439,
        1419,
        281,
        11,
        291,
        458,
        11,
        1269,
        493,
        48965,
        370,
        11984,
        293,
        3292,
        466,
        12660,
        990,
        364,
        8671,
        2985,
        281,
        2128,
        428,
        6419,
        13,
        50696
      ],
      "temperature": 0,
      "avg_logprob": -0.30708502120330555,
      "compression_ratio": 1.4761904761904763,
      "no_speech_prob": 1.973662769674811e-12,
      "speaker_id": 1
    },
    {
      "id": 233,
      "seek": 116042,
      "start": 1167.64,
      "end": 1169.72,
      "text": " It works well in a server environment.",
      "tokens": [
        50726,
        467,
        1985,
        731,
        294,
        257,
        7154,
        2823,
        13,
        50830
      ],
      "temperature": 0,
      "avg_logprob": -0.30708502120330555,
      "compression_ratio": 1.4761904761904763,
      "no_speech_prob": 1.973662769674811e-12,
      "speaker_id": 1
    },
    {
      "id": 234,
      "seek": 116042,
      "start": 1171.1200000000001,
      "end": 1171.3600000000001,
      "text": " Okay.",
      "tokens": [
        50900,
        1033,
        13,
        50912
      ],
      "temperature": 0,
      "avg_logprob": -0.30708502120330555,
      "compression_ratio": 1.4761904761904763,
      "no_speech_prob": 1.973662769674811e-12,
      "speaker_id": 1
    },
    {
      "id": 235,
      "seek": 116042,
      "start": 1172.46,
      "end": 1173.5,
      "text": " So, oh, Mark.",
      "tokens": [
        50967,
        407,
        11,
        1954,
        11,
        3934,
        13,
        51019
      ],
      "temperature": 0,
      "avg_logprob": -0.30708502120330555,
      "compression_ratio": 1.4761904761904763,
      "no_speech_prob": 1.973662769674811e-12,
      "speaker_id": 1
    },
    {
      "id": 236,
      "seek": 116042,
      "start": 1173.78,
      "end": 1174.6200000000001,
      "text": " Mark Nottingham.",
      "tokens": [
        51033,
        3934,
        1726,
        783,
        4822,
        13,
        51075
      ],
      "temperature": 0,
      "avg_logprob": -0.30708502120330555,
      "compression_ratio": 1.4761904761904763,
      "no_speech_prob": 1.973662769674811e-12,
      "speaker_id": 1
    },
    {
      "id": 237,
      "seek": 116042,
      "start": 1177.2,
      "end": 1177.9,
      "text": " Where is Mark?",
      "tokens": [
        51204,
        2305,
        307,
        3934,
        30,
        51239
      ],
      "temperature": 0,
      "avg_logprob": -0.30708502120330555,
      "compression_ratio": 1.4761904761904763,
      "no_speech_prob": 1.973662769674811e-12,
      "speaker_id": 1
    },
    {
      "id": 238,
      "seek": 116042,
      "start": 1178.26,
      "end": 1178.92,
      "text": " Mark, you're up.",
      "tokens": [
        51257,
        3934,
        11,
        291,
        434,
        493,
        13,
        51290
      ],
      "temperature": 0,
      "avg_logprob": -0.30708502120330555,
      "compression_ratio": 1.4761904761904763,
      "no_speech_prob": 1.973662769674811e-12,
      "speaker_id": 6
    },
    {
      "id": 239,
      "seek": 116042,
      "start": 1179.1000000000001,
      "end": 1179.3200000000002,
      "text": " What?",
      "tokens": [
        51299,
        708,
        30,
        51310
      ],
      "temperature": 0,
      "avg_logprob": -0.30708502120330555,
      "compression_ratio": 1.4761904761904763,
      "no_speech_prob": 1.973662769674811e-12,
      "speaker_id": 1
    },
    {
      "id": 240,
      "seek": 116042,
      "start": 1179.74,
      "end": 1180.3600000000001,
      "text": " Where'd you go?",
      "tokens": [
        51331,
        2305,
        1116,
        291,
        352,
        30,
        51362
      ],
      "temperature": 0,
      "avg_logprob": -0.30708502120330555,
      "compression_ratio": 1.4761904761904763,
      "no_speech_prob": 1.973662769674811e-12,
      "speaker_id": 1
    },
    {
      "id": 241,
      "seek": 116042,
      "start": 1180.4,
      "end": 1181.1000000000001,
      "text": " You're virtual now.",
      "tokens": [
        51364,
        509,
        434,
        6374,
        586,
        13,
        51399
      ],
      "temperature": 0,
      "avg_logprob": -0.30708502120330555,
      "compression_ratio": 1.4761904761904763,
      "no_speech_prob": 1.973662769674811e-12,
      "speaker_id": 1
    },
    {
      "id": 242,
      "seek": 116042,
      "start": 1181.98,
      "end": 1182.44,
      "text": " Is that working?",
      "tokens": [
        51443,
        1119,
        300,
        1364,
        30,
        51466
      ],
      "temperature": 0,
      "avg_logprob": -0.30708502120330555,
      "compression_ratio": 1.4761904761904763,
      "no_speech_prob": 1.973662769674811e-12,
      "speaker_id": 8
    },
    {
      "id": 243,
      "seek": 116042,
      "start": 1183.1000000000001,
      "end": 1183.78,
      "text": " There we go.",
      "tokens": [
        51499,
        821,
        321,
        352,
        13,
        51533
      ],
      "temperature": 0,
      "avg_logprob": -0.30708502120330555,
      "compression_ratio": 1.4761904761904763,
      "no_speech_prob": 1.973662769674811e-12,
      "speaker_id": 6
    },
    {
      "id": 244,
      "seek": 116042,
      "start": 1183.96,
      "end": 1184.0800000000002,
      "text": " Yep.",
      "tokens": [
        51542,
        7010,
        13,
        51548
      ],
      "temperature": 0,
      "avg_logprob": -0.30708502120330555,
      "compression_ratio": 1.4761904761904763,
      "no_speech_prob": 1.973662769674811e-12,
      "speaker_id": 6
    },
    {
      "id": 245,
      "seek": 116042,
      "start": 1184.14,
      "end": 1184.42,
      "text": " Go ahead.",
      "tokens": [
        51551,
        1037,
        2286,
        13,
        51565
      ],
      "temperature": 0,
      "avg_logprob": -0.30708502120330555,
      "compression_ratio": 1.4761904761904763,
      "no_speech_prob": 1.973662769674811e-12,
      "speaker_id": 6
    },
    {
      "id": 246,
      "seek": 118442,
      "start": 1184.42,
      "end": 1184.9,
      "text": " Okay.",
      "tokens": [
        50365,
        1033,
        13,
        50389
      ],
      "temperature": 0,
      "avg_logprob": -0.18252018914706464,
      "compression_ratio": 1.4635416666666667,
      "no_speech_prob": 2.9736592962320563e-12,
      "speaker_id": 8
    },
    {
      "id": 247,
      "seek": 118442,
      "start": 1185.98,
      "end": 1188.8000000000002,
      "text": " Not coming into the last day of meetings, enthusiast.",
      "tokens": [
        50443,
        1726,
        1348,
        666,
        264,
        1036,
        786,
        295,
        8410,
        11,
        18076,
        525,
        13,
        50584
      ],
      "temperature": 0,
      "avg_logprob": -0.18252018914706464,
      "compression_ratio": 1.4635416666666667,
      "no_speech_prob": 2.9736592962320563e-12,
      "speaker_id": 8
    },
    {
      "id": 248,
      "seek": 118442,
      "start": 1190.92,
      "end": 1199.22,
      "text": " I think, Tommy, what you were just talking about should probably go into the current Ohio draft or some abstract of it.",
      "tokens": [
        50690,
        286,
        519,
        11,
        19448,
        11,
        437,
        291,
        645,
        445,
        1417,
        466,
        820,
        1391,
        352,
        666,
        264,
        2190,
        14469,
        11206,
        420,
        512,
        12649,
        295,
        309,
        13,
        51105
      ],
      "temperature": 0,
      "avg_logprob": -0.18252018914706464,
      "compression_ratio": 1.4635416666666667,
      "no_speech_prob": 2.9736592962320563e-12,
      "speaker_id": 8
    },
    {
      "id": 249,
      "seek": 118442,
      "start": 1199.38,
      "end": 1205.24,
      "text": " That was a pretty good explanation of some of the differences between mask and what we're doing here.",
      "tokens": [
        51113,
        663,
        390,
        257,
        1238,
        665,
        10835,
        295,
        512,
        295,
        264,
        7300,
        1296,
        6094,
        293,
        437,
        321,
        434,
        884,
        510,
        13,
        51406
      ],
      "temperature": 0,
      "avg_logprob": -0.18252018914706464,
      "compression_ratio": 1.4635416666666667,
      "no_speech_prob": 2.9736592962320563e-12,
      "speaker_id": 8
    },
    {
      "id": 250,
      "seek": 120524,
      "start": 1205.24,
      "end": 1211.6,
      "text": " But I got into the queue to ask what your intentions are regarding non-final responses in HTTP.",
      "tokens": [
        50365,
        583,
        286,
        658,
        666,
        264,
        18639,
        281,
        1029,
        437,
        428,
        19354,
        366,
        8595,
        2107,
        12,
        69,
        2071,
        13019,
        294,
        33283,
        13,
        50683
      ],
      "temperature": 0,
      "avg_logprob": -0.2318687079087743,
      "compression_ratio": 1.6311111111111112,
      "no_speech_prob": 1.9625364701403303e-12,
      "speaker_id": 8
    },
    {
      "id": 251,
      "seek": 120524,
      "start": 1213.86,
      "end": 1215.38,
      "text": " I missed that.",
      "tokens": [
        50796,
        286,
        6721,
        300,
        13,
        50872
      ],
      "temperature": 0,
      "avg_logprob": -0.2318687079087743,
      "compression_ratio": 1.6311111111111112,
      "no_speech_prob": 1.9625364701403303e-12,
      "speaker_id": 1
    },
    {
      "id": 252,
      "seek": 120524,
      "start": 1216.3,
      "end": 1219.38,
      "text": " What do you want to do about informational responses, non-final responses?",
      "tokens": [
        50918,
        708,
        360,
        291,
        528,
        281,
        360,
        466,
        49391,
        13019,
        11,
        2107,
        12,
        69,
        2071,
        13019,
        30,
        51072
      ],
      "temperature": 0,
      "avg_logprob": -0.2318687079087743,
      "compression_ratio": 1.6311111111111112,
      "no_speech_prob": 1.9625364701403303e-12,
      "speaker_id": 8
    },
    {
      "id": 253,
      "seek": 120524,
      "start": 1220.88,
      "end": 1221.16,
      "text": " Oh.",
      "tokens": [
        51147,
        876,
        13,
        51161
      ],
      "temperature": 0,
      "avg_logprob": -0.2318687079087743,
      "compression_ratio": 1.6311111111111112,
      "no_speech_prob": 1.9625364701403303e-12,
      "speaker_id": 1
    },
    {
      "id": 254,
      "seek": 120524,
      "start": 1222.34,
      "end": 1226.06,
      "text": " Binary HTTP already handles those.",
      "tokens": [
        51220,
        363,
        4066,
        33283,
        1217,
        18722,
        729,
        13,
        51406
      ],
      "temperature": 0,
      "avg_logprob": -0.2318687079087743,
      "compression_ratio": 1.6311111111111112,
      "no_speech_prob": 1.9625364701403303e-12,
      "speaker_id": 1
    },
    {
      "id": 255,
      "seek": 120524,
      "start": 1226.24,
      "end": 1226.38,
      "text": " Right.",
      "tokens": [
        51415,
        1779,
        13,
        51422
      ],
      "temperature": 0,
      "avg_logprob": -0.2318687079087743,
      "compression_ratio": 1.6311111111111112,
      "no_speech_prob": 1.9625364701403303e-12,
      "speaker_id": 8
    },
    {
      "id": 256,
      "seek": 120524,
      "start": 1226.44,
      "end": 1228.6200000000001,
      "text": " So, actually, that makes this work.",
      "tokens": [
        51425,
        407,
        11,
        767,
        11,
        300,
        1669,
        341,
        589,
        13,
        51534
      ],
      "temperature": 0,
      "avg_logprob": -0.2318687079087743,
      "compression_ratio": 1.6311111111111112,
      "no_speech_prob": 1.9625364701403303e-12,
      "speaker_id": 1
    },
    {
      "id": 257,
      "seek": 120524,
      "start": 1228.98,
      "end": 1230.78,
      "text": " Like, the streaming makes it work pretty well.",
      "tokens": [
        51552,
        1743,
        11,
        264,
        11791,
        1669,
        309,
        589,
        1238,
        731,
        13,
        51642
      ],
      "temperature": 0,
      "avg_logprob": -0.2318687079087743,
      "compression_ratio": 1.6311111111111112,
      "no_speech_prob": 1.9625364701403303e-12,
      "speaker_id": 1
    },
    {
      "id": 258,
      "seek": 120524,
      "start": 1231.36,
      "end": 1233.74,
      "text": " Like, if you want to do that, you kind of need this.",
      "tokens": [
        51671,
        1743,
        11,
        498,
        291,
        528,
        281,
        360,
        300,
        11,
        291,
        733,
        295,
        643,
        341,
        13,
        51790
      ],
      "temperature": 0,
      "avg_logprob": -0.2318687079087743,
      "compression_ratio": 1.6311111111111112,
      "no_speech_prob": 1.9625364701403303e-12,
      "speaker_id": 1
    },
    {
      "id": 259,
      "seek": 123374,
      "start": 1234.56,
      "end": 1234.84,
      "text": " Right.",
      "tokens": [
        50406,
        1779,
        13,
        50420
      ],
      "temperature": 0,
      "avg_logprob": -0.3118121105691661,
      "compression_ratio": 1.6186770428015564,
      "no_speech_prob": 1.781325954797186e-12,
      "speaker_id": 8
    },
    {
      "id": 260,
      "seek": 123374,
      "start": 1235.02,
      "end": 1238.34,
      "text": " Because they were excluded last time, but this one, it should be okay.",
      "tokens": [
        50429,
        1436,
        436,
        645,
        29486,
        1036,
        565,
        11,
        457,
        341,
        472,
        11,
        309,
        820,
        312,
        1392,
        13,
        50595
      ],
      "temperature": 0,
      "avg_logprob": -0.3118121105691661,
      "compression_ratio": 1.6186770428015564,
      "no_speech_prob": 1.781325954797186e-12,
      "speaker_id": 8
    },
    {
      "id": 261,
      "seek": 123374,
      "start": 1239.2,
      "end": 1239.38,
      "text": " Yeah.",
      "tokens": [
        50638,
        865,
        13,
        50647
      ],
      "temperature": 0,
      "avg_logprob": -0.3118121105691661,
      "compression_ratio": 1.6186770428015564,
      "no_speech_prob": 1.781325954797186e-12,
      "speaker_id": 1
    },
    {
      "id": 262,
      "seek": 123374,
      "start": 1239.86,
      "end": 1240.3,
      "text": " Yeah.",
      "tokens": [
        50671,
        865,
        13,
        50693
      ],
      "temperature": 0,
      "avg_logprob": -0.3118121105691661,
      "compression_ratio": 1.6186770428015564,
      "no_speech_prob": 1.781325954797186e-12,
      "speaker_id": 8
    },
    {
      "id": 263,
      "seek": 123374,
      "start": 1240.34,
      "end": 1242.2,
      "text": " That's actually another good reason for this.",
      "tokens": [
        50695,
        663,
        311,
        767,
        1071,
        665,
        1778,
        337,
        341,
        13,
        50788
      ],
      "temperature": 0,
      "avg_logprob": -0.3118121105691661,
      "compression_ratio": 1.6186770428015564,
      "no_speech_prob": 1.781325954797186e-12,
      "speaker_id": 1
    },
    {
      "id": 264,
      "seek": 123374,
      "start": 1242.4,
      "end": 1242.52,
      "text": " Yeah.",
      "tokens": [
        50798,
        865,
        13,
        50804
      ],
      "temperature": 0,
      "avg_logprob": -0.3118121105691661,
      "compression_ratio": 1.6186770428015564,
      "no_speech_prob": 1.781325954797186e-12,
      "speaker_id": 8
    },
    {
      "id": 265,
      "seek": 123374,
      "start": 1242.84,
      "end": 1243.04,
      "text": " Yeah.",
      "tokens": [
        50820,
        865,
        13,
        50830
      ],
      "temperature": 0,
      "avg_logprob": -0.3118121105691661,
      "compression_ratio": 1.6186770428015564,
      "no_speech_prob": 1.781325954797186e-12,
      "speaker_id": 1
    },
    {
      "id": 266,
      "seek": 123374,
      "start": 1243.04,
      "end": 1243.82,
      "text": " We'll mention that.",
      "tokens": [
        50830,
        492,
        603,
        2152,
        300,
        13,
        50869
      ],
      "temperature": 0,
      "avg_logprob": -0.3118121105691661,
      "compression_ratio": 1.6186770428015564,
      "no_speech_prob": 1.781325954797186e-12,
      "speaker_id": 1
    },
    {
      "id": 267,
      "seek": 123374,
      "start": 1243.9,
      "end": 1244.72,
      "text": " Thank you for bringing that up.",
      "tokens": [
        50873,
        1044,
        291,
        337,
        5062,
        300,
        493,
        13,
        50914
      ],
      "temperature": 0,
      "avg_logprob": -0.3118121105691661,
      "compression_ratio": 1.6186770428015564,
      "no_speech_prob": 1.781325954797186e-12,
      "speaker_id": 1
    },
    {
      "id": 268,
      "seek": 123374,
      "start": 1245.86,
      "end": 1245.96,
      "text": " Okay.",
      "tokens": [
        50971,
        1033,
        13,
        50976
      ],
      "temperature": 0,
      "avg_logprob": -0.3118121105691661,
      "compression_ratio": 1.6186770428015564,
      "no_speech_prob": 1.781325954797186e-12,
      "speaker_id": 8
    },
    {
      "id": 269,
      "seek": 123374,
      "start": 1247.06,
      "end": 1247.56,
      "text": " All right.",
      "tokens": [
        51031,
        1057,
        558,
        13,
        51056
      ],
      "temperature": 0,
      "avg_logprob": -0.3118121105691661,
      "compression_ratio": 1.6186770428015564,
      "no_speech_prob": 1.781325954797186e-12,
      "speaker_id": 1
    },
    {
      "id": 270,
      "seek": 123374,
      "start": 1247.56,
      "end": 1250.56,
      "text": " So, I think next we're going to talk about the actual...",
      "tokens": [
        51056,
        407,
        11,
        286,
        519,
        958,
        321,
        434,
        516,
        281,
        751,
        466,
        264,
        3539,
        485,
        51206
      ],
      "temperature": 0,
      "avg_logprob": -0.3118121105691661,
      "compression_ratio": 1.6186770428015564,
      "no_speech_prob": 1.781325954797186e-12,
      "speaker_id": 1
    },
    {
      "id": 271,
      "seek": 123374,
      "start": 1252.24,
      "end": 1252.9,
      "text": " Yes.",
      "tokens": [
        51290,
        1079,
        13,
        51323
      ],
      "temperature": 0,
      "avg_logprob": -0.3118121105691661,
      "compression_ratio": 1.6186770428015564,
      "no_speech_prob": 1.781325954797186e-12,
      "speaker_id": 1
    },
    {
      "id": 272,
      "seek": 123374,
      "start": 1253.02,
      "end": 1253.16,
      "text": " Okay.",
      "tokens": [
        51329,
        1033,
        13,
        51336
      ],
      "temperature": 0,
      "avg_logprob": -0.3118121105691661,
      "compression_ratio": 1.6186770428015564,
      "no_speech_prob": 1.781325954797186e-12,
      "speaker_id": 1
    },
    {
      "id": 273,
      "seek": 123374,
      "start": 1254.3,
      "end": 1259.56,
      "text": " So, for chunk encapsulation, the things that we need to additionally protect...",
      "tokens": [
        51393,
        407,
        11,
        337,
        16635,
        38745,
        2776,
        11,
        264,
        721,
        300,
        321,
        643,
        281,
        43181,
        2371,
        485,
        51656
      ],
      "temperature": 0,
      "avg_logprob": -0.3118121105691661,
      "compression_ratio": 1.6186770428015564,
      "no_speech_prob": 1.781325954797186e-12,
      "speaker_id": 1
    },
    {
      "id": 274,
      "seek": 123374,
      "start": 1260.34,
      "end": 1260.54,
      "text": " Oh.",
      "tokens": [
        51695,
        876,
        13,
        51705
      ],
      "temperature": 0,
      "avg_logprob": -0.3118121105691661,
      "compression_ratio": 1.6186770428015564,
      "no_speech_prob": 1.781325954797186e-12,
      "speaker_id": 1
    },
    {
      "id": 275,
      "seek": 123374,
      "start": 1260.96,
      "end": 1261.56,
      "text": " Dennis, did you want to...",
      "tokens": [
        51726,
        23376,
        11,
        630,
        291,
        528,
        281,
        485,
        51756
      ],
      "temperature": 0,
      "avg_logprob": -0.3118121105691661,
      "compression_ratio": 1.6186770428015564,
      "no_speech_prob": 1.781325954797186e-12,
      "speaker_id": 1
    },
    {
      "id": 276,
      "seek": 123374,
      "start": 1262.22,
      "end": 1263.02,
      "text": " I forgot this slide.",
      "tokens": [
        51789,
        286,
        5298,
        341,
        4137,
        13,
        51829
      ],
      "temperature": 0,
      "avg_logprob": -0.3118121105691661,
      "compression_ratio": 1.6186770428015564,
      "no_speech_prob": 1.781325954797186e-12,
      "speaker_id": 4
    },
    {
      "id": 277,
      "seek": 126302,
      "start": 1263.02,
      "end": 1264.2,
      "text": " So, you get to the end.",
      "tokens": [
        50365,
        407,
        11,
        291,
        483,
        281,
        264,
        917,
        13,
        50424
      ],
      "temperature": 0,
      "avg_logprob": -0.1797977668651636,
      "compression_ratio": 1.473053892215569,
      "no_speech_prob": 1.798661805434143e-12,
      "speaker_id": 4
    },
    {
      "id": 278,
      "seek": 126302,
      "start": 1264.66,
      "end": 1265.1,
      "text": " Okay, great.",
      "tokens": [
        50447,
        1033,
        11,
        869,
        13,
        50469
      ],
      "temperature": 0,
      "avg_logprob": -0.1797977668651636,
      "compression_ratio": 1.473053892215569,
      "no_speech_prob": 1.798661805434143e-12,
      "speaker_id": 1
    },
    {
      "id": 279,
      "seek": 126302,
      "start": 1266.34,
      "end": 1269.42,
      "text": " These are what I'm positing is the things that we additionally want to protect.",
      "tokens": [
        50531,
        1981,
        366,
        437,
        286,
        478,
        1366,
        1748,
        307,
        264,
        721,
        300,
        321,
        43181,
        528,
        281,
        2371,
        13,
        50685
      ],
      "temperature": 0,
      "avg_logprob": -0.1797977668651636,
      "compression_ratio": 1.473053892215569,
      "no_speech_prob": 1.798661805434143e-12,
      "speaker_id": 1
    },
    {
      "id": 280,
      "seek": 126302,
      "start": 1269.82,
      "end": 1274.42,
      "text": " We need to make sure that if we are chunking things, that those chunks cannot be reordered",
      "tokens": [
        50705,
        492,
        643,
        281,
        652,
        988,
        300,
        498,
        321,
        366,
        16635,
        278,
        721,
        11,
        300,
        729,
        24004,
        2644,
        312,
        319,
        765,
        4073,
        50935
      ],
      "temperature": 0,
      "avg_logprob": -0.1797977668651636,
      "compression_ratio": 1.473053892215569,
      "no_speech_prob": 1.798661805434143e-12,
      "speaker_id": 1
    },
    {
      "id": 281,
      "seek": 126302,
      "start": 1274.42,
      "end": 1277.18,
      "text": " without the receiver knowing about it.",
      "tokens": [
        50935,
        1553,
        264,
        20086,
        5276,
        466,
        309,
        13,
        51073
      ],
      "temperature": 0,
      "avg_logprob": -0.1797977668651636,
      "compression_ratio": 1.473053892215569,
      "no_speech_prob": 1.798661805434143e-12,
      "speaker_id": 1
    },
    {
      "id": 282,
      "seek": 127718,
      "start": 1277.18,
      "end": 1283.96,
      "text": " And we also need to make sure that chunks cannot be dropped either in the middle or at the end.",
      "tokens": [
        50365,
        400,
        321,
        611,
        643,
        281,
        652,
        988,
        300,
        24004,
        2644,
        312,
        8119,
        2139,
        294,
        264,
        2808,
        420,
        412,
        264,
        917,
        13,
        50704
      ],
      "temperature": 0,
      "avg_logprob": -0.12187978744506836,
      "compression_ratio": 1.1585365853658536,
      "no_speech_prob": 1.952006698641151e-12,
      "speaker_id": 1
    },
    {
      "id": 283,
      "seek": 128396,
      "start": 1283.96,
      "end": 1298.92,
      "text": " We need to know that we got to the final chunk within a single chunk of the cryptographic properties we have from HPKE and the AAD on the way back, is that we don't have to worry about a single chunk being truncated that should be detected.",
      "tokens": [
        50365,
        492,
        643,
        281,
        458,
        300,
        321,
        658,
        281,
        264,
        2572,
        16635,
        1951,
        257,
        2167,
        16635,
        295,
        264,
        9844,
        12295,
        7221,
        321,
        362,
        490,
        12557,
        8522,
        293,
        264,
        316,
        6112,
        322,
        264,
        636,
        646,
        11,
        307,
        300,
        321,
        500,
        380,
        362,
        281,
        3292,
        466,
        257,
        2167,
        16635,
        885,
        504,
        409,
        66,
        770,
        300,
        820,
        312,
        21896,
        13,
        51113
      ],
      "temperature": 0,
      "avg_logprob": -0.22049692471822102,
      "compression_ratio": 1.4545454545454546,
      "no_speech_prob": 1.8974301296825757e-12,
      "speaker_id": 1
    },
    {
      "id": 284,
      "seek": 129892,
      "start": 1298.92,
      "end": 1304.8000000000002,
      "text": " So, these, I think, are the two primary new requirements tell me why I'm wrong.",
      "tokens": [
        50365,
        407,
        11,
        613,
        11,
        286,
        519,
        11,
        366,
        264,
        732,
        6194,
        777,
        7728,
        980,
        385,
        983,
        286,
        478,
        2085,
        13,
        50659
      ],
      "temperature": 0,
      "avg_logprob": -0.44558366139729816,
      "compression_ratio": 1.0128205128205128,
      "no_speech_prob": 1.6576434235665571e-12,
      "speaker_id": 1
    },
    {
      "id": 285,
      "seek": 130480,
      "start": 1304.8,
      "end": 1307.3999999999999,
      "text": " No, I think you're right.",
      "tokens": [
        50365,
        883,
        11,
        286,
        519,
        291,
        434,
        558,
        13,
        50495
      ],
      "temperature": 0,
      "avg_logprob": -0.39584473201206755,
      "compression_ratio": 1.063157894736842,
      "no_speech_prob": 2.2206928136647708e-12,
      "speaker_id": 4
    },
    {
      "id": 286,
      "seek": 130480,
      "start": 1307.3999999999999,
      "end": 1309.2,
      "text": " Sorry, Dennis Jackson, Mozilla.",
      "tokens": [
        50495,
        4919,
        11,
        23376,
        10647,
        11,
        3335,
        26403,
        13,
        50585
      ],
      "temperature": 0,
      "avg_logprob": -0.39584473201206755,
      "compression_ratio": 1.063157894736842,
      "no_speech_prob": 2.2206928136647708e-12,
      "speaker_id": 4
    },
    {
      "id": 287,
      "seek": 130480,
      "start": 1309.82,
      "end": 1312.54,
      "text": " But I also think it's not enough by itself.",
      "tokens": [
        50616,
        583,
        286,
        611,
        519,
        309,
        311,
        406,
        1547,
        538,
        2564,
        13,
        50752
      ],
      "temperature": 0,
      "avg_logprob": -0.39584473201206755,
      "compression_ratio": 1.063157894736842,
      "no_speech_prob": 2.2206928136647708e-12,
      "speaker_id": 4
    },
    {
      "id": 288,
      "seek": 131254,
      "start": 1312.54,
      "end": 1321.54,
      "text": " So, if the client or the server are going to start processing this stream earlier on, they don't necessarily know what's in the next chunk.",
      "tokens": [
        50365,
        407,
        11,
        498,
        264,
        6423,
        420,
        264,
        7154,
        366,
        516,
        281,
        722,
        9007,
        341,
        4309,
        3071,
        322,
        11,
        436,
        500,
        380,
        4725,
        458,
        437,
        311,
        294,
        264,
        958,
        16635,
        13,
        50815
      ],
      "temperature": 0,
      "avg_logprob": -0.11656575343188118,
      "compression_ratio": 1.287037037037037,
      "no_speech_prob": 1.549334553881121e-12,
      "speaker_id": 4
    },
    {
      "id": 289,
      "seek": 132154,
      "start": 1321.54,
      "end": 1329.3799999999999,
      "text": " So, without some additional guidance about where applications cut those chunks, you're back to the same kind of truncation attack.",
      "tokens": [
        50365,
        407,
        11,
        1553,
        512,
        4497,
        10056,
        466,
        689,
        5821,
        1723,
        729,
        24004,
        11,
        291,
        434,
        646,
        281,
        264,
        912,
        733,
        295,
        504,
        409,
        46252,
        2690,
        13,
        50757
      ],
      "temperature": 0,
      "avg_logprob": -0.12837213940090603,
      "compression_ratio": 1.5862068965517242,
      "no_speech_prob": 1.646205958008573e-12,
      "speaker_id": 4
    },
    {
      "id": 290,
      "seek": 132154,
      "start": 1330.18,
      "end": 1341.3799999999999,
      "text": " So, an example might be in a situation where you can introduce extra content, like, into that stream to move the chunk boundaries, which turns out to be quite common in a lot of applications.",
      "tokens": [
        50797,
        407,
        11,
        364,
        1365,
        1062,
        312,
        294,
        257,
        2590,
        689,
        291,
        393,
        5366,
        2857,
        2701,
        11,
        411,
        11,
        666,
        300,
        4309,
        281,
        1286,
        264,
        16635,
        13180,
        11,
        597,
        4523,
        484,
        281,
        312,
        1596,
        2689,
        294,
        257,
        688,
        295,
        5821,
        13,
        51357
      ],
      "temperature": 0,
      "avg_logprob": -0.12837213940090603,
      "compression_ratio": 1.5862068965517242,
      "no_speech_prob": 1.646205958008573e-12,
      "speaker_id": 4
    },
    {
      "id": 291,
      "seek": 134138,
      "start": 1341.38,
      "end": 1355.0400000000002,
      "text": " And then, potentially, you get some kind of confusion at the application layer, because you start processing the chunk early, without having seen what's in the next chunk, which is going to change what you would have done, had you seen them both at the same time.",
      "tokens": [
        50365,
        400,
        550,
        11,
        7263,
        11,
        291,
        483,
        512,
        733,
        295,
        15075,
        412,
        264,
        3861,
        4583,
        11,
        570,
        291,
        722,
        9007,
        264,
        16635,
        2440,
        11,
        1553,
        1419,
        1612,
        437,
        311,
        294,
        264,
        958,
        16635,
        11,
        597,
        307,
        516,
        281,
        1319,
        437,
        291,
        576,
        362,
        1096,
        11,
        632,
        291,
        1612,
        552,
        1293,
        412,
        264,
        912,
        565,
        13,
        51048
      ],
      "temperature": 0,
      "avg_logprob": -0.14471505327922543,
      "compression_ratio": 1.6713615023474178,
      "no_speech_prob": 1.901514102425894e-12,
      "speaker_id": 4
    },
    {
      "id": 292,
      "seek": 134138,
      "start": 1355.98,
      "end": 1356.6000000000001,
      "text": " Does that make sense?",
      "tokens": [
        51095,
        4402,
        300,
        652,
        2020,
        30,
        51126
      ],
      "temperature": 0,
      "avg_logprob": -0.14471505327922543,
      "compression_ratio": 1.6713615023474178,
      "no_speech_prob": 1.901514102425894e-12,
      "speaker_id": 4
    },
    {
      "id": 293,
      "seek": 134138,
      "start": 1356.6000000000001,
      "end": 1360.0800000000002,
      "text": " Are you talking about this at the, essentially, the application layer?",
      "tokens": [
        51126,
        2014,
        291,
        1417,
        466,
        341,
        412,
        264,
        11,
        4476,
        11,
        264,
        3861,
        4583,
        30,
        51300
      ],
      "temperature": 0,
      "avg_logprob": -0.14471505327922543,
      "compression_ratio": 1.6713615023474178,
      "no_speech_prob": 1.901514102425894e-12,
      "speaker_id": 1
    },
    {
      "id": 294,
      "seek": 136008,
      "start": 1360.08,
      "end": 1379.56,
      "text": " Where, like, once I've decapsulated and I have the binary HTTP, is that fundamentally different from the fact that when I'm reading from an H2 stream, I don't know what's going to come up in the future, because I'm downloading an enormous file, and it's going to take several minutes.",
      "tokens": [
        50365,
        2305,
        11,
        411,
        11,
        1564,
        286,
        600,
        368,
        496,
        1878,
        6987,
        293,
        286,
        362,
        264,
        17434,
        33283,
        11,
        307,
        300,
        17879,
        819,
        490,
        264,
        1186,
        300,
        562,
        286,
        478,
        3760,
        490,
        364,
        389,
        17,
        4309,
        11,
        286,
        500,
        380,
        458,
        437,
        311,
        516,
        281,
        808,
        493,
        294,
        264,
        2027,
        11,
        570,
        286,
        478,
        32529,
        364,
        11322,
        3991,
        11,
        293,
        309,
        311,
        516,
        281,
        747,
        2940,
        2077,
        13,
        51339
      ],
      "temperature": 0,
      "avg_logprob": -0.09372658327401402,
      "compression_ratio": 1.4976958525345623,
      "no_speech_prob": 1.5941122120249895e-12,
      "speaker_id": 1
    },
    {
      "id": 295,
      "seek": 136008,
      "start": 1379.82,
      "end": 1381.22,
      "text": " Yeah, no, you're absolutely right there.",
      "tokens": [
        51352,
        865,
        11,
        572,
        11,
        291,
        434,
        3122,
        558,
        456,
        13,
        51422
      ],
      "temperature": 0,
      "avg_logprob": -0.09372658327401402,
      "compression_ratio": 1.4976958525345623,
      "no_speech_prob": 1.5941122120249895e-12,
      "speaker_id": 4
    },
    {
      "id": 296,
      "seek": 138122,
      "start": 1381.22,
      "end": 1395.16,
      "text": " But in this circumstance, because you've got that middle relay, which is trusted to know identities, but not content, you've got an attacker that's much more capable of observing those chunks and delaying chunks, or potentially, you know, fiddling around with what's going on.",
      "tokens": [
        50365,
        583,
        294,
        341,
        27640,
        11,
        570,
        291,
        600,
        658,
        300,
        2808,
        24214,
        11,
        597,
        307,
        16034,
        281,
        458,
        24239,
        11,
        457,
        406,
        2701,
        11,
        291,
        600,
        658,
        364,
        35871,
        300,
        311,
        709,
        544,
        8189,
        295,
        22107,
        729,
        24004,
        293,
        8577,
        278,
        24004,
        11,
        420,
        7263,
        11,
        291,
        458,
        11,
        283,
        14273,
        1688,
        926,
        365,
        437,
        311,
        516,
        322,
        13,
        51062
      ],
      "temperature": 0,
      "avg_logprob": -0.1071595175791595,
      "compression_ratio": 1.6688524590163933,
      "no_speech_prob": 1.6765436695181934e-12,
      "speaker_id": 4
    },
    {
      "id": 297,
      "seek": 138122,
      "start": 1395.2,
      "end": 1400.22,
      "text": " So, you've got a stronger privacy property here than what TLS ordinarily is striving to.",
      "tokens": [
        51064,
        407,
        11,
        291,
        600,
        658,
        257,
        7249,
        11427,
        4707,
        510,
        813,
        437,
        314,
        19198,
        25376,
        3289,
        307,
        36582,
        281,
        13,
        51315
      ],
      "temperature": 0,
      "avg_logprob": -0.1071595175791595,
      "compression_ratio": 1.6688524590163933,
      "no_speech_prob": 1.6765436695181934e-12,
      "speaker_id": 4
    },
    {
      "id": 298,
      "seek": 138122,
      "start": 1400.22,
      "end": 1400.74,
      "text": " I see, I see.",
      "tokens": [
        51315,
        286,
        536,
        11,
        286,
        536,
        13,
        51341
      ],
      "temperature": 0,
      "avg_logprob": -0.1071595175791595,
      "compression_ratio": 1.6688524590163933,
      "no_speech_prob": 1.6765436695181934e-12,
      "speaker_id": 1
    },
    {
      "id": 299,
      "seek": 138122,
      "start": 1401.74,
      "end": 1409.6200000000001,
      "text": " So, absolutely something we should note, do you believe there's anything beyond privacy security considerations to be done there?",
      "tokens": [
        51391,
        407,
        11,
        3122,
        746,
        321,
        820,
        3637,
        11,
        360,
        291,
        1697,
        456,
        311,
        1340,
        4399,
        11427,
        3825,
        24070,
        281,
        312,
        1096,
        456,
        30,
        51785
      ],
      "temperature": 0,
      "avg_logprob": -0.1071595175791595,
      "compression_ratio": 1.6688524590163933,
      "no_speech_prob": 1.6765436695181934e-12,
      "speaker_id": 1
    },
    {
      "id": 300,
      "seek": 140962,
      "start": 1409.62,
      "end": 1410.9599999999998,
      "text": " I mean, I can't think of anything.",
      "tokens": [
        50365,
        286,
        914,
        11,
        286,
        393,
        380,
        519,
        295,
        1340,
        13,
        50432
      ],
      "temperature": 0,
      "avg_logprob": -0.1488563338322426,
      "compression_ratio": 1.7010309278350515,
      "no_speech_prob": 1.91236870089595e-12,
      "speaker_id": 4
    },
    {
      "id": 301,
      "seek": 140962,
      "start": 1411.04,
      "end": 1417.84,
      "text": " If it's going to be general, you can't do anything better, but getting that careful wording in the draft, I think, is really important.",
      "tokens": [
        50436,
        759,
        309,
        311,
        516,
        281,
        312,
        2674,
        11,
        291,
        393,
        380,
        360,
        1340,
        1101,
        11,
        457,
        1242,
        300,
        5026,
        47602,
        294,
        264,
        11206,
        11,
        286,
        519,
        11,
        307,
        534,
        1021,
        13,
        50776
      ],
      "temperature": 0,
      "avg_logprob": -0.1488563338322426,
      "compression_ratio": 1.7010309278350515,
      "no_speech_prob": 1.91236870089595e-12,
      "speaker_id": 4
    },
    {
      "id": 302,
      "seek": 140962,
      "start": 1417.9199999999998,
      "end": 1418.54,
      "text": " Yeah, that's a good point.",
      "tokens": [
        50780,
        865,
        11,
        300,
        311,
        257,
        665,
        935,
        13,
        50811
      ],
      "temperature": 0,
      "avg_logprob": -0.1488563338322426,
      "compression_ratio": 1.7010309278350515,
      "no_speech_prob": 1.91236870089595e-12,
      "speaker_id": 1
    },
    {
      "id": 303,
      "seek": 140962,
      "start": 1418.6599999999999,
      "end": 1428.8,
      "text": " And it does, what I'm getting out of this overall is that, you know, we need to have clear text on what is suitable content for these and what is unsuitable.",
      "tokens": [
        50817,
        400,
        309,
        775,
        11,
        437,
        286,
        478,
        1242,
        484,
        295,
        341,
        4787,
        307,
        300,
        11,
        291,
        458,
        11,
        321,
        643,
        281,
        362,
        1850,
        2487,
        322,
        437,
        307,
        12873,
        2701,
        337,
        613,
        293,
        437,
        307,
        2693,
        1983,
        712,
        13,
        51324
      ],
      "temperature": 0,
      "avg_logprob": -0.1488563338322426,
      "compression_ratio": 1.7010309278350515,
      "no_speech_prob": 1.91236870089595e-12,
      "speaker_id": 1
    },
    {
      "id": 304,
      "seek": 140962,
      "start": 1428.8,
      "end": 1434.4399999999998,
      "text": " Yeah, and maybe a very clear example of how it might go wrong, because that is often more compelling than unless it's not.",
      "tokens": [
        51324,
        865,
        11,
        293,
        1310,
        257,
        588,
        1850,
        1365,
        295,
        577,
        309,
        1062,
        352,
        2085,
        11,
        570,
        300,
        307,
        2049,
        544,
        20050,
        813,
        5969,
        309,
        311,
        406,
        13,
        51606
      ],
      "temperature": 0,
      "avg_logprob": -0.1488563338322426,
      "compression_ratio": 1.7010309278350515,
      "no_speech_prob": 1.91236870089595e-12,
      "speaker_id": 4
    },
    {
      "id": 305,
      "seek": 140962,
      "start": 1434.8,
      "end": 1434.9399999999998,
      "text": " Thanks.",
      "tokens": [
        51624,
        2561,
        13,
        51631
      ],
      "temperature": 0,
      "avg_logprob": -0.1488563338322426,
      "compression_ratio": 1.7010309278350515,
      "no_speech_prob": 1.91236870089595e-12,
      "speaker_id": 1
    },
    {
      "id": 306,
      "seek": 140962,
      "start": 1436.6599999999999,
      "end": 1437.06,
      "text": " Richard.",
      "tokens": [
        51717,
        9809,
        13,
        51737
      ],
      "temperature": 0,
      "avg_logprob": -0.1488563338322426,
      "compression_ratio": 1.7010309278350515,
      "no_speech_prob": 1.91236870089595e-12,
      "speaker_id": 1
    },
    {
      "id": 307,
      "seek": 143706,
      "start": 1437.06,
      "end": 1442.84,
      "text": " Richard, Brian, speaking logically from the floor, but too late to actually get on there.",
      "tokens": [
        50365,
        9809,
        11,
        10765,
        11,
        4124,
        38887,
        490,
        264,
        4123,
        11,
        457,
        886,
        3469,
        281,
        767,
        483,
        322,
        456,
        13,
        50654
      ],
      "temperature": 0,
      "avg_logprob": -0.13182408100849874,
      "compression_ratio": 1.572139303482587,
      "no_speech_prob": 1.6749520607289847e-12,
      "speaker_id": 6
    },
    {
      "id": 308,
      "seek": 143706,
      "start": 1442.84,
      "end": 1443.1599999999999,
      "text": " Yeah, yeah, yeah.",
      "tokens": [
        50654,
        865,
        11,
        1338,
        11,
        1338,
        13,
        50670
      ],
      "temperature": 0,
      "avg_logprob": -0.13182408100849874,
      "compression_ratio": 1.572139303482587,
      "no_speech_prob": 1.6749520607289847e-12,
      "speaker_id": 1
    },
    {
      "id": 309,
      "seek": 143706,
      "start": 1443.52,
      "end": 1456.5,
      "text": " It seems like maybe one additional requirement you actually have here that I don't see on the slide is guaranteeing that all of the chunks arrive at the same server or at the same ultimate destination, right?",
      "tokens": [
        50688,
        467,
        2544,
        411,
        1310,
        472,
        4497,
        11695,
        291,
        767,
        362,
        510,
        300,
        286,
        500,
        380,
        536,
        322,
        264,
        4137,
        307,
        10815,
        278,
        300,
        439,
        295,
        264,
        24004,
        8881,
        412,
        264,
        912,
        7154,
        420,
        412,
        264,
        912,
        9705,
        12236,
        11,
        558,
        30,
        51337
      ],
      "temperature": 0,
      "avg_logprob": -0.13182408100849874,
      "compression_ratio": 1.572139303482587,
      "no_speech_prob": 1.6749520607289847e-12,
      "speaker_id": 6
    },
    {
      "id": 310,
      "seek": 145650,
      "start": 1456.5,
      "end": 1463.96,
      "text": " Because if there's something processing this and, you know, there are multiple instances of the same, as Ted was saying, for ECMP, right?",
      "tokens": [
        50365,
        1436,
        498,
        456,
        311,
        746,
        9007,
        341,
        293,
        11,
        291,
        458,
        11,
        456,
        366,
        3866,
        14519,
        295,
        264,
        912,
        11,
        382,
        14985,
        390,
        1566,
        11,
        337,
        19081,
        12224,
        11,
        558,
        30,
        50738
      ],
      "temperature": 0,
      "avg_logprob": -0.0967336654663086,
      "compression_ratio": 1.170940170940171,
      "no_speech_prob": 1.7961525279261426e-12,
      "speaker_id": 6
    },
    {
      "id": 311,
      "seek": 146396,
      "start": 1463.96,
      "end": 1470.18,
      "text": " With the existing uncorrelated requests, different OHI messages can land on different servers and there's no problem.",
      "tokens": [
        50365,
        2022,
        264,
        6741,
        6219,
        284,
        12004,
        12475,
        11,
        819,
        13931,
        40,
        7897,
        393,
        2117,
        322,
        819,
        15909,
        293,
        456,
        311,
        572,
        1154,
        13,
        50676
      ],
      "temperature": 0,
      "avg_logprob": -0.1586742877960205,
      "compression_ratio": 1.3884297520661157,
      "no_speech_prob": 1.902749008700355e-12,
      "speaker_id": 6
    },
    {
      "id": 312,
      "seek": 146396,
      "start": 1470.52,
      "end": 1472.9,
      "text": " And that's not the case for different chunks here.",
      "tokens": [
        50693,
        400,
        300,
        311,
        406,
        264,
        1389,
        337,
        819,
        24004,
        510,
        13,
        50812
      ],
      "temperature": 0,
      "avg_logprob": -0.1586742877960205,
      "compression_ratio": 1.3884297520661157,
      "no_speech_prob": 1.902749008700355e-12,
      "speaker_id": 6
    },
    {
      "id": 313,
      "seek": 147290,
      "start": 1472.9,
      "end": 1473.3400000000001,
      "text": " True.",
      "tokens": [
        50365,
        13587,
        13,
        50387
      ],
      "temperature": 0,
      "avg_logprob": -0.3156108209642313,
      "compression_ratio": 1.5138888888888888,
      "no_speech_prob": 1.8453634887533044e-12,
      "speaker_id": 1
    },
    {
      "id": 314,
      "seek": 147290,
      "start": 1474.96,
      "end": 1475.3600000000001,
      "text": " True.",
      "tokens": [
        50468,
        13587,
        13,
        50488
      ],
      "temperature": 0,
      "avg_logprob": -0.3156108209642313,
      "compression_ratio": 1.5138888888888888,
      "no_speech_prob": 1.8453634887533044e-12,
      "speaker_id": 1
    },
    {
      "id": 315,
      "seek": 147290,
      "start": 1476.8200000000002,
      "end": 1488.8600000000001,
      "text": " I think the requirements, which I did not clearly state who these requirements are for, I was thinking of these as for the receiver in order to verify the integrity of the data.",
      "tokens": [
        50561,
        286,
        519,
        264,
        7728,
        11,
        597,
        286,
        630,
        406,
        4448,
        1785,
        567,
        613,
        7728,
        366,
        337,
        11,
        286,
        390,
        1953,
        295,
        613,
        382,
        337,
        264,
        20086,
        294,
        1668,
        281,
        16888,
        264,
        16000,
        295,
        264,
        1412,
        13,
        51163
      ],
      "temperature": 0,
      "avg_logprob": -0.3156108209642313,
      "compression_ratio": 1.5138888888888888,
      "no_speech_prob": 1.8453634887533044e-12,
      "speaker_id": 1
    },
    {
      "id": 316,
      "seek": 147290,
      "start": 1489.3000000000002,
      "end": 1489.3200000000002,
      "text": " Yeah.",
      "tokens": [
        51185,
        865,
        13,
        51186
      ],
      "temperature": 0,
      "avg_logprob": -0.3156108209642313,
      "compression_ratio": 1.5138888888888888,
      "no_speech_prob": 1.8453634887533044e-12,
      "speaker_id": 6
    },
    {
      "id": 317,
      "seek": 147290,
      "start": 1489.64,
      "end": 1490.3600000000001,
      "text": " Nothing about the set.",
      "tokens": [
        51202,
        6693,
        466,
        264,
        992,
        13,
        51238
      ],
      "temperature": 0,
      "avg_logprob": -0.3156108209642313,
      "compression_ratio": 1.5138888888888888,
      "no_speech_prob": 1.8453634887533044e-12,
      "speaker_id": 1
    },
    {
      "id": 318,
      "seek": 149036,
      "start": 1490.36,
      "end": 1494.58,
      "text": " And I think there may not be anything to do in the protocol to assure that the property I mentioned.",
      "tokens": [
        50365,
        400,
        286,
        519,
        456,
        815,
        406,
        312,
        1340,
        281,
        360,
        294,
        264,
        10336,
        281,
        20968,
        300,
        264,
        4707,
        286,
        2835,
        13,
        50576
      ],
      "temperature": 0,
      "avg_logprob": -0.17022977901410452,
      "compression_ratio": 1.58,
      "no_speech_prob": 2.150864772745842e-12,
      "speaker_id": 6
    },
    {
      "id": 319,
      "seek": 149036,
      "start": 1494.6999999999998,
      "end": 1495.1599999999999,
      "text": " That's a great point.",
      "tokens": [
        50582,
        663,
        311,
        257,
        869,
        935,
        13,
        50605
      ],
      "temperature": 0,
      "avg_logprob": -0.17022977901410452,
      "compression_ratio": 1.58,
      "no_speech_prob": 2.150864772745842e-12,
      "speaker_id": 1
    },
    {
      "id": 320,
      "seek": 149036,
      "start": 1495.28,
      "end": 1498.34,
      "text": " But it may be something you mentioned in deployment considerations, operation considerations.",
      "tokens": [
        50611,
        583,
        309,
        815,
        312,
        746,
        291,
        2835,
        294,
        19317,
        24070,
        11,
        6916,
        24070,
        13,
        50764
      ],
      "temperature": 0,
      "avg_logprob": -0.17022977901410452,
      "compression_ratio": 1.58,
      "no_speech_prob": 2.150864772745842e-12,
      "speaker_id": 6
    },
    {
      "id": 321,
      "seek": 149036,
      "start": 1498.58,
      "end": 1503.84,
      "text": " Well, one of the failure modes here is your chunks get misrouted and you have a very sad day.",
      "tokens": [
        50776,
        1042,
        11,
        472,
        295,
        264,
        7763,
        14068,
        510,
        307,
        428,
        24004,
        483,
        3346,
        81,
        346,
        292,
        293,
        291,
        362,
        257,
        588,
        4227,
        786,
        13,
        51039
      ],
      "temperature": 0,
      "avg_logprob": -0.17022977901410452,
      "compression_ratio": 1.58,
      "no_speech_prob": 2.150864772745842e-12,
      "speaker_id": 1
    },
    {
      "id": 322,
      "seek": 149036,
      "start": 1504.08,
      "end": 1504.2199999999998,
      "text": " Yeah.",
      "tokens": [
        51051,
        865,
        13,
        51058
      ],
      "temperature": 0,
      "avg_logprob": -0.17022977901410452,
      "compression_ratio": 1.58,
      "no_speech_prob": 2.150864772745842e-12,
      "speaker_id": 6
    },
    {
      "id": 323,
      "seek": 150422,
      "start": 1504.22,
      "end": 1516.14,
      "text": " But I had been musing on your last slide as to whether you had mentioned the server beginning to process the request while it was still inbound.",
      "tokens": [
        50365,
        583,
        286,
        632,
        668,
        1038,
        278,
        322,
        428,
        1036,
        4137,
        382,
        281,
        1968,
        291,
        632,
        2835,
        264,
        7154,
        2863,
        281,
        1399,
        264,
        5308,
        1339,
        309,
        390,
        920,
        294,
        18767,
        13,
        50961
      ],
      "temperature": 0,
      "avg_logprob": -0.1454550015029087,
      "compression_ratio": 1.7417840375586855,
      "no_speech_prob": 2.248037259816593e-12,
      "speaker_id": 6
    },
    {
      "id": 324,
      "seek": 150422,
      "start": 1516.54,
      "end": 1523.96,
      "text": " I had been musing on the question of whether the server was allowed to begin responding to the request while the request was still arriving.",
      "tokens": [
        50981,
        286,
        632,
        668,
        1038,
        278,
        322,
        264,
        1168,
        295,
        1968,
        264,
        7154,
        390,
        4350,
        281,
        1841,
        16670,
        281,
        264,
        5308,
        1339,
        264,
        5308,
        390,
        920,
        22436,
        13,
        51352
      ],
      "temperature": 0,
      "avg_logprob": -0.1454550015029087,
      "compression_ratio": 1.7417840375586855,
      "no_speech_prob": 2.248037259816593e-12,
      "speaker_id": 6
    },
    {
      "id": 325,
      "seek": 150422,
      "start": 1524.66,
      "end": 1525.52,
      "text": " And it sounds like...",
      "tokens": [
        51387,
        400,
        309,
        3263,
        411,
        485,
        51430
      ],
      "temperature": 0,
      "avg_logprob": -0.1454550015029087,
      "compression_ratio": 1.7417840375586855,
      "no_speech_prob": 2.248037259816593e-12,
      "speaker_id": 6
    },
    {
      "id": 326,
      "seek": 150422,
      "start": 1525.52,
      "end": 1527.58,
      "text": " As a property of HTTP, it is.",
      "tokens": [
        51430,
        1018,
        257,
        4707,
        295,
        33283,
        11,
        309,
        307,
        13,
        51533
      ],
      "temperature": 0,
      "avg_logprob": -0.1454550015029087,
      "compression_ratio": 1.7417840375586855,
      "no_speech_prob": 2.248037259816593e-12,
      "speaker_id": 1
    },
    {
      "id": 327,
      "seek": 150422,
      "start": 1528.06,
      "end": 1529.46,
      "text": " Yeah, no, I realize that's legal.",
      "tokens": [
        51557,
        865,
        11,
        572,
        11,
        286,
        4325,
        300,
        311,
        5089,
        13,
        51627
      ],
      "temperature": 0,
      "avg_logprob": -0.1454550015029087,
      "compression_ratio": 1.7417840375586855,
      "no_speech_prob": 2.248037259816593e-12,
      "speaker_id": 6
    },
    {
      "id": 328,
      "seek": 152946,
      "start": 1529.46,
      "end": 1536.98,
      "text": " But if you're concerned about these properties, then maybe you want to have some sort of cryptographic interlock between the request and the response that would prevent that.",
      "tokens": [
        50365,
        583,
        498,
        291,
        434,
        5922,
        466,
        613,
        7221,
        11,
        550,
        1310,
        291,
        528,
        281,
        362,
        512,
        1333,
        295,
        9844,
        12295,
        728,
        4102,
        1296,
        264,
        5308,
        293,
        264,
        4134,
        300,
        576,
        4871,
        300,
        13,
        50741
      ],
      "temperature": 0,
      "avg_logprob": -0.22258150142474767,
      "compression_ratio": 1.6091205211726385,
      "no_speech_prob": 2.1687645169327086e-12,
      "speaker_id": 6
    },
    {
      "id": 329,
      "seek": 152946,
      "start": 1538.06,
      "end": 1538.74,
      "text": " I don't...",
      "tokens": [
        50795,
        286,
        500,
        380,
        485,
        50829
      ],
      "temperature": 0,
      "avg_logprob": -0.22258150142474767,
      "compression_ratio": 1.6091205211726385,
      "no_speech_prob": 2.1687645169327086e-12,
      "speaker_id": 1
    },
    {
      "id": 330,
      "seek": 152946,
      "start": 1538.74,
      "end": 1543.3400000000001,
      "text": " My initial response is, I don't believe we need it, but that's something we should discuss more.",
      "tokens": [
        50829,
        1222,
        5883,
        4134,
        307,
        11,
        286,
        500,
        380,
        1697,
        321,
        643,
        309,
        11,
        457,
        300,
        311,
        746,
        321,
        820,
        2248,
        544,
        13,
        51059
      ],
      "temperature": 0,
      "avg_logprob": -0.22258150142474767,
      "compression_ratio": 1.6091205211726385,
      "no_speech_prob": 2.1687645169327086e-12,
      "speaker_id": 1
    },
    {
      "id": 331,
      "seek": 152946,
      "start": 1543.56,
      "end": 1543.6200000000001,
      "text": " Okay.",
      "tokens": [
        51070,
        1033,
        13,
        51073
      ],
      "temperature": 0,
      "avg_logprob": -0.22258150142474767,
      "compression_ratio": 1.6091205211726385,
      "no_speech_prob": 2.1687645169327086e-12,
      "speaker_id": 6
    },
    {
      "id": 332,
      "seek": 152946,
      "start": 1544.8,
      "end": 1545.74,
      "text": " Who's after Richard?",
      "tokens": [
        51132,
        2102,
        311,
        934,
        9809,
        30,
        51179
      ],
      "temperature": 0,
      "avg_logprob": -0.22258150142474767,
      "compression_ratio": 1.6091205211726385,
      "no_speech_prob": 2.1687645169327086e-12,
      "speaker_id": 1
    },
    {
      "id": 333,
      "seek": 152946,
      "start": 1546.7,
      "end": 1547.52,
      "text": " Jonathan, I'm next.",
      "tokens": [
        51227,
        15471,
        11,
        286,
        478,
        958,
        13,
        51268
      ],
      "temperature": 0,
      "avg_logprob": -0.22258150142474767,
      "compression_ratio": 1.6091205211726385,
      "no_speech_prob": 2.1687645169327086e-12,
      "speaker_id": 6
    },
    {
      "id": 334,
      "seek": 152946,
      "start": 1547.66,
      "end": 1548.2,
      "text": " Jonathan Hoyland.",
      "tokens": [
        51275,
        15471,
        28664,
        1661,
        13,
        51302
      ],
      "temperature": 0,
      "avg_logprob": -0.22258150142474767,
      "compression_ratio": 1.6091205211726385,
      "no_speech_prob": 2.1687645169327086e-12,
      "speaker_id": 1
    },
    {
      "id": 335,
      "seek": 152946,
      "start": 1548.42,
      "end": 1549.64,
      "text": " Jonathan Hoyland, Cloud Player.",
      "tokens": [
        51313,
        15471,
        28664,
        1661,
        11,
        8061,
        24920,
        13,
        51374
      ],
      "temperature": 0,
      "avg_logprob": -0.22258150142474767,
      "compression_ratio": 1.6091205211726385,
      "no_speech_prob": 2.1687645169327086e-12,
      "speaker_id": 2
    },
    {
      "id": 336,
      "seek": 152946,
      "start": 1550.56,
      "end": 1554.56,
      "text": " There's also the trivial one, i.e. you shouldn't allow insertions.",
      "tokens": [
        51420,
        821,
        311,
        611,
        264,
        26703,
        472,
        11,
        741,
        13,
        68,
        13,
        291,
        4659,
        380,
        2089,
        8969,
        626,
        13,
        51620
      ],
      "temperature": 0,
      "avg_logprob": -0.22258150142474767,
      "compression_ratio": 1.6091205211726385,
      "no_speech_prob": 2.1687645169327086e-12,
      "speaker_id": 2
    },
    {
      "id": 337,
      "seek": 152946,
      "start": 1555.58,
      "end": 1556.24,
      "text": " Thank you.",
      "tokens": [
        51671,
        1044,
        291,
        13,
        51704
      ],
      "temperature": 0,
      "avg_logprob": -0.22258150142474767,
      "compression_ratio": 1.6091205211726385,
      "no_speech_prob": 2.1687645169327086e-12,
      "speaker_id": 1
    },
    {
      "id": 338,
      "seek": 152946,
      "start": 1556.24,
      "end": 1559.1200000000001,
      "text": " Yeah, like I think that's already...",
      "tokens": [
        51704,
        865,
        11,
        411,
        286,
        519,
        300,
        311,
        1217,
        485,
        51848
      ],
      "temperature": 0,
      "avg_logprob": -0.22258150142474767,
      "compression_ratio": 1.6091205211726385,
      "no_speech_prob": 2.1687645169327086e-12,
      "speaker_id": 2
    },
    {
      "id": 339,
      "seek": 155912,
      "start": 1559.12,
      "end": 1562.1399999999999,
      "text": " It's already handled, but that's one of the requirements.",
      "tokens": [
        50365,
        467,
        311,
        1217,
        18033,
        11,
        457,
        300,
        311,
        472,
        295,
        264,
        7728,
        13,
        50516
      ],
      "temperature": 0,
      "avg_logprob": -0.2321219481239023,
      "compression_ratio": 1.7117437722419928,
      "no_speech_prob": 2.4009407743191824e-12,
      "speaker_id": 1
    },
    {
      "id": 340,
      "seek": 155912,
      "start": 1562.32,
      "end": 1563.1399999999999,
      "text": " Yes, that's fantastic.",
      "tokens": [
        50525,
        1079,
        11,
        300,
        311,
        5456,
        13,
        50566
      ],
      "temperature": 0,
      "avg_logprob": -0.2321219481239023,
      "compression_ratio": 1.7117437722419928,
      "no_speech_prob": 2.4009407743191824e-12,
      "speaker_id": 1
    },
    {
      "id": 341,
      "seek": 155912,
      "start": 1563.26,
      "end": 1563.8,
      "text": " Thank you so much.",
      "tokens": [
        50572,
        1044,
        291,
        370,
        709,
        13,
        50599
      ],
      "temperature": 0,
      "avg_logprob": -0.2321219481239023,
      "compression_ratio": 1.7117437722419928,
      "no_speech_prob": 2.4009407743191824e-12,
      "speaker_id": 1
    },
    {
      "id": 342,
      "seek": 155912,
      "start": 1566.2399999999998,
      "end": 1566.4399999999998,
      "text": " Yeah.",
      "tokens": [
        50721,
        865,
        13,
        50731
      ],
      "temperature": 0,
      "avg_logprob": -0.2321219481239023,
      "compression_ratio": 1.7117437722419928,
      "no_speech_prob": 2.4009407743191824e-12,
      "speaker_id": 1
    },
    {
      "id": 343,
      "seek": 155912,
      "start": 1566.7199999999998,
      "end": 1570.54,
      "text": " Yeah, cool, but sorry, one hail of a question while I'm listening to the discussion.",
      "tokens": [
        50745,
        865,
        11,
        1627,
        11,
        457,
        2597,
        11,
        472,
        38157,
        295,
        257,
        1168,
        1339,
        286,
        478,
        4764,
        281,
        264,
        5017,
        13,
        50936
      ],
      "temperature": 0,
      "avg_logprob": -0.2321219481239023,
      "compression_ratio": 1.7117437722419928,
      "no_speech_prob": 2.4009407743191824e-12,
      "speaker_id": 9
    },
    {
      "id": 344,
      "seek": 155912,
      "start": 1570.7399999999998,
      "end": 1575.3999999999999,
      "text": " And maybe I don't understand the use case well enough, and maybe I don't understand the non-existing solution you're proposing well enough.",
      "tokens": [
        50946,
        400,
        1310,
        286,
        500,
        380,
        1223,
        264,
        764,
        1389,
        731,
        1547,
        11,
        293,
        1310,
        286,
        500,
        380,
        1223,
        264,
        2107,
        12,
        36447,
        3827,
        291,
        434,
        29939,
        731,
        1547,
        13,
        51179
      ],
      "temperature": 0,
      "avg_logprob": -0.2321219481239023,
      "compression_ratio": 1.7117437722419928,
      "no_speech_prob": 2.4009407743191824e-12,
      "speaker_id": 9
    },
    {
      "id": 345,
      "seek": 155912,
      "start": 1576.28,
      "end": 1577.2199999999998,
      "text": " But it sounds like...",
      "tokens": [
        51223,
        583,
        309,
        3263,
        411,
        485,
        51270
      ],
      "temperature": 0,
      "avg_logprob": -0.2321219481239023,
      "compression_ratio": 1.7117437722419928,
      "no_speech_prob": 2.4009407743191824e-12,
      "speaker_id": 9
    },
    {
      "id": 346,
      "seek": 155912,
      "start": 1577.2199999999998,
      "end": 1577.4399999999998,
      "text": " It's just here.",
      "tokens": [
        51270,
        467,
        311,
        445,
        510,
        13,
        51281
      ],
      "temperature": 0,
      "avg_logprob": -0.2321219481239023,
      "compression_ratio": 1.7117437722419928,
      "no_speech_prob": 2.4009407743191824e-12,
      "speaker_id": 1
    },
    {
      "id": 347,
      "seek": 155912,
      "start": 1577.8,
      "end": 1578.04,
      "text": " Yes.",
      "tokens": [
        51299,
        1079,
        13,
        51311
      ],
      "temperature": 0,
      "avg_logprob": -0.2321219481239023,
      "compression_ratio": 1.7117437722419928,
      "no_speech_prob": 2.4009407743191824e-12,
      "speaker_id": 9
    },
    {
      "id": 348,
      "seek": 155912,
      "start": 1578.52,
      "end": 1584.36,
      "text": " It sounds like there are applications who actually can provide small and bigger chunks on their own, right?",
      "tokens": [
        51335,
        467,
        3263,
        411,
        456,
        366,
        5821,
        567,
        767,
        393,
        2893,
        1359,
        293,
        3801,
        24004,
        322,
        641,
        1065,
        11,
        558,
        30,
        51627
      ],
      "temperature": 0,
      "avg_logprob": -0.2321219481239023,
      "compression_ratio": 1.7117437722419928,
      "no_speech_prob": 2.4009407743191824e-12,
      "speaker_id": 9
    },
    {
      "id": 349,
      "seek": 158436,
      "start": 1584.36,
      "end": 1587.06,
      "text": " So I'm wondering, is that actually the right layer to provide this functionality?",
      "tokens": [
        50365,
        407,
        286,
        478,
        6359,
        11,
        307,
        300,
        767,
        264,
        558,
        4583,
        281,
        2893,
        341,
        14980,
        30,
        50500
      ],
      "temperature": 0,
      "avg_logprob": -0.13220581175789,
      "compression_ratio": 1.4438502673796791,
      "no_speech_prob": 2.1236690791320845e-12,
      "speaker_id": 9
    },
    {
      "id": 350,
      "seek": 158436,
      "start": 1587.32,
      "end": 1591.1,
      "text": " Is this something that the upper layer can do anyway in a lot of cases already?",
      "tokens": [
        50513,
        1119,
        341,
        746,
        300,
        264,
        6597,
        4583,
        393,
        360,
        4033,
        294,
        257,
        688,
        295,
        3331,
        1217,
        30,
        50702
      ],
      "temperature": 0,
      "avg_logprob": -0.13220581175789,
      "compression_ratio": 1.4438502673796791,
      "no_speech_prob": 2.1236690791320845e-12,
      "speaker_id": 9
    },
    {
      "id": 351,
      "seek": 158436,
      "start": 1591.58,
      "end": 1600.34,
      "text": " I mean, I think the fundamental problem with the existing OHDP is that it allows exactly one chunk entirely.",
      "tokens": [
        50726,
        286,
        914,
        11,
        286,
        519,
        264,
        8088,
        1154,
        365,
        264,
        6741,
        13931,
        11373,
        307,
        300,
        309,
        4045,
        2293,
        472,
        16635,
        7696,
        13,
        51164
      ],
      "temperature": 0,
      "avg_logprob": -0.13220581175789,
      "compression_ratio": 1.4438502673796791,
      "no_speech_prob": 2.1236690791320845e-12,
      "speaker_id": 1
    },
    {
      "id": 352,
      "seek": 160034,
      "start": 1600.34,
      "end": 1606,
      "text": " So it means you must have the complete request or the complete response before you can process any one byte of it.",
      "tokens": [
        50365,
        407,
        309,
        1355,
        291,
        1633,
        362,
        264,
        3566,
        5308,
        420,
        264,
        3566,
        4134,
        949,
        291,
        393,
        1399,
        604,
        472,
        40846,
        295,
        309,
        13,
        50648
      ],
      "temperature": 0,
      "avg_logprob": -0.16994947316695233,
      "compression_ratio": 1.4452554744525548,
      "no_speech_prob": 1.6007774533005614e-12,
      "speaker_id": 1
    },
    {
      "id": 353,
      "seek": 160034,
      "start": 1607.22,
      "end": 1610.76,
      "text": " And so as long as that's not a problem, we should continue to use the existing one.",
      "tokens": [
        50709,
        400,
        370,
        382,
        938,
        382,
        300,
        311,
        406,
        257,
        1154,
        11,
        321,
        820,
        2354,
        281,
        764,
        264,
        6741,
        472,
        13,
        50886
      ],
      "temperature": 0,
      "avg_logprob": -0.16994947316695233,
      "compression_ratio": 1.4452554744525548,
      "no_speech_prob": 1.6007774533005614e-12,
      "speaker_id": 1
    },
    {
      "id": 354,
      "seek": 161076,
      "start": 1610.76,
      "end": 1619.52,
      "text": " But if I can just put my requests and replies into really small chunks, I can process them one by one on the application layer if I can.",
      "tokens": [
        50365,
        583,
        498,
        286,
        393,
        445,
        829,
        452,
        12475,
        293,
        42289,
        666,
        534,
        1359,
        24004,
        11,
        286,
        393,
        1399,
        552,
        472,
        538,
        472,
        322,
        264,
        3861,
        4583,
        498,
        286,
        393,
        13,
        50803
      ],
      "temperature": 0,
      "avg_logprob": -0.122711181640625,
      "compression_ratio": 1.7269503546099292,
      "no_speech_prob": 2.0851629884549583e-12,
      "speaker_id": 9
    },
    {
      "id": 355,
      "seek": 161076,
      "start": 1619.78,
      "end": 1631.1,
      "text": " Well, no, but with the layer going inside OHDP, you could chunk them up very, very small, but then the other side would have to wait for everything to arrive before it could even decrypt a single byte.",
      "tokens": [
        50816,
        1042,
        11,
        572,
        11,
        457,
        365,
        264,
        4583,
        516,
        1854,
        13931,
        11373,
        11,
        291,
        727,
        16635,
        552,
        493,
        588,
        11,
        588,
        1359,
        11,
        457,
        550,
        264,
        661,
        1252,
        576,
        362,
        281,
        1699,
        337,
        1203,
        281,
        8881,
        949,
        309,
        727,
        754,
        979,
        627,
        662,
        257,
        2167,
        40846,
        13,
        51382
      ],
      "temperature": 0,
      "avg_logprob": -0.122711181640625,
      "compression_ratio": 1.7269503546099292,
      "no_speech_prob": 2.0851629884549583e-12,
      "speaker_id": 1
    },
    {
      "id": 356,
      "seek": 161076,
      "start": 1632.5,
      "end": 1635.84,
      "text": " No, I'm talking about chunking them up in different requests.",
      "tokens": [
        51452,
        883,
        11,
        286,
        478,
        1417,
        466,
        16635,
        278,
        552,
        493,
        294,
        819,
        12475,
        13,
        51619
      ],
      "temperature": 0,
      "avg_logprob": -0.122711181640625,
      "compression_ratio": 1.7269503546099292,
      "no_speech_prob": 2.0851629884549583e-12,
      "speaker_id": 9
    },
    {
      "id": 357,
      "seek": 161076,
      "start": 1635.8799999999999,
      "end": 1638.42,
      "text": " In separate requests, but the separate requests are completely non-correlatable.",
      "tokens": [
        51621,
        682,
        4994,
        12475,
        11,
        457,
        264,
        4994,
        12475,
        366,
        2584,
        2107,
        12,
        19558,
        4419,
        31415,
        13,
        51748
      ],
      "temperature": 0,
      "avg_logprob": -0.122711181640625,
      "compression_ratio": 1.7269503546099292,
      "no_speech_prob": 2.0851629884549583e-12,
      "speaker_id": 1
    },
    {
      "id": 358,
      "seek": 161076,
      "start": 1638.82,
      "end": 1639.02,
      "text": " Yeah.",
      "tokens": [
        51768,
        865,
        13,
        51778
      ],
      "temperature": 0,
      "avg_logprob": -0.122711181640625,
      "compression_ratio": 1.7269503546099292,
      "no_speech_prob": 2.0851629884549583e-12,
      "speaker_id": 9
    },
    {
      "id": 359,
      "seek": 163902,
      "start": 1639.02,
      "end": 1639.42,
      "text": " Right.",
      "tokens": [
        50365,
        1779,
        13,
        50385
      ],
      "temperature": 0,
      "avg_logprob": -0.18223987926136365,
      "compression_ratio": 1.7216494845360826,
      "no_speech_prob": 2.0030509369217686e-12,
      "speaker_id": 1
    },
    {
      "id": 360,
      "seek": 163902,
      "start": 1639.62,
      "end": 1642.4,
      "text": " So I mean, I need some logic on the higher.",
      "tokens": [
        50395,
        407,
        286,
        914,
        11,
        286,
        643,
        512,
        9952,
        322,
        264,
        2946,
        13,
        50534
      ],
      "temperature": 0,
      "avg_logprob": -0.18223987926136365,
      "compression_ratio": 1.7216494845360826,
      "no_speech_prob": 2.0030509369217686e-12,
      "speaker_id": 9
    },
    {
      "id": 361,
      "seek": 163902,
      "start": 1642.58,
      "end": 1648.36,
      "text": " Yes, I mean, I guess if I need to download a giant file, I could have, you know, what is the patch request?",
      "tokens": [
        50543,
        1079,
        11,
        286,
        914,
        11,
        286,
        2041,
        498,
        286,
        643,
        281,
        5484,
        257,
        7410,
        3991,
        11,
        286,
        727,
        362,
        11,
        291,
        458,
        11,
        437,
        307,
        264,
        9972,
        5308,
        30,
        50832
      ],
      "temperature": 0,
      "avg_logprob": -0.18223987926136365,
      "compression_ratio": 1.7216494845360826,
      "no_speech_prob": 2.0030509369217686e-12,
      "speaker_id": 1
    },
    {
      "id": 362,
      "seek": 163902,
      "start": 1648.44,
      "end": 1654.68,
      "text": " I mean, like to say, like, you know, give me bytes one through 10, then bytes 10 through 20 as like separate requests.",
      "tokens": [
        50836,
        286,
        914,
        11,
        411,
        281,
        584,
        11,
        411,
        11,
        291,
        458,
        11,
        976,
        385,
        36088,
        472,
        807,
        1266,
        11,
        550,
        36088,
        1266,
        807,
        945,
        382,
        411,
        4994,
        12475,
        13,
        51148
      ],
      "temperature": 0,
      "avg_logprob": -0.18223987926136365,
      "compression_ratio": 1.7216494845360826,
      "no_speech_prob": 2.0030509369217686e-12,
      "speaker_id": 1
    },
    {
      "id": 363,
      "seek": 163902,
      "start": 1655.8,
      "end": 1657.3799999999999,
      "text": " But that seems a bit.",
      "tokens": [
        51204,
        583,
        300,
        2544,
        257,
        857,
        13,
        51283
      ],
      "temperature": 0,
      "avg_logprob": -0.18223987926136365,
      "compression_ratio": 1.7216494845360826,
      "no_speech_prob": 2.0030509369217686e-12,
      "speaker_id": 1
    },
    {
      "id": 364,
      "seek": 163902,
      "start": 1658.04,
      "end": 1665.78,
      "text": " I mean, what I'm saying at the beginning, it's like, I'm not sure I understand the use case well enough to understand if that is the right solution or if there's a different solution.",
      "tokens": [
        51316,
        286,
        914,
        11,
        437,
        286,
        478,
        1566,
        412,
        264,
        2863,
        11,
        309,
        311,
        411,
        11,
        286,
        478,
        406,
        988,
        286,
        1223,
        264,
        764,
        1389,
        731,
        1547,
        281,
        1223,
        498,
        300,
        307,
        264,
        558,
        3827,
        420,
        498,
        456,
        311,
        257,
        819,
        3827,
        13,
        51703
      ],
      "temperature": 0,
      "avg_logprob": -0.18223987926136365,
      "compression_ratio": 1.7216494845360826,
      "no_speech_prob": 2.0030509369217686e-12,
      "speaker_id": 9
    },
    {
      "id": 365,
      "seek": 163902,
      "start": 1665.78,
      "end": 1666.3,
      "text": " Okay.",
      "tokens": [
        51703,
        1033,
        13,
        51729
      ],
      "temperature": 0,
      "avg_logprob": -0.18223987926136365,
      "compression_ratio": 1.7216494845360826,
      "no_speech_prob": 2.0030509369217686e-12,
      "speaker_id": 1
    },
    {
      "id": 366,
      "seek": 163902,
      "start": 1667.62,
      "end": 1668.34,
      "text": " Who's next?",
      "tokens": [
        51795,
        2102,
        311,
        958,
        30,
        51831
      ],
      "temperature": 0,
      "avg_logprob": -0.18223987926136365,
      "compression_ratio": 1.7216494845360826,
      "no_speech_prob": 2.0030509369217686e-12,
      "speaker_id": 1
    },
    {
      "id": 367,
      "seek": 166902,
      "start": 1669.02,
      "end": 1669.46,
      "text": " Hector.",
      "tokens": [
        50365,
        389,
        20814,
        13,
        50387
      ],
      "temperature": 0,
      "avg_logprob": -0.49036643084357767,
      "compression_ratio": 1.6535433070866141,
      "no_speech_prob": 2.2379147146533995e-12,
      "speaker_id": 6
    },
    {
      "id": 368,
      "seek": 166902,
      "start": 1669.46,
      "end": 1669.8,
      "text": " Hector.",
      "tokens": [
        50387,
        389,
        20814,
        13,
        50404
      ],
      "temperature": 0,
      "avg_logprob": -0.49036643084357767,
      "compression_ratio": 1.6535433070866141,
      "no_speech_prob": 2.2379147146533995e-12,
      "speaker_id": 6
    },
    {
      "id": 369,
      "seek": 166902,
      "start": 1669.8,
      "end": 1669.82,
      "text": " Hector.",
      "tokens": [
        50404,
        389,
        20814,
        13,
        50405
      ],
      "temperature": 0,
      "avg_logprob": -0.49036643084357767,
      "compression_ratio": 1.6535433070866141,
      "no_speech_prob": 2.2379147146533995e-12,
      "speaker_id": 6
    },
    {
      "id": 370,
      "seek": 166902,
      "start": 1669.82,
      "end": 1670.4,
      "text": " Hector.",
      "tokens": [
        50405,
        389,
        20814,
        13,
        50434
      ],
      "temperature": 0,
      "avg_logprob": -0.49036643084357767,
      "compression_ratio": 1.6535433070866141,
      "no_speech_prob": 2.2379147146533995e-12,
      "speaker_id": 1
    },
    {
      "id": 371,
      "seek": 166902,
      "start": 1670.4,
      "end": 1670.5,
      "text": " Hector.",
      "tokens": [
        50434,
        389,
        20814,
        13,
        50439
      ],
      "temperature": 0,
      "avg_logprob": -0.49036643084357767,
      "compression_ratio": 1.6535433070866141,
      "no_speech_prob": 2.2379147146533995e-12,
      "speaker_id": 1
    },
    {
      "id": 372,
      "seek": 166902,
      "start": 1674.94,
      "end": 1686.2,
      "text": " So we have like some handshake and then that we like start with and then like we start streaming this data and that seems like we're kind of triumphantly reinventing TLS.",
      "tokens": [
        50661,
        407,
        321,
        362,
        411,
        512,
        2377,
        34593,
        293,
        550,
        300,
        321,
        411,
        722,
        365,
        293,
        550,
        411,
        321,
        722,
        11791,
        341,
        1412,
        293,
        300,
        2544,
        411,
        321,
        434,
        733,
        295,
        1376,
        449,
        15071,
        356,
        33477,
        278,
        314,
        19198,
        13,
        51224
      ],
      "temperature": 0,
      "avg_logprob": -0.49036643084357767,
      "compression_ratio": 1.6535433070866141,
      "no_speech_prob": 2.2379147146533995e-12,
      "speaker_id": 10
    },
    {
      "id": 373,
      "seek": 168620,
      "start": 1686.2,
      "end": 1688.8400000000001,
      "text": " It's a little concerning, frankly.",
      "tokens": [
        50365,
        467,
        311,
        257,
        707,
        18087,
        11,
        11939,
        13,
        50497
      ],
      "temperature": 0,
      "avg_logprob": -0.48520930608113605,
      "compression_ratio": 0.8095238095238095,
      "no_speech_prob": 1.9012779631927268e-12,
      "speaker_id": 10
    },
    {
      "id": 374,
      "seek": 168884,
      "start": 1688.84,
      "end": 1703.22,
      "text": " So like the rationale behind HTTP was that you wanted to have things that were like too expensive to like tee up the TLS handshake and you want anyone to correlate across things.",
      "tokens": [
        50365,
        407,
        411,
        264,
        41989,
        2261,
        33283,
        390,
        300,
        291,
        1415,
        281,
        362,
        721,
        300,
        645,
        411,
        886,
        5124,
        281,
        411,
        33863,
        493,
        264,
        314,
        19198,
        2377,
        34593,
        293,
        291,
        528,
        2878,
        281,
        48742,
        2108,
        721,
        13,
        51084
      ],
      "temperature": 0,
      "avg_logprob": -0.18483346373170287,
      "compression_ratio": 1.577092511013216,
      "no_speech_prob": 1.6712332472773594e-12,
      "speaker_id": 10
    },
    {
      "id": 375,
      "seek": 168884,
      "start": 1703.6,
      "end": 1706.3799999999999,
      "text": " And so I'm like a little trying to figure out like why this isn't called mask.",
      "tokens": [
        51103,
        400,
        370,
        286,
        478,
        411,
        257,
        707,
        1382,
        281,
        2573,
        484,
        411,
        983,
        341,
        1943,
        380,
        1219,
        6094,
        13,
        51242
      ],
      "temperature": 0,
      "avg_logprob": -0.18483346373170287,
      "compression_ratio": 1.577092511013216,
      "no_speech_prob": 1.6712332472773594e-12,
      "speaker_id": 10
    },
    {
      "id": 376,
      "seek": 168884,
      "start": 1707.58,
      "end": 1707.6599999999999,
      "text": " Right.",
      "tokens": [
        51302,
        1779,
        13,
        51306
      ],
      "temperature": 0,
      "avg_logprob": -0.18483346373170287,
      "compression_ratio": 1.577092511013216,
      "no_speech_prob": 1.6712332472773594e-12,
      "speaker_id": 1
    },
    {
      "id": 377,
      "seek": 168884,
      "start": 1707.74,
      "end": 1708.86,
      "text": " So you missed the earlier discussion.",
      "tokens": [
        51310,
        407,
        291,
        6721,
        264,
        3071,
        5017,
        13,
        51366
      ],
      "temperature": 0,
      "avg_logprob": -0.18483346373170287,
      "compression_ratio": 1.577092511013216,
      "no_speech_prob": 1.6712332472773594e-12,
      "speaker_id": 1
    },
    {
      "id": 378,
      "seek": 168884,
      "start": 1708.9599999999998,
      "end": 1709.1399999999999,
      "text": " I did.",
      "tokens": [
        51371,
        286,
        630,
        13,
        51380
      ],
      "temperature": 0,
      "avg_logprob": -0.18483346373170287,
      "compression_ratio": 1.577092511013216,
      "no_speech_prob": 1.6712332472773594e-12,
      "speaker_id": 10
    },
    {
      "id": 379,
      "seek": 168884,
      "start": 1709.24,
      "end": 1712.12,
      "text": " Which we have for several minutes on this topic.",
      "tokens": [
        51385,
        3013,
        321,
        362,
        337,
        2940,
        2077,
        322,
        341,
        4829,
        13,
        51529
      ],
      "temperature": 0,
      "avg_logprob": -0.18483346373170287,
      "compression_ratio": 1.577092511013216,
      "no_speech_prob": 1.6712332472773594e-12,
      "speaker_id": 1
    },
    {
      "id": 380,
      "seek": 171212,
      "start": 1712.12,
      "end": 1712.28,
      "text": " Okay.",
      "tokens": [
        50365,
        1033,
        13,
        50373
      ],
      "temperature": 0,
      "avg_logprob": -0.4065559985590916,
      "compression_ratio": 1.1694915254237288,
      "no_speech_prob": 2.1971322330949228e-12,
      "speaker_id": 1
    },
    {
      "id": 381,
      "seek": 171212,
      "start": 1712.62,
      "end": 1713.9599999999998,
      "text": " I'll be happy to take it to the list.",
      "tokens": [
        50390,
        286,
        603,
        312,
        2055,
        281,
        747,
        309,
        281,
        264,
        1329,
        13,
        50457
      ],
      "temperature": 0,
      "avg_logprob": -0.4065559985590916,
      "compression_ratio": 1.1694915254237288,
      "no_speech_prob": 2.1971322330949228e-12,
      "speaker_id": 10
    },
    {
      "id": 382,
      "seek": 171212,
      "start": 1716.02,
      "end": 1716.3799999999999,
      "text": " Okay.",
      "tokens": [
        50560,
        1033,
        13,
        50578
      ],
      "temperature": 0,
      "avg_logprob": -0.4065559985590916,
      "compression_ratio": 1.1694915254237288,
      "no_speech_prob": 2.1971322330949228e-12,
      "speaker_id": 1
    },
    {
      "id": 383,
      "seek": 171212,
      "start": 1716.62,
      "end": 1716.76,
      "text": " Sure.",
      "tokens": [
        50590,
        4894,
        13,
        50597
      ],
      "temperature": 0,
      "avg_logprob": -0.4065559985590916,
      "compression_ratio": 1.1694915254237288,
      "no_speech_prob": 2.1971322330949228e-12,
      "speaker_id": 10
    },
    {
      "id": 384,
      "seek": 171212,
      "start": 1718.04,
      "end": 1718.3999999999999,
      "text": " Yeah.",
      "tokens": [
        50661,
        865,
        13,
        50679
      ],
      "temperature": 0,
      "avg_logprob": -0.4065559985590916,
      "compression_ratio": 1.1694915254237288,
      "no_speech_prob": 2.1971322330949228e-12,
      "speaker_id": 1
    },
    {
      "id": 385,
      "seek": 171212,
      "start": 1718.4599999999998,
      "end": 1724.7399999999998,
      "text": " The TLDR, I mean, essentially is, this is a performance improvement on that.",
      "tokens": [
        50682,
        440,
        40277,
        9301,
        11,
        286,
        914,
        11,
        4476,
        307,
        11,
        341,
        307,
        257,
        3389,
        10444,
        322,
        300,
        13,
        50996
      ],
      "temperature": 0,
      "avg_logprob": -0.4065559985590916,
      "compression_ratio": 1.1694915254237288,
      "no_speech_prob": 2.1971322330949228e-12,
      "speaker_id": 1
    },
    {
      "id": 386,
      "seek": 172474,
      "start": 1724.74,
      "end": 1741.56,
      "text": " Like, yes, you could do a full TLS handshake and then do exactly, and set up an HTTP session and then do exactly one request over it and then tear the whole thing down in order to get the same privacy properties as this through a forward proxy.",
      "tokens": [
        50365,
        1743,
        11,
        2086,
        11,
        291,
        727,
        360,
        257,
        1577,
        314,
        19198,
        2377,
        34593,
        293,
        550,
        360,
        2293,
        11,
        293,
        992,
        493,
        364,
        33283,
        5481,
        293,
        550,
        360,
        2293,
        472,
        5308,
        670,
        309,
        293,
        550,
        12556,
        264,
        1379,
        551,
        760,
        294,
        1668,
        281,
        483,
        264,
        912,
        11427,
        7221,
        382,
        341,
        807,
        257,
        2128,
        29690,
        13,
        51206
      ],
      "temperature": 0,
      "avg_logprob": -0.09514706710289264,
      "compression_ratio": 1.5061728395061729,
      "no_speech_prob": 2.47254799100316e-12,
      "speaker_id": 1
    },
    {
      "id": 387,
      "seek": 174156,
      "start": 1741.56,
      "end": 1756.02,
      "text": " Um, but that's far more back and forth than just saying, I, I'm just going to use HPK's existing ability to, you know, chunk up the data into two chunks if it's being slow and driving it.",
      "tokens": [
        50365,
        3301,
        11,
        457,
        300,
        311,
        1400,
        544,
        646,
        293,
        5220,
        813,
        445,
        1566,
        11,
        286,
        11,
        286,
        478,
        445,
        516,
        281,
        764,
        12557,
        42,
        311,
        6741,
        3485,
        281,
        11,
        291,
        458,
        11,
        16635,
        493,
        264,
        1412,
        666,
        732,
        24004,
        498,
        309,
        311,
        885,
        2964,
        293,
        4840,
        309,
        13,
        51088
      ],
      "temperature": 0,
      "avg_logprob": -0.33243614918476827,
      "compression_ratio": 1.3411764705882352,
      "no_speech_prob": 2.072668425778801e-12,
      "speaker_id": 1
    },
    {
      "id": 388,
      "seek": 174156,
      "start": 1759.76,
      "end": 1760.28,
      "text": " Yeah.",
      "tokens": [
        51275,
        865,
        13,
        51301
      ],
      "temperature": 0,
      "avg_logprob": -0.33243614918476827,
      "compression_ratio": 1.3411764705882352,
      "no_speech_prob": 2.072668425778801e-12,
      "speaker_id": 10
    },
    {
      "id": 389,
      "seek": 174156,
      "start": 1760.6799999999998,
      "end": 1761.2,
      "text": " Two.",
      "tokens": [
        51321,
        4453,
        13,
        51347
      ],
      "temperature": 0,
      "avg_logprob": -0.33243614918476827,
      "compression_ratio": 1.3411764705882352,
      "no_speech_prob": 2.072668425778801e-12,
      "speaker_id": 10
    },
    {
      "id": 390,
      "seek": 174156,
      "start": 1761.7,
      "end": 1762.1599999999999,
      "text": " Yes.",
      "tokens": [
        51372,
        1079,
        13,
        51395
      ],
      "temperature": 0,
      "avg_logprob": -0.33243614918476827,
      "compression_ratio": 1.3411764705882352,
      "no_speech_prob": 2.072668425778801e-12,
      "speaker_id": 10
    },
    {
      "id": 391,
      "seek": 174156,
      "start": 1762.22,
      "end": 1764.3,
      "text": " Now five, 50, a million.",
      "tokens": [
        51398,
        823,
        1732,
        11,
        2625,
        11,
        257,
        2459,
        13,
        51502
      ],
      "temperature": 0,
      "avg_logprob": -0.33243614918476827,
      "compression_ratio": 1.3411764705882352,
      "no_speech_prob": 2.072668425778801e-12,
      "speaker_id": 10
    },
    {
      "id": 392,
      "seek": 176430,
      "start": 1764.3,
      "end": 1771.78,
      "text": " Like, I guess what I'm observing is you're like verging into recreating a TLS in pieces.",
      "tokens": [
        50365,
        1743,
        11,
        286,
        2041,
        437,
        286,
        478,
        22107,
        307,
        291,
        434,
        411,
        1306,
        3249,
        666,
        850,
        44613,
        257,
        314,
        19198,
        294,
        3755,
        13,
        50739
      ],
      "temperature": 0,
      "avg_logprob": -0.19675898551940918,
      "compression_ratio": 1.3333333333333333,
      "no_speech_prob": 2.4218671772907596e-12,
      "speaker_id": 10
    },
    {
      "id": 393,
      "seek": 176430,
      "start": 1772.46,
      "end": 1782.1599999999999,
      "text": " I mean, fundamentally, like what we're doing with HPK and OHCP to begin with is, you know, just choosing to send one chunk.",
      "tokens": [
        50773,
        286,
        914,
        11,
        17879,
        11,
        411,
        437,
        321,
        434,
        884,
        365,
        12557,
        42,
        293,
        13931,
        20049,
        281,
        1841,
        365,
        307,
        11,
        291,
        458,
        11,
        445,
        10875,
        281,
        2845,
        472,
        16635,
        13,
        51258
      ],
      "temperature": 0,
      "avg_logprob": -0.19675898551940918,
      "compression_ratio": 1.3333333333333333,
      "no_speech_prob": 2.4218671772907596e-12,
      "speaker_id": 1
    },
    {
      "id": 394,
      "seek": 178216,
      "start": 1782.16,
      "end": 1804.68,
      "text": " Um, I, I don't see why going, just expanding that to use several more chunks means we would want to then entirely switch over to a different protocol here where we need to now set up a stream state.",
      "tokens": [
        50365,
        3301,
        11,
        286,
        11,
        286,
        500,
        380,
        536,
        983,
        516,
        11,
        445,
        14702,
        300,
        281,
        764,
        2940,
        544,
        24004,
        1355,
        321,
        576,
        528,
        281,
        550,
        7696,
        3679,
        670,
        281,
        257,
        819,
        10336,
        510,
        689,
        321,
        643,
        281,
        586,
        992,
        493,
        257,
        4309,
        1785,
        13,
        51491
      ],
      "temperature": 0,
      "avg_logprob": -0.1638277326311384,
      "compression_ratio": 1.47,
      "no_speech_prob": 2.1435782836254358e-12,
      "speaker_id": 1
    },
    {
      "id": 395,
      "seek": 178216,
      "start": 1804.68,
      "end": 1809.9,
      "text": " And this is the way that you gradually reproduce every feature of TLS by one feature at a time.",
      "tokens": [
        51491,
        400,
        341,
        307,
        264,
        636,
        300,
        291,
        13145,
        29501,
        633,
        4111,
        295,
        314,
        19198,
        538,
        472,
        4111,
        412,
        257,
        565,
        13,
        51752
      ],
      "temperature": 0,
      "avg_logprob": -0.1638277326311384,
      "compression_ratio": 1.47,
      "no_speech_prob": 2.1435782836254358e-12,
      "speaker_id": 10
    },
    {
      "id": 396,
      "seek": 181216,
      "start": 1812.16,
      "end": 1817.96,
      "text": " Like, I, I guess, I guess I understand what you're saying and like that marginal, that argument doesn't make sense in the margins.",
      "tokens": [
        50365,
        1743,
        11,
        286,
        11,
        286,
        2041,
        11,
        286,
        2041,
        286,
        1223,
        437,
        291,
        434,
        1566,
        293,
        411,
        300,
        16885,
        11,
        300,
        6770,
        1177,
        380,
        652,
        2020,
        294,
        264,
        30317,
        13,
        50655
      ],
      "temperature": 0,
      "avg_logprob": -0.1284144093671183,
      "compression_ratio": 1.731060606060606,
      "no_speech_prob": 2.2690475800363208e-12,
      "speaker_id": 10
    },
    {
      "id": 397,
      "seek": 181216,
      "start": 1817.96,
      "end": 1820.3000000000002,
      "text": " And I'm trying to ask you to look at the general equilibrium rather than the margin.",
      "tokens": [
        50655,
        400,
        286,
        478,
        1382,
        281,
        1029,
        291,
        281,
        574,
        412,
        264,
        2674,
        15625,
        2831,
        813,
        264,
        10270,
        13,
        50772
      ],
      "temperature": 0,
      "avg_logprob": -0.1284144093671183,
      "compression_ratio": 1.731060606060606,
      "no_speech_prob": 2.2690475800363208e-12,
      "speaker_id": 10
    },
    {
      "id": 398,
      "seek": 181216,
      "start": 1821.02,
      "end": 1821.42,
      "text": " Right.",
      "tokens": [
        50808,
        1779,
        13,
        50828
      ],
      "temperature": 0,
      "avg_logprob": -0.1284144093671183,
      "compression_ratio": 1.731060606060606,
      "no_speech_prob": 2.2690475800363208e-12,
      "speaker_id": 1
    },
    {
      "id": 399,
      "seek": 181216,
      "start": 1821.48,
      "end": 1825.74,
      "text": " But I mean, also like, you know, in, in the case of mass, let's say I have a forward proxy.",
      "tokens": [
        50831,
        583,
        286,
        914,
        11,
        611,
        411,
        11,
        291,
        458,
        11,
        294,
        11,
        294,
        264,
        1389,
        295,
        2758,
        11,
        718,
        311,
        584,
        286,
        362,
        257,
        2128,
        29690,
        13,
        51044
      ],
      "temperature": 0,
      "avg_logprob": -0.1284144093671183,
      "compression_ratio": 1.731060606060606,
      "no_speech_prob": 2.2690475800363208e-12,
      "speaker_id": 1
    },
    {
      "id": 400,
      "seek": 181216,
      "start": 1826.22,
      "end": 1836.68,
      "text": " So, uh, you know, that would require that forward proxy to, you know, set up what, you know, some TCP or UDP forwarding state to the next hop.",
      "tokens": [
        51068,
        407,
        11,
        2232,
        11,
        291,
        458,
        11,
        300,
        576,
        3651,
        300,
        2128,
        29690,
        281,
        11,
        291,
        458,
        11,
        992,
        493,
        437,
        11,
        291,
        458,
        11,
        512,
        48965,
        420,
        624,
        11373,
        2128,
        278,
        1785,
        281,
        264,
        958,
        3818,
        13,
        51591
      ],
      "temperature": 0,
      "avg_logprob": -0.1284144093671183,
      "compression_ratio": 1.731060606060606,
      "no_speech_prob": 2.2690475800363208e-12,
      "speaker_id": 1
    },
    {
      "id": 401,
      "seek": 183668,
      "start": 1836.68,
      "end": 1841.8600000000001,
      "text": " And if I'm, you know, bouncing this around, like that is a lot of infrastructure and IP allocation.",
      "tokens": [
        50365,
        400,
        498,
        286,
        478,
        11,
        291,
        458,
        11,
        27380,
        341,
        926,
        11,
        411,
        300,
        307,
        257,
        688,
        295,
        6896,
        293,
        8671,
        27599,
        13,
        50624
      ],
      "temperature": 0,
      "avg_logprob": -0.14431650885220232,
      "compression_ratio": 1.6415770609318996,
      "no_speech_prob": 1.584226565036484e-12,
      "speaker_id": 1
    },
    {
      "id": 402,
      "seek": 183668,
      "start": 1841.8600000000001,
      "end": 1844.44,
      "text": " Whereas here it's, you know, just more of like a reverse proxy.",
      "tokens": [
        50624,
        13813,
        510,
        309,
        311,
        11,
        291,
        458,
        11,
        445,
        544,
        295,
        411,
        257,
        9943,
        29690,
        13,
        50753
      ],
      "temperature": 0,
      "avg_logprob": -0.14431650885220232,
      "compression_ratio": 1.6415770609318996,
      "no_speech_prob": 1.584226565036484e-12,
      "speaker_id": 1
    },
    {
      "id": 403,
      "seek": 183668,
      "start": 1844.5600000000002,
      "end": 1846.68,
      "text": " Like I just send a post request with my message.",
      "tokens": [
        50759,
        1743,
        286,
        445,
        2845,
        257,
        2183,
        5308,
        365,
        452,
        3636,
        13,
        50865
      ],
      "temperature": 0,
      "avg_logprob": -0.14431650885220232,
      "compression_ratio": 1.6415770609318996,
      "no_speech_prob": 1.584226565036484e-12,
      "speaker_id": 1
    },
    {
      "id": 404,
      "seek": 183668,
      "start": 1847.04,
      "end": 1851.42,
      "text": " It just forwards it along through whatever chain it needs to and sends it back to a different layer here.",
      "tokens": [
        50883,
        467,
        445,
        30126,
        309,
        2051,
        807,
        2035,
        5021,
        309,
        2203,
        281,
        293,
        14790,
        309,
        646,
        281,
        257,
        819,
        4583,
        510,
        13,
        51102
      ],
      "temperature": 0,
      "avg_logprob": -0.14431650885220232,
      "compression_ratio": 1.6415770609318996,
      "no_speech_prob": 1.584226565036484e-12,
      "speaker_id": 1
    },
    {
      "id": 405,
      "seek": 183668,
      "start": 1851.8200000000002,
      "end": 1854.42,
      "text": " And to repeat myself, yes, that sounds like a sensible argument at the margin.",
      "tokens": [
        51122,
        400,
        281,
        7149,
        2059,
        11,
        2086,
        11,
        300,
        3263,
        411,
        257,
        25380,
        6770,
        412,
        264,
        10270,
        13,
        51252
      ],
      "temperature": 0,
      "avg_logprob": -0.14431650885220232,
      "compression_ratio": 1.6415770609318996,
      "no_speech_prob": 1.584226565036484e-12,
      "speaker_id": 10
    },
    {
      "id": 406,
      "seek": 183668,
      "start": 1854.64,
      "end": 1856.96,
      "text": " And yet the end game is you've reproduced the game protocol.",
      "tokens": [
        51263,
        400,
        1939,
        264,
        917,
        1216,
        307,
        291,
        600,
        11408,
        1232,
        264,
        1216,
        10336,
        13,
        51379
      ],
      "temperature": 0,
      "avg_logprob": -0.14431650885220232,
      "compression_ratio": 1.6415770609318996,
      "no_speech_prob": 1.584226565036484e-12,
      "speaker_id": 10
    },
    {
      "id": 407,
      "seek": 185696,
      "start": 1856.96,
      "end": 1863.28,
      "text": " Um, so, um, okay, well, I guess, is the end game of this a call for adoption?",
      "tokens": [
        50365,
        3301,
        11,
        370,
        11,
        1105,
        11,
        1392,
        11,
        731,
        11,
        286,
        2041,
        11,
        307,
        264,
        917,
        1216,
        295,
        341,
        257,
        818,
        337,
        19215,
        30,
        50681
      ],
      "temperature": 0,
      "avg_logprob": -0.28067651342173094,
      "compression_ratio": 1.631578947368421,
      "no_speech_prob": 1.8835785795673354e-12,
      "speaker_id": 10
    },
    {
      "id": 408,
      "seek": 185696,
      "start": 1863.44,
      "end": 1864.24,
      "text": " Because I guess I just talked about it.",
      "tokens": [
        50689,
        1436,
        286,
        2041,
        286,
        445,
        2825,
        466,
        309,
        13,
        50729
      ],
      "temperature": 0,
      "avg_logprob": -0.28067651342173094,
      "compression_ratio": 1.631578947368421,
      "no_speech_prob": 1.8835785795673354e-12,
      "speaker_id": 10
    },
    {
      "id": 409,
      "seek": 185696,
      "start": 1864.24,
      "end": 1871.54,
      "text": " No, no, the end game is talk about what should go into a zero, zero draft, individual draft to even have that discussion further.",
      "tokens": [
        50729,
        883,
        11,
        572,
        11,
        264,
        917,
        1216,
        307,
        751,
        466,
        437,
        820,
        352,
        666,
        257,
        4018,
        11,
        4018,
        11206,
        11,
        2609,
        11206,
        281,
        754,
        362,
        300,
        5017,
        3052,
        13,
        51094
      ],
      "temperature": 0,
      "avg_logprob": -0.28067651342173094,
      "compression_ratio": 1.631578947368421,
      "no_speech_prob": 1.8835785795673354e-12,
      "speaker_id": 1
    },
    {
      "id": 410,
      "seek": 185696,
      "start": 1872.6000000000001,
      "end": 1872.96,
      "text": " Okay.",
      "tokens": [
        51147,
        1033,
        13,
        51165
      ],
      "temperature": 0,
      "avg_logprob": -0.28067651342173094,
      "compression_ratio": 1.631578947368421,
      "no_speech_prob": 1.8835785795673354e-12,
      "speaker_id": 10
    },
    {
      "id": 411,
      "seek": 185696,
      "start": 1873.54,
      "end": 1873.7,
      "text": " Yeah.",
      "tokens": [
        51194,
        865,
        13,
        51202
      ],
      "temperature": 0,
      "avg_logprob": -0.28067651342173094,
      "compression_ratio": 1.631578947368421,
      "no_speech_prob": 1.8835785795673354e-12,
      "speaker_id": 1
    },
    {
      "id": 412,
      "seek": 185696,
      "start": 1873.9,
      "end": 1874.1200000000001,
      "text": " Yeah.",
      "tokens": [
        51212,
        865,
        13,
        51223
      ],
      "temperature": 0,
      "avg_logprob": -0.28067651342173094,
      "compression_ratio": 1.631578947368421,
      "no_speech_prob": 1.8835785795673354e-12,
      "speaker_id": 7
    },
    {
      "id": 413,
      "seek": 185696,
      "start": 1874.16,
      "end": 1877.8600000000001,
      "text": " We want to see if there's interest and see if folks are interested in working on it.",
      "tokens": [
        51225,
        492,
        528,
        281,
        536,
        498,
        456,
        311,
        1179,
        293,
        536,
        498,
        4024,
        366,
        3102,
        294,
        1364,
        322,
        309,
        13,
        51410
      ],
      "temperature": 0,
      "avg_logprob": -0.28067651342173094,
      "compression_ratio": 1.631578947368421,
      "no_speech_prob": 1.8835785795673354e-12,
      "speaker_id": 7
    },
    {
      "id": 414,
      "seek": 185696,
      "start": 1878.1000000000001,
      "end": 1878.28,
      "text": " And then.",
      "tokens": [
        51422,
        400,
        550,
        13,
        51431
      ],
      "temperature": 0,
      "avg_logprob": -0.28067651342173094,
      "compression_ratio": 1.631578947368421,
      "no_speech_prob": 1.8835785795673354e-12,
      "speaker_id": 1
    },
    {
      "id": 415,
      "seek": 185696,
      "start": 1878.32,
      "end": 1879.4,
      "text": " And see if there are obvious holes.",
      "tokens": [
        51433,
        400,
        536,
        498,
        456,
        366,
        6322,
        8118,
        13,
        51487
      ],
      "temperature": 0,
      "avg_logprob": -0.28067651342173094,
      "compression_ratio": 1.631578947368421,
      "no_speech_prob": 1.8835785795673354e-12,
      "speaker_id": 1
    },
    {
      "id": 416,
      "seek": 185696,
      "start": 1879.9,
      "end": 1880.04,
      "text": " Right.",
      "tokens": [
        51512,
        1779,
        13,
        51519
      ],
      "temperature": 0,
      "avg_logprob": -0.28067651342173094,
      "compression_ratio": 1.631578947368421,
      "no_speech_prob": 1.8835785795673354e-12,
      "speaker_id": 7
    },
    {
      "id": 417,
      "seek": 188004,
      "start": 1880.04,
      "end": 1888.26,
      "text": " Um, I guess one question I had Tommy was, um, um, do you have like a list of use cases that, um, that need this right now?",
      "tokens": [
        50365,
        3301,
        11,
        286,
        2041,
        472,
        1168,
        286,
        632,
        19448,
        390,
        11,
        1105,
        11,
        1105,
        11,
        360,
        291,
        362,
        411,
        257,
        1329,
        295,
        764,
        3331,
        300,
        11,
        1105,
        11,
        300,
        643,
        341,
        558,
        586,
        30,
        50776
      ],
      "temperature": 0,
      "avg_logprob": -0.22594808603261973,
      "compression_ratio": 1.7589928057553956,
      "no_speech_prob": 1.7456715326741312e-12,
      "speaker_id": 7
    },
    {
      "id": 418,
      "seek": 188004,
      "start": 1888.26,
      "end": 1891.82,
      "text": " Or, uh, I've described the properties of the use cases, right?",
      "tokens": [
        50776,
        1610,
        11,
        2232,
        11,
        286,
        600,
        7619,
        264,
        7221,
        295,
        264,
        764,
        3331,
        11,
        558,
        30,
        50954
      ],
      "temperature": 0,
      "avg_logprob": -0.22594808603261973,
      "compression_ratio": 1.7589928057553956,
      "no_speech_prob": 1.7456715326741312e-12,
      "speaker_id": 1
    },
    {
      "id": 419,
      "seek": 188004,
      "start": 1891.82,
      "end": 1898.26,
      "text": " But did you have like, um, so is there any, I guess right now, are there use cases that, um, you think would benefit from this?",
      "tokens": [
        50954,
        583,
        630,
        291,
        362,
        411,
        11,
        1105,
        11,
        370,
        307,
        456,
        604,
        11,
        286,
        2041,
        558,
        586,
        11,
        366,
        456,
        764,
        3331,
        300,
        11,
        1105,
        11,
        291,
        519,
        576,
        5121,
        490,
        341,
        30,
        51276
      ],
      "temperature": 0,
      "avg_logprob": -0.22594808603261973,
      "compression_ratio": 1.7589928057553956,
      "no_speech_prob": 1.7456715326741312e-12,
      "speaker_id": 7
    },
    {
      "id": 420,
      "seek": 188004,
      "start": 1898.26,
      "end": 1900.86,
      "text": " Because we just had some chat about a chatter about that in the chat.",
      "tokens": [
        51276,
        1436,
        321,
        445,
        632,
        512,
        5081,
        466,
        257,
        26929,
        466,
        300,
        294,
        264,
        5081,
        13,
        51406
      ],
      "temperature": 0,
      "avg_logprob": -0.22594808603261973,
      "compression_ratio": 1.7589928057553956,
      "no_speech_prob": 1.7456715326741312e-12,
      "speaker_id": 7
    },
    {
      "id": 421,
      "seek": 188004,
      "start": 1901.04,
      "end": 1901.46,
      "text": " Yes.",
      "tokens": [
        51415,
        1079,
        13,
        51436
      ],
      "temperature": 0,
      "avg_logprob": -0.22594808603261973,
      "compression_ratio": 1.7589928057553956,
      "no_speech_prob": 1.7456715326741312e-12,
      "speaker_id": 1
    },
    {
      "id": 422,
      "seek": 188004,
      "start": 1901.6,
      "end": 1901.78,
      "text": " Okay.",
      "tokens": [
        51443,
        1033,
        13,
        51452
      ],
      "temperature": 0,
      "avg_logprob": -0.22594808603261973,
      "compression_ratio": 1.7589928057553956,
      "no_speech_prob": 1.7456715326741312e-12,
      "speaker_id": 7
    },
    {
      "id": 423,
      "seek": 188004,
      "start": 1902.02,
      "end": 1903.62,
      "text": " Um, yeah, I'm not sure.",
      "tokens": [
        51464,
        3301,
        11,
        1338,
        11,
        286,
        478,
        406,
        988,
        13,
        51544
      ],
      "temperature": 0,
      "avg_logprob": -0.22594808603261973,
      "compression_ratio": 1.7589928057553956,
      "no_speech_prob": 1.7456715326741312e-12,
      "speaker_id": 1
    },
    {
      "id": 424,
      "seek": 188004,
      "start": 1903.62,
      "end": 1904.8799999999999,
      "text": " I can't get into the details of them.",
      "tokens": [
        51544,
        286,
        393,
        380,
        483,
        666,
        264,
        4365,
        295,
        552,
        13,
        51607
      ],
      "temperature": 0,
      "avg_logprob": -0.22594808603261973,
      "compression_ratio": 1.7589928057553956,
      "no_speech_prob": 1.7456715326741312e-12,
      "speaker_id": 1
    },
    {
      "id": 425,
      "seek": 188004,
      "start": 1905.02,
      "end": 1905.18,
      "text": " Sure.",
      "tokens": [
        51614,
        4894,
        13,
        51622
      ],
      "temperature": 0,
      "avg_logprob": -0.22594808603261973,
      "compression_ratio": 1.7589928057553956,
      "no_speech_prob": 1.7456715326741312e-12,
      "speaker_id": 7
    },
    {
      "id": 426,
      "seek": 188004,
      "start": 1905.18,
      "end": 1905.72,
      "text": " Absolutely.",
      "tokens": [
        51622,
        7021,
        13,
        51649
      ],
      "temperature": 0,
      "avg_logprob": -0.22594808603261973,
      "compression_ratio": 1.7589928057553956,
      "no_speech_prob": 1.7456715326741312e-12,
      "speaker_id": 7
    },
    {
      "id": 427,
      "seek": 188004,
      "start": 1905.72,
      "end": 1906.04,
      "text": " Yes.",
      "tokens": [
        51649,
        1079,
        13,
        51665
      ],
      "temperature": 0,
      "avg_logprob": -0.22594808603261973,
      "compression_ratio": 1.7589928057553956,
      "no_speech_prob": 1.7456715326741312e-12,
      "speaker_id": 1
    },
    {
      "id": 428,
      "seek": 188004,
      "start": 1907.72,
      "end": 1908.1599999999999,
      "text": " Jonathan.",
      "tokens": [
        51749,
        15471,
        13,
        51771
      ],
      "temperature": 0,
      "avg_logprob": -0.22594808603261973,
      "compression_ratio": 1.7589928057553956,
      "no_speech_prob": 1.7456715326741312e-12,
      "speaker_id": 7
    },
    {
      "id": 429,
      "seek": 191004,
      "start": 1910.04,
      "end": 1913.8799999999999,
      "text": " So is one property of this, okay.",
      "tokens": [
        50365,
        407,
        307,
        472,
        4707,
        295,
        341,
        11,
        1392,
        13,
        50557
      ],
      "temperature": 0,
      "avg_logprob": -0.2690750233178,
      "compression_ratio": 1.6391304347826088,
      "no_speech_prob": 2.0049472065214458e-12,
      "speaker_id": 2
    },
    {
      "id": 430,
      "seek": 191004,
      "start": 1913.94,
      "end": 1915.3799999999999,
      "text": " And I really should have read the draft already.",
      "tokens": [
        50560,
        400,
        286,
        534,
        820,
        362,
        1401,
        264,
        11206,
        1217,
        13,
        50632
      ],
      "temperature": 0,
      "avg_logprob": -0.2690750233178,
      "compression_ratio": 1.6391304347826088,
      "no_speech_prob": 2.0049472065214458e-12,
      "speaker_id": 2
    },
    {
      "id": 431,
      "seek": 191004,
      "start": 1915.56,
      "end": 1918.1,
      "text": " Uh, is one property of this that the entire.",
      "tokens": [
        50641,
        4019,
        11,
        307,
        472,
        4707,
        295,
        341,
        300,
        264,
        2302,
        13,
        50768
      ],
      "temperature": 0,
      "avg_logprob": -0.2690750233178,
      "compression_ratio": 1.6391304347826088,
      "no_speech_prob": 2.0049472065214458e-12,
      "speaker_id": 2
    },
    {
      "id": 432,
      "seek": 191004,
      "start": 1918.6,
      "end": 1919.1599999999999,
      "text": " Okay.",
      "tokens": [
        50793,
        1033,
        13,
        50821
      ],
      "temperature": 0,
      "avg_logprob": -0.2690750233178,
      "compression_ratio": 1.6391304347826088,
      "no_speech_prob": 2.0049472065214458e-12,
      "speaker_id": 2
    },
    {
      "id": 433,
      "seek": 191004,
      "start": 1919.44,
      "end": 1919.92,
      "text": " Excellent.",
      "tokens": [
        50835,
        16723,
        13,
        50859
      ],
      "temperature": 0,
      "avg_logprob": -0.2690750233178,
      "compression_ratio": 1.6391304347826088,
      "no_speech_prob": 2.0049472065214458e-12,
      "speaker_id": 2
    },
    {
      "id": 434,
      "seek": 191004,
      "start": 1920.12,
      "end": 1930.3,
      "text": " Then I, uh, the, the entire request has arrived before the first bite of the response has been sent.",
      "tokens": [
        50869,
        1396,
        286,
        11,
        2232,
        11,
        264,
        11,
        264,
        2302,
        5308,
        575,
        6678,
        949,
        264,
        700,
        7988,
        295,
        264,
        4134,
        575,
        668,
        2279,
        13,
        51378
      ],
      "temperature": 0,
      "avg_logprob": -0.2690750233178,
      "compression_ratio": 1.6391304347826088,
      "no_speech_prob": 2.0049472065214458e-12,
      "speaker_id": 2
    },
    {
      "id": 435,
      "seek": 191004,
      "start": 1930.54,
      "end": 1932.44,
      "text": " That is an artificial limitation.",
      "tokens": [
        51390,
        663,
        307,
        364,
        11677,
        27432,
        13,
        51485
      ],
      "temperature": 0,
      "avg_logprob": -0.2690750233178,
      "compression_ratio": 1.6391304347826088,
      "no_speech_prob": 2.0049472065214458e-12,
      "speaker_id": 1
    },
    {
      "id": 436,
      "seek": 191004,
      "start": 1932.44,
      "end": 1933.56,
      "text": " You could add.",
      "tokens": [
        51485,
        509,
        727,
        909,
        13,
        51541
      ],
      "temperature": 0,
      "avg_logprob": -0.2690750233178,
      "compression_ratio": 1.6391304347826088,
      "no_speech_prob": 2.0049472065214458e-12,
      "speaker_id": 1
    },
    {
      "id": 437,
      "seek": 191004,
      "start": 1933.56,
      "end": 1939.82,
      "text": " There is no reason to necessarily do that unless we say that we want a particular.",
      "tokens": [
        51541,
        821,
        307,
        572,
        1778,
        281,
        4725,
        360,
        300,
        5969,
        321,
        584,
        300,
        321,
        528,
        257,
        1729,
        13,
        51854
      ],
      "temperature": 0,
      "avg_logprob": -0.2690750233178,
      "compression_ratio": 1.6391304347826088,
      "no_speech_prob": 2.0049472065214458e-12,
      "speaker_id": 1
    },
    {
      "id": 438,
      "seek": 194004,
      "start": 1940.04,
      "end": 1949.22,
      "text": " Security privacy property that otherwise would not be achievable, but there's no requirement for that from the protocol, nor from like binary HTTP within that.",
      "tokens": [
        50365,
        11164,
        11427,
        4707,
        300,
        5911,
        576,
        406,
        312,
        3538,
        17915,
        11,
        457,
        456,
        311,
        572,
        11695,
        337,
        300,
        490,
        264,
        10336,
        11,
        6051,
        490,
        411,
        17434,
        33283,
        1951,
        300,
        13,
        50824
      ],
      "temperature": 0,
      "avg_logprob": -0.2571566525627585,
      "compression_ratio": 1.282258064516129,
      "no_speech_prob": 1.786062291987689e-12,
      "speaker_id": 1
    },
    {
      "id": 439,
      "seek": 194922,
      "start": 1949.22,
      "end": 1957.02,
      "text": " So if I'm being a psychopath, I could just do like a full, like, this is TLS, ha, ha, ha.",
      "tokens": [
        50365,
        407,
        498,
        286,
        478,
        885,
        257,
        47577,
        11,
        286,
        727,
        445,
        360,
        411,
        257,
        1577,
        11,
        411,
        11,
        341,
        307,
        314,
        19198,
        11,
        324,
        11,
        324,
        11,
        324,
        13,
        50755
      ],
      "temperature": 0,
      "avg_logprob": -0.1495372558308539,
      "compression_ratio": 1.8533333333333333,
      "no_speech_prob": 1.5955971353204257e-12,
      "speaker_id": 2
    },
    {
      "id": 440,
      "seek": 194922,
      "start": 1957.74,
      "end": 1963.7,
      "text": " Um, and just like continuously send a bunch of different requests, get a bunch of different responses, send a bunch of different requests.",
      "tokens": [
        50791,
        3301,
        11,
        293,
        445,
        411,
        15684,
        2845,
        257,
        3840,
        295,
        819,
        12475,
        11,
        483,
        257,
        3840,
        295,
        819,
        13019,
        11,
        2845,
        257,
        3840,
        295,
        819,
        12475,
        13,
        51089
      ],
      "temperature": 0,
      "avg_logprob": -0.1495372558308539,
      "compression_ratio": 1.8533333333333333,
      "no_speech_prob": 1.5955971353204257e-12,
      "speaker_id": 2
    },
    {
      "id": 441,
      "seek": 194922,
      "start": 1963.94,
      "end": 1964.3,
      "text": " Sure.",
      "tokens": [
        51101,
        4894,
        13,
        51119
      ],
      "temperature": 0,
      "avg_logprob": -0.1495372558308539,
      "compression_ratio": 1.8533333333333333,
      "no_speech_prob": 1.5955971353204257e-12,
      "speaker_id": 1
    },
    {
      "id": 442,
      "seek": 194922,
      "start": 1964.3600000000001,
      "end": 1972.94,
      "text": " And if you wanted to essentially make this, like, this is not TLS, you could just add the artificial requirement that you must send all of the requests and then get all the response.",
      "tokens": [
        51122,
        400,
        498,
        291,
        1415,
        281,
        4476,
        652,
        341,
        11,
        411,
        11,
        341,
        307,
        406,
        314,
        19198,
        11,
        291,
        727,
        445,
        909,
        264,
        11677,
        11695,
        300,
        291,
        1633,
        2845,
        439,
        295,
        264,
        12475,
        293,
        550,
        483,
        439,
        264,
        4134,
        13,
        51551
      ],
      "temperature": 0,
      "avg_logprob": -0.1495372558308539,
      "compression_ratio": 1.8533333333333333,
      "no_speech_prob": 1.5955971353204257e-12,
      "speaker_id": 1
    },
    {
      "id": 443,
      "seek": 197294,
      "start": 1972.94,
      "end": 1976.46,
      "text": " Like that is a thing you could artificially put on this if you want.",
      "tokens": [
        50365,
        1743,
        300,
        307,
        257,
        551,
        291,
        727,
        39905,
        2270,
        829,
        322,
        341,
        498,
        291,
        528,
        13,
        50541
      ],
      "temperature": 0,
      "avg_logprob": -0.2765055633172756,
      "compression_ratio": 1.5957446808510638,
      "no_speech_prob": 2.3817130993114555e-12,
      "speaker_id": 1
    },
    {
      "id": 444,
      "seek": 197294,
      "start": 1978.3,
      "end": 1980.3200000000002,
      "text": " It, it, if we thought that was valuable.",
      "tokens": [
        50633,
        467,
        11,
        309,
        11,
        498,
        321,
        1194,
        300,
        390,
        8263,
        13,
        50734
      ],
      "temperature": 0,
      "avg_logprob": -0.2765055633172756,
      "compression_ratio": 1.5957446808510638,
      "no_speech_prob": 2.3817130993114555e-12,
      "speaker_id": 1
    },
    {
      "id": 445,
      "seek": 197294,
      "start": 1981.72,
      "end": 1988.24,
      "text": " It seems that that is a property that is in the original HTTP.",
      "tokens": [
        50804,
        467,
        2544,
        300,
        300,
        307,
        257,
        4707,
        300,
        307,
        294,
        264,
        3380,
        33283,
        13,
        51130
      ],
      "temperature": 0,
      "avg_logprob": -0.2765055633172756,
      "compression_ratio": 1.5957446808510638,
      "no_speech_prob": 2.3817130993114555e-12,
      "speaker_id": 2
    },
    {
      "id": 446,
      "seek": 197294,
      "start": 1988.8600000000001,
      "end": 1989.26,
      "text": " Yes.",
      "tokens": [
        51161,
        1079,
        13,
        51181
      ],
      "temperature": 0,
      "avg_logprob": -0.2765055633172756,
      "compression_ratio": 1.5957446808510638,
      "no_speech_prob": 2.3817130993114555e-12,
      "speaker_id": 1
    },
    {
      "id": 447,
      "seek": 197294,
      "start": 1989.3600000000001,
      "end": 1990.98,
      "text": " So if we wanted to preserve that property.",
      "tokens": [
        51186,
        407,
        498,
        321,
        1415,
        281,
        15665,
        300,
        4707,
        13,
        51267
      ],
      "temperature": 0,
      "avg_logprob": -0.2765055633172756,
      "compression_ratio": 1.5957446808510638,
      "no_speech_prob": 2.3817130993114555e-12,
      "speaker_id": 1
    },
    {
      "id": 448,
      "seek": 197294,
      "start": 1991.54,
      "end": 1997.0800000000002,
      "text": " And I think that does have an impact on the security property you actually get.",
      "tokens": [
        51295,
        400,
        286,
        519,
        300,
        775,
        362,
        364,
        2712,
        322,
        264,
        3825,
        4707,
        291,
        767,
        483,
        13,
        51572
      ],
      "temperature": 0,
      "avg_logprob": -0.2765055633172756,
      "compression_ratio": 1.5957446808510638,
      "no_speech_prob": 2.3817130993114555e-12,
      "speaker_id": 2
    },
    {
      "id": 449,
      "seek": 199708,
      "start": 1997.08,
      "end": 2001.48,
      "text": " I'm not sure that's necessarily the one you want, but it is something that it makes a difference.",
      "tokens": [
        50365,
        286,
        478,
        406,
        988,
        300,
        311,
        4725,
        264,
        472,
        291,
        528,
        11,
        457,
        309,
        307,
        746,
        300,
        309,
        1669,
        257,
        2649,
        13,
        50585
      ],
      "temperature": 0,
      "avg_logprob": -0.16833896415178165,
      "compression_ratio": 1.4926108374384237,
      "no_speech_prob": 1.896030858358766e-12,
      "speaker_id": 2
    },
    {
      "id": 450,
      "seek": 199708,
      "start": 2001.78,
      "end": 2001.8999999999999,
      "text": " Yes.",
      "tokens": [
        50600,
        1079,
        13,
        50606
      ],
      "temperature": 0,
      "avg_logprob": -0.16833896415178165,
      "compression_ratio": 1.4926108374384237,
      "no_speech_prob": 1.896030858358766e-12,
      "speaker_id": 1
    },
    {
      "id": 451,
      "seek": 199708,
      "start": 2002.04,
      "end": 2002.3,
      "text": " You're right.",
      "tokens": [
        50613,
        509,
        434,
        558,
        13,
        50626
      ],
      "temperature": 0,
      "avg_logprob": -0.16833896415178165,
      "compression_ratio": 1.4926108374384237,
      "no_speech_prob": 1.896030858358766e-12,
      "speaker_id": 1
    },
    {
      "id": 452,
      "seek": 199708,
      "start": 2002.5,
      "end": 2003.6799999999998,
      "text": " You're absolutely correct.",
      "tokens": [
        50636,
        509,
        434,
        3122,
        3006,
        13,
        50695
      ],
      "temperature": 0,
      "avg_logprob": -0.16833896415178165,
      "compression_ratio": 1.4926108374384237,
      "no_speech_prob": 1.896030858358766e-12,
      "speaker_id": 1
    },
    {
      "id": 453,
      "seek": 199708,
      "start": 2004.1799999999998,
      "end": 2013.74,
      "text": " And if I'm thinking about the use cases that we would have for this, um, I believe, you know, a lot of them, yeah, I think they'd be fine with that separation.",
      "tokens": [
        50720,
        400,
        498,
        286,
        478,
        1953,
        466,
        264,
        764,
        3331,
        300,
        321,
        576,
        362,
        337,
        341,
        11,
        1105,
        11,
        286,
        1697,
        11,
        291,
        458,
        11,
        257,
        688,
        295,
        552,
        11,
        1338,
        11,
        286,
        519,
        436,
        1116,
        312,
        2489,
        365,
        300,
        14634,
        13,
        51198
      ],
      "temperature": 0,
      "avg_logprob": -0.16833896415178165,
      "compression_ratio": 1.4926108374384237,
      "no_speech_prob": 1.896030858358766e-12,
      "speaker_id": 1
    },
    {
      "id": 454,
      "seek": 201374,
      "start": 2013.74,
      "end": 2022.64,
      "text": " Or even just a commitment to the full request in the first message, right?",
      "tokens": [
        50365,
        1610,
        754,
        445,
        257,
        8371,
        281,
        264,
        1577,
        5308,
        294,
        264,
        700,
        3636,
        11,
        558,
        30,
        50810
      ],
      "temperature": 0,
      "avg_logprob": -0.13312936865765115,
      "compression_ratio": 1.6475770925110131,
      "no_speech_prob": 1.5074484629312712e-12,
      "speaker_id": 2
    },
    {
      "id": 455,
      "seek": 201374,
      "start": 2022.74,
      "end": 2031.1200000000001,
      "text": " Like, just part of the thing is like, when you think about indeterminate length binary HTTP, like it does not necessarily know the entire length of a response.",
      "tokens": [
        50815,
        1743,
        11,
        445,
        644,
        295,
        264,
        551,
        307,
        411,
        11,
        562,
        291,
        519,
        466,
        1016,
        35344,
        13923,
        4641,
        17434,
        33283,
        11,
        411,
        309,
        775,
        406,
        4725,
        458,
        264,
        2302,
        4641,
        295,
        257,
        4134,
        13,
        51234
      ],
      "temperature": 0,
      "avg_logprob": -0.13312936865765115,
      "compression_ratio": 1.6475770925110131,
      "no_speech_prob": 1.5074484629312712e-12,
      "speaker_id": 1
    },
    {
      "id": 456,
      "seek": 201374,
      "start": 2031.52,
      "end": 2040.2,
      "text": " So what I'm, what I'm worried about is an adaptive, like based on something I get in the response, I change what I'm sending in my request.",
      "tokens": [
        51254,
        407,
        437,
        286,
        478,
        11,
        437,
        286,
        478,
        5804,
        466,
        307,
        364,
        27912,
        11,
        411,
        2361,
        322,
        746,
        286,
        483,
        294,
        264,
        4134,
        11,
        286,
        1319,
        437,
        286,
        478,
        7750,
        294,
        452,
        5308,
        13,
        51688
      ],
      "temperature": 0,
      "avg_logprob": -0.13312936865765115,
      "compression_ratio": 1.6475770925110131,
      "no_speech_prob": 1.5074484629312712e-12,
      "speaker_id": 2
    },
    {
      "id": 457,
      "seek": 204020,
      "start": 2040.2,
      "end": 2043.5,
      "text": " Um, and not just like, right.",
      "tokens": [
        50365,
        3301,
        11,
        293,
        406,
        445,
        411,
        11,
        558,
        13,
        50530
      ],
      "temperature": 0,
      "avg_logprob": -0.3058253466072729,
      "compression_ratio": 1.6829268292682926,
      "no_speech_prob": 1.8960176310922616e-12,
      "speaker_id": 2
    },
    {
      "id": 458,
      "seek": 204020,
      "start": 2043.5800000000002,
      "end": 2044.3600000000001,
      "text": " So we can prohibit that.",
      "tokens": [
        50534,
        407,
        321,
        393,
        16015,
        270,
        300,
        13,
        50573
      ],
      "temperature": 0,
      "avg_logprob": -0.3058253466072729,
      "compression_ratio": 1.6829268292682926,
      "no_speech_prob": 1.8960176310922616e-12,
      "speaker_id": 1
    },
    {
      "id": 459,
      "seek": 204020,
      "start": 2044.6200000000001,
      "end": 2053.54,
      "text": " And that, that does like prohibiting that would make this, you know, true, truly only a performance optimization and not change any other property.",
      "tokens": [
        50586,
        400,
        300,
        11,
        300,
        775,
        411,
        16015,
        1748,
        300,
        576,
        652,
        341,
        11,
        291,
        458,
        11,
        2074,
        11,
        4908,
        787,
        257,
        3389,
        19618,
        293,
        406,
        1319,
        604,
        661,
        4707,
        13,
        51032
      ],
      "temperature": 0,
      "avg_logprob": -0.3058253466072729,
      "compression_ratio": 1.6829268292682926,
      "no_speech_prob": 1.8960176310922616e-12,
      "speaker_id": 1
    },
    {
      "id": 460,
      "seek": 204020,
      "start": 2053.54,
      "end": 2056.4,
      "text": " And that would make it easier to reason about for analysis purposes.",
      "tokens": [
        51032,
        400,
        300,
        576,
        652,
        309,
        3571,
        281,
        1778,
        466,
        337,
        5215,
        9932,
        13,
        51175
      ],
      "temperature": 0,
      "avg_logprob": -0.3058253466072729,
      "compression_ratio": 1.6829268292682926,
      "no_speech_prob": 1.8960176310922616e-12,
      "speaker_id": 1
    },
    {
      "id": 461,
      "seek": 204020,
      "start": 2056.94,
      "end": 2057.96,
      "text": " I agree with you there.",
      "tokens": [
        51202,
        286,
        3986,
        365,
        291,
        456,
        13,
        51253
      ],
      "temperature": 0,
      "avg_logprob": -0.3058253466072729,
      "compression_ratio": 1.6829268292682926,
      "no_speech_prob": 1.8960176310922616e-12,
      "speaker_id": 1
    },
    {
      "id": 462,
      "seek": 204020,
      "start": 2058.76,
      "end": 2059.6,
      "text": " Thank you.",
      "tokens": [
        51293,
        1044,
        291,
        13,
        51335
      ],
      "temperature": 0,
      "avg_logprob": -0.3058253466072729,
      "compression_ratio": 1.6829268292682926,
      "no_speech_prob": 1.8960176310922616e-12,
      "speaker_id": 2
    },
    {
      "id": 463,
      "seek": 204020,
      "start": 2059.6,
      "end": 2059.98,
      "text": " Thank you.",
      "tokens": [
        51335,
        1044,
        291,
        13,
        51354
      ],
      "temperature": 0,
      "avg_logprob": -0.3058253466072729,
      "compression_ratio": 1.6829268292682926,
      "no_speech_prob": 1.8960176310922616e-12,
      "speaker_id": 1
    },
    {
      "id": 464,
      "seek": 204020,
      "start": 2061.06,
      "end": 2063.7200000000003,
      "text": " Uh, David Skenazi is me again.",
      "tokens": [
        51408,
        4019,
        11,
        4389,
        318,
        2653,
        26637,
        307,
        385,
        797,
        13,
        51541
      ],
      "temperature": 0,
      "avg_logprob": -0.3058253466072729,
      "compression_ratio": 1.6829268292682926,
      "no_speech_prob": 1.8960176310922616e-12,
      "speaker_id": 5
    },
    {
      "id": 465,
      "seek": 204020,
      "start": 2064.32,
      "end": 2070.18,
      "text": " Um, so thinking about this more, um, this, so this is a question.",
      "tokens": [
        51571,
        3301,
        11,
        370,
        1953,
        466,
        341,
        544,
        11,
        1105,
        11,
        341,
        11,
        370,
        341,
        307,
        257,
        1168,
        13,
        51864
      ],
      "temperature": 0,
      "avg_logprob": -0.3058253466072729,
      "compression_ratio": 1.6829268292682926,
      "no_speech_prob": 1.8960176310922616e-12,
      "speaker_id": 5
    },
    {
      "id": 466,
      "seek": 207020,
      "start": 2070.2,
      "end": 2074.7,
      "text": " So this is an optimization, but it isn't a free optimization.",
      "tokens": [
        50365,
        407,
        341,
        307,
        364,
        19618,
        11,
        457,
        309,
        1943,
        380,
        257,
        1737,
        19618,
        13,
        50590
      ],
      "temperature": 0,
      "avg_logprob": -0.2549072636498345,
      "compression_ratio": 1.44,
      "no_speech_prob": 2.1415638359889577e-12,
      "speaker_id": 5
    },
    {
      "id": 467,
      "seek": 207020,
      "start": 2075.24,
      "end": 2086.18,
      "text": " Um, uh, um, OHTP has weaker security properties than layered TLS or, you know, like mask in the multiple nested mask proxies.",
      "tokens": [
        50617,
        3301,
        11,
        2232,
        11,
        1105,
        11,
        422,
        39,
        16804,
        575,
        24286,
        3825,
        7221,
        813,
        34666,
        314,
        19198,
        420,
        11,
        291,
        458,
        11,
        411,
        6094,
        294,
        264,
        3866,
        15646,
        292,
        6094,
        447,
        87,
        530,
        13,
        51164
      ],
      "temperature": 0,
      "avg_logprob": -0.2549072636498345,
      "compression_ratio": 1.44,
      "no_speech_prob": 2.1415638359889577e-12,
      "speaker_id": 5
    },
    {
      "id": 468,
      "seek": 207020,
      "start": 2086.2799999999997,
      "end": 2087.2599999999998,
      "text": " What are the weaker properties?",
      "tokens": [
        51169,
        708,
        366,
        264,
        24286,
        7221,
        30,
        51218
      ],
      "temperature": 0,
      "avg_logprob": -0.2549072636498345,
      "compression_ratio": 1.44,
      "no_speech_prob": 2.1415638359889577e-12,
      "speaker_id": 1
    },
    {
      "id": 469,
      "seek": 207020,
      "start": 2087.3799999999997,
      "end": 2088.96,
      "text": " I does not have forward secrecy.",
      "tokens": [
        51224,
        286,
        775,
        406,
        362,
        2128,
        34432,
        1344,
        13,
        51303
      ],
      "temperature": 0,
      "avg_logprob": -0.2549072636498345,
      "compression_ratio": 1.44,
      "no_speech_prob": 2.1415638359889577e-12,
      "speaker_id": 5
    },
    {
      "id": 470,
      "seek": 208896,
      "start": 2088.96,
      "end": 2100.54,
      "text": " Um, if the, uh, HPK private key leaks, then information can be decrypted as opposed to, if you do an end to end TLS and the session keys get thrown out, um, you have perfect forward secrecy.",
      "tokens": [
        50365,
        3301,
        11,
        498,
        264,
        11,
        2232,
        11,
        12557,
        42,
        4551,
        2141,
        28885,
        11,
        550,
        1589,
        393,
        312,
        979,
        627,
        25383,
        382,
        8851,
        281,
        11,
        498,
        291,
        360,
        364,
        917,
        281,
        917,
        314,
        19198,
        293,
        264,
        5481,
        9317,
        483,
        11732,
        484,
        11,
        1105,
        11,
        291,
        362,
        2176,
        2128,
        34432,
        1344,
        13,
        50944
      ],
      "temperature": 0,
      "avg_logprob": -0.10584739585975547,
      "compression_ratio": 1.4795918367346939,
      "no_speech_prob": 2.245381615015307e-12,
      "speaker_id": 5
    },
    {
      "id": 471,
      "seek": 208896,
      "start": 2101.46,
      "end": 2111.2,
      "text": " Um, so there are times when it is worthwhile to reduce your security for improved, um, performance.",
      "tokens": [
        50990,
        3301,
        11,
        370,
        456,
        366,
        1413,
        562,
        309,
        307,
        28159,
        281,
        5407,
        428,
        3825,
        337,
        9689,
        11,
        1105,
        11,
        3389,
        13,
        51477
      ],
      "temperature": 0,
      "avg_logprob": -0.10584739585975547,
      "compression_ratio": 1.4795918367346939,
      "no_speech_prob": 2.245381615015307e-12,
      "speaker_id": 5
    },
    {
      "id": 472,
      "seek": 211120,
      "start": 2111.2,
      "end": 2113.7599999999998,
      "text": " And that's what the original HTTP is for.",
      "tokens": [
        50365,
        400,
        300,
        311,
        437,
        264,
        3380,
        33283,
        307,
        337,
        13,
        50493
      ],
      "temperature": 0,
      "avg_logprob": -0.16779569479135367,
      "compression_ratio": 1.3333333333333333,
      "no_speech_prob": 3.06837953482908e-12,
      "speaker_id": 5
    },
    {
      "id": 473,
      "seek": 211120,
      "start": 2114.64,
      "end": 2124,
      "text": " Um, when you have small, many, many small requests that are decorrelated, it makes perfect sense to do that instead of the heavyweight approach of nested TLS.",
      "tokens": [
        50537,
        3301,
        11,
        562,
        291,
        362,
        1359,
        11,
        867,
        11,
        867,
        1359,
        12475,
        300,
        366,
        7919,
        12004,
        11,
        309,
        1669,
        2176,
        2020,
        281,
        360,
        300,
        2602,
        295,
        264,
        4676,
        12329,
        3109,
        295,
        15646,
        292,
        314,
        19198,
        13,
        51005
      ],
      "temperature": 0,
      "avg_logprob": -0.16779569479135367,
      "compression_ratio": 1.3333333333333333,
      "no_speech_prob": 3.06837953482908e-12,
      "speaker_id": 5
    },
    {
      "id": 474,
      "seek": 212400,
      "start": 2124,
      "end": 2127.88,
      "text": " Um, but in this case, it's not guaranteed.",
      "tokens": [
        50365,
        3301,
        11,
        457,
        294,
        341,
        1389,
        11,
        309,
        311,
        406,
        18031,
        13,
        50559
      ],
      "temperature": 0,
      "avg_logprob": -0.20697336485891632,
      "compression_ratio": 1.4463276836158192,
      "no_speech_prob": 2.189068587857279e-12,
      "speaker_id": 5
    },
    {
      "id": 475,
      "seek": 212400,
      "start": 2128.14,
      "end": 2132.14,
      "text": " So one of your use cases is a giant file download.",
      "tokens": [
        50572,
        407,
        472,
        295,
        428,
        764,
        3331,
        307,
        257,
        7410,
        3991,
        5484,
        13,
        50772
      ],
      "temperature": 0,
      "avg_logprob": -0.20697336485891632,
      "compression_ratio": 1.4463276836158192,
      "no_speech_prob": 2.189068587857279e-12,
      "speaker_id": 5
    },
    {
      "id": 476,
      "seek": 212400,
      "start": 2132.66,
      "end": 2143.04,
      "text": " Um, I think the correct answer there is to spend the slightly bigger performance hit, which will be negligible in the end of the day to get the improved security.",
      "tokens": [
        50798,
        3301,
        11,
        286,
        519,
        264,
        3006,
        1867,
        456,
        307,
        281,
        3496,
        264,
        4748,
        3801,
        3389,
        2045,
        11,
        597,
        486,
        312,
        32570,
        964,
        294,
        264,
        917,
        295,
        264,
        786,
        281,
        483,
        264,
        9689,
        3825,
        13,
        51317
      ],
      "temperature": 0,
      "avg_logprob": -0.20697336485891632,
      "compression_ratio": 1.4463276836158192,
      "no_speech_prob": 2.189068587857279e-12,
      "speaker_id": 5
    },
    {
      "id": 477,
      "seek": 214304,
      "start": 2143.04,
      "end": 2160.92,
      "text": " Um, I totally understand that, uh, you might not be at liberty to talk about the, uh, uh, all your use cases, but I think before I consider wanting to work on this and adoption, I would need to know more why this use case needs this.",
      "tokens": [
        50365,
        3301,
        11,
        286,
        3879,
        1223,
        300,
        11,
        2232,
        11,
        291,
        1062,
        406,
        312,
        412,
        22849,
        281,
        751,
        466,
        264,
        11,
        2232,
        11,
        2232,
        11,
        439,
        428,
        764,
        3331,
        11,
        457,
        286,
        519,
        949,
        286,
        1949,
        7935,
        281,
        589,
        322,
        341,
        293,
        19215,
        11,
        286,
        576,
        643,
        281,
        458,
        544,
        983,
        341,
        764,
        1389,
        2203,
        341,
        13,
        51259
      ],
      "temperature": 0,
      "avg_logprob": -0.14993551890055337,
      "compression_ratio": 1.478021978021978,
      "no_speech_prob": 2.7645023856909257e-12,
      "speaker_id": 5
    },
    {
      "id": 478,
      "seek": 214304,
      "start": 2161.02,
      "end": 2162.04,
      "text": " It's still unclear to me.",
      "tokens": [
        51264,
        467,
        311,
        920,
        25636,
        281,
        385,
        13,
        51315
      ],
      "temperature": 0,
      "avg_logprob": -0.14993551890055337,
      "compression_ratio": 1.478021978021978,
      "no_speech_prob": 2.7645023856909257e-12,
      "speaker_id": 5
    },
    {
      "id": 479,
      "seek": 214304,
      "start": 2166.04,
      "end": 2167.52,
      "text": " Oh, Eric.",
      "tokens": [
        51515,
        876,
        11,
        9336,
        13,
        51589
      ],
      "temperature": 0,
      "avg_logprob": -0.14993551890055337,
      "compression_ratio": 1.478021978021978,
      "no_speech_prob": 2.7645023856909257e-12,
      "speaker_id": 1
    },
    {
      "id": 480,
      "seek": 216752,
      "start": 2167.52,
      "end": 2176.1,
      "text": " Yeah, just based on all the feedback of, oh, it seems like TLS in these cases, oh, you should use TLS in that case, stuff like that.",
      "tokens": [
        50365,
        865,
        11,
        445,
        2361,
        322,
        439,
        264,
        5824,
        295,
        11,
        1954,
        11,
        309,
        2544,
        411,
        314,
        19198,
        294,
        613,
        3331,
        11,
        1954,
        11,
        291,
        820,
        764,
        314,
        19198,
        294,
        300,
        1389,
        11,
        1507,
        411,
        300,
        13,
        50794
      ],
      "temperature": 0,
      "avg_logprob": -0.13104459351184322,
      "compression_ratio": 1.8433179723502304,
      "no_speech_prob": 1.967823256773804e-12,
      "speaker_id": 11
    },
    {
      "id": 481,
      "seek": 216752,
      "start": 2176.32,
      "end": 2193.2,
      "text": " I think any draft on this needs a pretty good guidance section on you should use TLS if these things are true and talk about if, if you need back and forth, if you need more than so many chunks in your response, you need more, more than so much time in your response.",
      "tokens": [
        50805,
        286,
        519,
        604,
        11206,
        322,
        341,
        2203,
        257,
        1238,
        665,
        10056,
        3541,
        322,
        291,
        820,
        764,
        314,
        19198,
        498,
        613,
        721,
        366,
        2074,
        293,
        751,
        466,
        498,
        11,
        498,
        291,
        643,
        646,
        293,
        5220,
        11,
        498,
        291,
        643,
        544,
        813,
        370,
        867,
        24004,
        294,
        428,
        4134,
        11,
        291,
        643,
        544,
        11,
        544,
        813,
        370,
        709,
        565,
        294,
        428,
        4134,
        13,
        51649
      ],
      "temperature": 0,
      "avg_logprob": -0.13104459351184322,
      "compression_ratio": 1.8433179723502304,
      "no_speech_prob": 1.967823256773804e-12,
      "speaker_id": 11
    },
    {
      "id": 482,
      "seek": 219320,
      "start": 2193.2,
      "end": 2203.14,
      "text": " And that also means we kind of need to figure out where these borders are for when it's useful to use oh, high versus TLS before we have this draft.",
      "tokens": [
        50365,
        400,
        300,
        611,
        1355,
        321,
        733,
        295,
        643,
        281,
        2573,
        484,
        689,
        613,
        16287,
        366,
        337,
        562,
        309,
        311,
        4420,
        281,
        764,
        1954,
        11,
        1090,
        5717,
        314,
        19198,
        949,
        321,
        362,
        341,
        11206,
        13,
        50862
      ],
      "temperature": 0,
      "avg_logprob": -0.15505110911833933,
      "compression_ratio": 1.7306273062730628,
      "no_speech_prob": 1.8579289582515424e-12,
      "speaker_id": 11
    },
    {
      "id": 483,
      "seek": 219320,
      "start": 2203.46,
      "end": 2205.8799999999997,
      "text": " So I think that's stuff we need to figure out.",
      "tokens": [
        50878,
        407,
        286,
        519,
        300,
        311,
        1507,
        321,
        643,
        281,
        2573,
        484,
        13,
        50999
      ],
      "temperature": 0,
      "avg_logprob": -0.15505110911833933,
      "compression_ratio": 1.7306273062730628,
      "no_speech_prob": 1.8579289582515424e-12,
      "speaker_id": 11
    },
    {
      "id": 484,
      "seek": 219320,
      "start": 2205.9199999999996,
      "end": 2209.64,
      "text": " And I think stuff we need to write down before we can really have a reasonable draft that can go far.",
      "tokens": [
        51001,
        400,
        286,
        519,
        1507,
        321,
        643,
        281,
        2464,
        760,
        949,
        321,
        393,
        534,
        362,
        257,
        10585,
        11206,
        300,
        393,
        352,
        1400,
        13,
        51187
      ],
      "temperature": 0,
      "avg_logprob": -0.15505110911833933,
      "compression_ratio": 1.7306273062730628,
      "no_speech_prob": 1.8579289582515424e-12,
      "speaker_id": 11
    },
    {
      "id": 485,
      "seek": 219320,
      "start": 2215.06,
      "end": 2223.12,
      "text": " Eric, I mean, just sort of like follow up on what I was saying earlier when Eric and David were just saying, I mean, like one of the advantages of using a transport layer,",
      "tokens": [
        51458,
        9336,
        11,
        286,
        914,
        11,
        445,
        1333,
        295,
        411,
        1524,
        493,
        322,
        437,
        286,
        390,
        1566,
        3071,
        562,
        9336,
        293,
        4389,
        645,
        445,
        1566,
        11,
        286,
        914,
        11,
        411,
        472,
        295,
        264,
        14906,
        295,
        1228,
        257,
        5495,
        4583,
        11,
        51861
      ],
      "temperature": 0,
      "avg_logprob": -0.15505110911833933,
      "compression_ratio": 1.7306273062730628,
      "no_speech_prob": 1.8579289582515424e-12,
      "speaker_id": 10
    },
    {
      "id": 486,
      "seek": 222312,
      "start": 2223.12,
      "end": 2230.3199999999997,
      "text": " another thing when you were downloading a big thing is that the rate control works and here the rate control, you have two decoupled transport sessions and I don't understand the rate control works in any meaningful way.",
      "tokens": [
        50365,
        1071,
        551,
        562,
        291,
        645,
        32529,
        257,
        955,
        551,
        307,
        300,
        264,
        3314,
        1969,
        1985,
        293,
        510,
        264,
        3314,
        1969,
        11,
        291,
        362,
        732,
        979,
        263,
        15551,
        5495,
        11081,
        293,
        286,
        500,
        380,
        1223,
        264,
        3314,
        1969,
        1985,
        294,
        604,
        10995,
        636,
        13,
        50725
      ],
      "temperature": 0,
      "avg_logprob": -0.23591669458542427,
      "compression_ratio": 2.0032154340836015,
      "no_speech_prob": 1.956496162996979e-12,
      "speaker_id": 10
    },
    {
      "id": 487,
      "seek": 222312,
      "start": 2232.3199999999997,
      "end": 2239.2,
      "text": " And if there's rate control from like this rate control from the center to the rate control from like, from like the center of the proxy and the proxy there and it's like, correct.",
      "tokens": [
        50825,
        400,
        498,
        456,
        311,
        3314,
        1969,
        490,
        411,
        341,
        3314,
        1969,
        490,
        264,
        3056,
        281,
        264,
        3314,
        1969,
        490,
        411,
        11,
        490,
        411,
        264,
        3056,
        295,
        264,
        29690,
        293,
        264,
        29690,
        456,
        293,
        309,
        311,
        411,
        11,
        3006,
        13,
        51169
      ],
      "temperature": 0,
      "avg_logprob": -0.23591669458542427,
      "compression_ratio": 2.0032154340836015,
      "no_speech_prob": 1.956496162996979e-12,
      "speaker_id": 10
    },
    {
      "id": 488,
      "seek": 222312,
      "start": 2239.74,
      "end": 2240.62,
      "text": " And it's like extraordinarily goofy.",
      "tokens": [
        51196,
        400,
        309,
        311,
        411,
        34557,
        42995,
        13,
        51240
      ],
      "temperature": 0,
      "avg_logprob": -0.23591669458542427,
      "compression_ratio": 2.0032154340836015,
      "no_speech_prob": 1.956496162996979e-12,
      "speaker_id": 10
    },
    {
      "id": 489,
      "seek": 222312,
      "start": 2241.1,
      "end": 2251.48,
      "text": " Well, I mean, essentially the, the relay or any relays or other servers in between are responsible for propagating back pressure, which would be the case for any reverse proxy as well.",
      "tokens": [
        51264,
        1042,
        11,
        286,
        914,
        11,
        4476,
        264,
        11,
        264,
        24214,
        420,
        604,
        1039,
        3772,
        420,
        661,
        15909,
        294,
        1296,
        366,
        6250,
        337,
        12425,
        990,
        646,
        3321,
        11,
        597,
        576,
        312,
        264,
        1389,
        337,
        604,
        9943,
        29690,
        382,
        731,
        13,
        51783
      ],
      "temperature": 0,
      "avg_logprob": -0.23591669458542427,
      "compression_ratio": 2.0032154340836015,
      "no_speech_prob": 1.956496162996979e-12,
      "speaker_id": 1
    },
    {
      "id": 490,
      "seek": 225312,
      "start": 2253.12,
      "end": 2265.18,
      "text": " Well, I mean, like the, the, the, the way that certainly is the way that, and that is not one reason why when we're just running quick over mask rather than, rather than running to TCP termination, turning each side.",
      "tokens": [
        50365,
        1042,
        11,
        286,
        914,
        11,
        411,
        264,
        11,
        264,
        11,
        264,
        11,
        264,
        636,
        300,
        3297,
        307,
        264,
        636,
        300,
        11,
        293,
        300,
        307,
        406,
        472,
        1778,
        983,
        562,
        321,
        434,
        445,
        2614,
        1702,
        670,
        6094,
        2831,
        813,
        11,
        2831,
        813,
        2614,
        281,
        48965,
        1433,
        2486,
        11,
        6246,
        1184,
        1252,
        13,
        50968
      ],
      "temperature": 0,
      "avg_logprob": -0.3008002078894413,
      "compression_ratio": 1.5609756097560976,
      "no_speech_prob": 2.065433978362674e-12,
      "speaker_id": 10
    },
    {
      "id": 491,
      "seek": 225312,
      "start": 2266.62,
      "end": 2267.88,
      "text": " Because the rate control is end to end.",
      "tokens": [
        51040,
        1436,
        264,
        3314,
        1969,
        307,
        917,
        281,
        917,
        13,
        51103
      ],
      "temperature": 0,
      "avg_logprob": -0.3008002078894413,
      "compression_ratio": 1.5609756097560976,
      "no_speech_prob": 2.065433978362674e-12,
      "speaker_id": 10
    },
    {
      "id": 492,
      "seek": 226788,
      "start": 2267.88,
      "end": 2284.7200000000003,
      "text": " But I think, I mean, I think in the case here, because remember like a large OHDP relay is going to have essentially like normally just one giant, you know, H2 session with the gateway or, you know, a handful of those.",
      "tokens": [
        50365,
        583,
        286,
        519,
        11,
        286,
        914,
        11,
        286,
        519,
        294,
        264,
        1389,
        510,
        11,
        570,
        1604,
        411,
        257,
        2416,
        13931,
        11373,
        24214,
        307,
        516,
        281,
        362,
        4476,
        411,
        5646,
        445,
        472,
        7410,
        11,
        291,
        458,
        11,
        389,
        17,
        5481,
        365,
        264,
        28532,
        420,
        11,
        291,
        458,
        11,
        257,
        16458,
        295,
        729,
        13,
        51207
      ],
      "temperature": 0,
      "avg_logprob": -0.11978851897375924,
      "compression_ratio": 1.4248366013071896,
      "no_speech_prob": 2.475889285258326e-12,
      "speaker_id": 1
    },
    {
      "id": 493,
      "seek": 228472,
      "start": 2284.72,
      "end": 2290.06,
      "text": " Which overall for, it is a better performance thing.",
      "tokens": [
        50365,
        3013,
        4787,
        337,
        11,
        309,
        307,
        257,
        1101,
        3389,
        551,
        13,
        50632
      ],
      "temperature": 0,
      "avg_logprob": -0.20281976642030658,
      "compression_ratio": 1.4473684210526316,
      "no_speech_prob": 2.7335601230499273e-12,
      "speaker_id": 1
    },
    {
      "id": 494,
      "seek": 228472,
      "start": 2290.14,
      "end": 2293.9199999999996,
      "text": " And it is just having to deal with the rate for the streams.",
      "tokens": [
        50636,
        400,
        309,
        307,
        445,
        1419,
        281,
        2028,
        365,
        264,
        3314,
        337,
        264,
        15842,
        13,
        50825
      ],
      "temperature": 0,
      "avg_logprob": -0.20281976642030658,
      "compression_ratio": 1.4473684210526316,
      "no_speech_prob": 2.7335601230499273e-12,
      "speaker_id": 1
    },
    {
      "id": 495,
      "seek": 228472,
      "start": 2294.14,
      "end": 2302.14,
      "text": " I, I, I'm not sure I'm following, but maybe that's a separate, I mean, why, why, like, why is that better?",
      "tokens": [
        50836,
        286,
        11,
        286,
        11,
        286,
        478,
        406,
        988,
        286,
        478,
        3480,
        11,
        457,
        1310,
        300,
        311,
        257,
        4994,
        11,
        286,
        914,
        11,
        983,
        11,
        983,
        11,
        411,
        11,
        983,
        307,
        300,
        1101,
        30,
        51236
      ],
      "temperature": 0,
      "avg_logprob": -0.20281976642030658,
      "compression_ratio": 1.4473684210526316,
      "no_speech_prob": 2.7335601230499273e-12,
      "speaker_id": 10
    },
    {
      "id": 496,
      "seek": 230214,
      "start": 2302.14,
      "end": 2327.8799999999997,
      "text": " I mean, the, so in the case of like mask, right, we are like, what we want ultimately is, you know, this end to end session that we are doing lots of stuff, you know, we're doing H3, H2 to the end servers, where we want that end to end flow control by pressure, et cetera.",
      "tokens": [
        50365,
        286,
        914,
        11,
        264,
        11,
        370,
        294,
        264,
        1389,
        295,
        411,
        6094,
        11,
        558,
        11,
        321,
        366,
        411,
        11,
        437,
        321,
        528,
        6284,
        307,
        11,
        291,
        458,
        11,
        341,
        917,
        281,
        917,
        5481,
        300,
        321,
        366,
        884,
        3195,
        295,
        1507,
        11,
        291,
        458,
        11,
        321,
        434,
        884,
        389,
        18,
        11,
        389,
        17,
        281,
        264,
        917,
        15909,
        11,
        689,
        321,
        528,
        300,
        917,
        281,
        917,
        3095,
        1969,
        538,
        3321,
        11,
        1030,
        11458,
        13,
        51652
      ],
      "temperature": 0,
      "avg_logprob": -0.13745841227079691,
      "compression_ratio": 1.6094674556213018,
      "no_speech_prob": 2.392817064281183e-12,
      "speaker_id": 1
    },
    {
      "id": 497,
      "seek": 232788,
      "start": 2327.88,
      "end": 2343.7000000000003,
      "text": " In this case, I mean, clients will have some session to the relay, that relay will have essentially, you know, one logical large session to the gateway and they just independently deal with their own.",
      "tokens": [
        50365,
        682,
        341,
        1389,
        11,
        286,
        914,
        11,
        6982,
        486,
        362,
        512,
        5481,
        281,
        264,
        24214,
        11,
        300,
        24214,
        486,
        362,
        4476,
        11,
        291,
        458,
        11,
        472,
        14978,
        2416,
        5481,
        281,
        264,
        28532,
        293,
        436,
        445,
        21761,
        2028,
        365,
        641,
        1065,
        13,
        51156
      ],
      "temperature": 0,
      "avg_logprob": -0.19200968742370605,
      "compression_ratio": 1.5625,
      "no_speech_prob": 1.955452726826179e-12,
      "speaker_id": 1
    },
    {
      "id": 498,
      "seek": 232788,
      "start": 2343.78,
      "end": 2345.52,
      "text": " No, no, I'm saying, but I'm saying it's a defect.",
      "tokens": [
        51160,
        883,
        11,
        572,
        11,
        286,
        478,
        1566,
        11,
        457,
        286,
        478,
        1566,
        309,
        311,
        257,
        16445,
        13,
        51247
      ],
      "temperature": 0,
      "avg_logprob": -0.19200968742370605,
      "compression_ratio": 1.5625,
      "no_speech_prob": 1.955452726826179e-12,
      "speaker_id": 10
    },
    {
      "id": 499,
      "seek": 234552,
      "start": 2345.52,
      "end": 2356.94,
      "text": " I'm saying that, like, that, like the rate control, like, that, like, that the, that, like, you have totally decoupled rate control between the data in each direction.",
      "tokens": [
        50365,
        286,
        478,
        1566,
        300,
        11,
        411,
        11,
        300,
        11,
        411,
        264,
        3314,
        1969,
        11,
        411,
        11,
        300,
        11,
        411,
        11,
        300,
        264,
        11,
        300,
        11,
        411,
        11,
        291,
        362,
        3879,
        979,
        263,
        15551,
        3314,
        1969,
        1296,
        264,
        1412,
        294,
        1184,
        3513,
        13,
        50936
      ],
      "temperature": 0,
      "avg_logprob": -0.23732855870173528,
      "compression_ratio": 1.587837837837838,
      "no_speech_prob": 1.791200434503315e-12,
      "speaker_id": 10
    },
    {
      "id": 500,
      "seek": 234552,
      "start": 2357.2599999999998,
      "end": 2358.18,
      "text": " And it's just weird.",
      "tokens": [
        50952,
        400,
        309,
        311,
        445,
        3657,
        13,
        50998
      ],
      "temperature": 0,
      "avg_logprob": -0.23732855870173528,
      "compression_ratio": 1.587837837837838,
      "no_speech_prob": 1.791200434503315e-12,
      "speaker_id": 10
    },
    {
      "id": 501,
      "seek": 234552,
      "start": 2358.58,
      "end": 2360.18,
      "text": " But I think for single request response pairs.",
      "tokens": [
        51018,
        583,
        286,
        519,
        337,
        2167,
        5308,
        4134,
        15494,
        13,
        51098
      ],
      "temperature": 0,
      "avg_logprob": -0.23732855870173528,
      "compression_ratio": 1.587837837837838,
      "no_speech_prob": 1.791200434503315e-12,
      "speaker_id": 1
    },
    {
      "id": 502,
      "seek": 236018,
      "start": 2360.18,
      "end": 2371.5,
      "text": " Is the, is the, the scenario you're envisioning, like, the relay to gateway link gets stopped up for some reason, but the inbound to the relay from the client doesn't know about that.",
      "tokens": [
        50365,
        1119,
        264,
        11,
        307,
        264,
        11,
        264,
        9005,
        291,
        434,
        24739,
        278,
        11,
        411,
        11,
        264,
        24214,
        281,
        28532,
        2113,
        2170,
        5936,
        493,
        337,
        512,
        1778,
        11,
        457,
        264,
        294,
        18767,
        281,
        264,
        24214,
        490,
        264,
        6423,
        1177,
        380,
        458,
        466,
        300,
        13,
        50931
      ],
      "temperature": 0,
      "avg_logprob": -0.10801048616392422,
      "compression_ratio": 1.62890625,
      "no_speech_prob": 1.712905967239664e-12,
      "speaker_id": 6
    },
    {
      "id": 503,
      "seek": 236018,
      "start": 2371.58,
      "end": 2372.58,
      "text": " It feels no back pressure.",
      "tokens": [
        50935,
        467,
        3417,
        572,
        646,
        3321,
        13,
        50985
      ],
      "temperature": 0,
      "avg_logprob": -0.10801048616392422,
      "compression_ratio": 1.62890625,
      "no_speech_prob": 1.712905967239664e-12,
      "speaker_id": 6
    },
    {
      "id": 504,
      "seek": 236018,
      "start": 2372.96,
      "end": 2375.72,
      "text": " And so the relay ends up with basically unbounded buffering.",
      "tokens": [
        51004,
        400,
        370,
        264,
        24214,
        5314,
        493,
        365,
        1936,
        517,
        18767,
        292,
        9204,
        1794,
        13,
        51142
      ],
      "temperature": 0,
      "avg_logprob": -0.10801048616392422,
      "compression_ratio": 1.62890625,
      "no_speech_prob": 1.712905967239664e-12,
      "speaker_id": 6
    },
    {
      "id": 505,
      "seek": 236018,
      "start": 2376,
      "end": 2381.66,
      "text": " But it should just have back pressure on its, whatever units like H2 or H3 streams are on either side.",
      "tokens": [
        51156,
        583,
        309,
        820,
        445,
        362,
        646,
        3321,
        322,
        1080,
        11,
        2035,
        6815,
        411,
        389,
        17,
        420,
        389,
        18,
        15842,
        366,
        322,
        2139,
        1252,
        13,
        51439
      ],
      "temperature": 0,
      "avg_logprob": -0.10801048616392422,
      "compression_ratio": 1.62890625,
      "no_speech_prob": 1.712905967239664e-12,
      "speaker_id": 1
    },
    {
      "id": 506,
      "seek": 236018,
      "start": 2381.7799999999997,
      "end": 2381.9199999999996,
      "text": " Yeah.",
      "tokens": [
        51445,
        865,
        13,
        51452
      ],
      "temperature": 0,
      "avg_logprob": -0.10801048616392422,
      "compression_ratio": 1.62890625,
      "no_speech_prob": 1.712905967239664e-12,
      "speaker_id": 10
    },
    {
      "id": 507,
      "seek": 236018,
      "start": 2381.9199999999996,
      "end": 2383.06,
      "text": " I'm not saying one can't build that.",
      "tokens": [
        51452,
        286,
        478,
        406,
        1566,
        472,
        393,
        380,
        1322,
        300,
        13,
        51509
      ],
      "temperature": 0,
      "avg_logprob": -0.10801048616392422,
      "compression_ratio": 1.62890625,
      "no_speech_prob": 1.712905967239664e-12,
      "speaker_id": 10
    },
    {
      "id": 508,
      "seek": 238306,
      "start": 2383.06,
      "end": 2390.18,
      "text": " What I'm saying is that you took a situation where you had, like, automatic back pressure, both in the transfer protocol, and you're placing it by, by back pressure at the application layer.",
      "tokens": [
        50365,
        708,
        286,
        478,
        1566,
        307,
        300,
        291,
        1890,
        257,
        2590,
        689,
        291,
        632,
        11,
        411,
        11,
        12509,
        646,
        3321,
        11,
        1293,
        294,
        264,
        5003,
        10336,
        11,
        293,
        291,
        434,
        17221,
        309,
        538,
        11,
        538,
        646,
        3321,
        412,
        264,
        3861,
        4583,
        13,
        50721
      ],
      "temperature": 0,
      "avg_logprob": -0.17218498711113459,
      "compression_ratio": 1.6014492753623188,
      "no_speech_prob": 1.7762168688997826e-12,
      "speaker_id": 10
    },
    {
      "id": 509,
      "seek": 238306,
      "start": 2391.54,
      "end": 2399.7,
      "text": " Which is, I mean, extremely, like, it's a pattern that we already have to deal with for OHDP and essentially every reverse proxy case.",
      "tokens": [
        50789,
        3013,
        307,
        11,
        286,
        914,
        11,
        4664,
        11,
        411,
        11,
        309,
        311,
        257,
        5102,
        300,
        321,
        1217,
        362,
        281,
        2028,
        365,
        337,
        13931,
        11373,
        293,
        4476,
        633,
        9943,
        29690,
        1389,
        13,
        51197
      ],
      "temperature": 0,
      "avg_logprob": -0.17218498711113459,
      "compression_ratio": 1.6014492753623188,
      "no_speech_prob": 1.7762168688997826e-12,
      "speaker_id": 1
    },
    {
      "id": 510,
      "seek": 238306,
      "start": 2401.58,
      "end": 2408.2999999999997,
      "text": " Like, it's, it's also a pretty well-established thing here when we are just doing an end-to-end HP request response.",
      "tokens": [
        51291,
        1743,
        11,
        309,
        311,
        11,
        309,
        311,
        611,
        257,
        1238,
        731,
        12,
        33542,
        4173,
        551,
        510,
        562,
        321,
        366,
        445,
        884,
        364,
        917,
        12,
        1353,
        12,
        521,
        12557,
        5308,
        4134,
        13,
        51627
      ],
      "temperature": 0,
      "avg_logprob": -0.17218498711113459,
      "compression_ratio": 1.6014492753623188,
      "no_speech_prob": 1.7762168688997826e-12,
      "speaker_id": 1
    },
    {
      "id": 511,
      "seek": 240830,
      "start": 2408.3,
      "end": 2416.3,
      "text": " Um, yes, but there's, they're self-clocking because they're, because, because, because they, they're one-to-one.",
      "tokens": [
        50365,
        3301,
        11,
        2086,
        11,
        457,
        456,
        311,
        11,
        436,
        434,
        2698,
        12,
        9023,
        278,
        570,
        436,
        434,
        11,
        570,
        11,
        570,
        11,
        570,
        436,
        11,
        436,
        434,
        472,
        12,
        1353,
        12,
        546,
        13,
        50765
      ],
      "temperature": 0,
      "avg_logprob": -0.19760477820108102,
      "compression_ratio": 1.7155963302752293,
      "no_speech_prob": 2.1589362242391275e-12,
      "speaker_id": 10
    },
    {
      "id": 512,
      "seek": 240830,
      "start": 2418.42,
      "end": 2419.02,
      "text": " They're what?",
      "tokens": [
        50871,
        814,
        434,
        437,
        30,
        50901
      ],
      "temperature": 0,
      "avg_logprob": -0.19760477820108102,
      "compression_ratio": 1.7155963302752293,
      "no_speech_prob": 2.1589362242391275e-12,
      "speaker_id": 1
    },
    {
      "id": 513,
      "seek": 240830,
      "start": 2419.28,
      "end": 2419.82,
      "text": " One-to-one.",
      "tokens": [
        50914,
        1485,
        12,
        1353,
        12,
        546,
        13,
        50941
      ],
      "temperature": 0,
      "avg_logprob": -0.19760477820108102,
      "compression_ratio": 1.7155963302752293,
      "no_speech_prob": 2.1589362242391275e-12,
      "speaker_id": 10
    },
    {
      "id": 514,
      "seek": 240830,
      "start": 2420.0800000000004,
      "end": 2420.96,
      "text": " The part before that.",
      "tokens": [
        50954,
        440,
        644,
        949,
        300,
        13,
        50998
      ],
      "temperature": 0,
      "avg_logprob": -0.19760477820108102,
      "compression_ratio": 1.7155963302752293,
      "no_speech_prob": 2.1589362242391275e-12,
      "speaker_id": 1
    },
    {
      "id": 515,
      "seek": 240830,
      "start": 2421.48,
      "end": 2422.2000000000003,
      "text": " It's self-clocking.",
      "tokens": [
        51024,
        467,
        311,
        2698,
        12,
        9023,
        278,
        13,
        51060
      ],
      "temperature": 0,
      "avg_logprob": -0.19760477820108102,
      "compression_ratio": 1.7155963302752293,
      "no_speech_prob": 2.1589362242391275e-12,
      "speaker_id": 10
    },
    {
      "id": 516,
      "seek": 240830,
      "start": 2423.7400000000002,
      "end": 2424.6800000000003,
      "text": " Self-clocking?",
      "tokens": [
        51137,
        16348,
        12,
        9023,
        278,
        30,
        51184
      ],
      "temperature": 0,
      "avg_logprob": -0.19760477820108102,
      "compression_ratio": 1.7155963302752293,
      "no_speech_prob": 2.1589362242391275e-12,
      "speaker_id": 1
    },
    {
      "id": 517,
      "seek": 240830,
      "start": 2424.88,
      "end": 2425.1800000000003,
      "text": " Yes.",
      "tokens": [
        51194,
        1079,
        13,
        51209
      ],
      "temperature": 0,
      "avg_logprob": -0.19760477820108102,
      "compression_ratio": 1.7155963302752293,
      "no_speech_prob": 2.1589362242391275e-12,
      "speaker_id": 10
    },
    {
      "id": 518,
      "seek": 240830,
      "start": 2426.7200000000003,
      "end": 2429.28,
      "text": " Anyway, like, it may require an offline discussion.",
      "tokens": [
        51286,
        5684,
        11,
        411,
        11,
        309,
        815,
        3651,
        364,
        21857,
        5017,
        13,
        51414
      ],
      "temperature": 0,
      "avg_logprob": -0.19760477820108102,
      "compression_ratio": 1.7155963302752293,
      "no_speech_prob": 2.1589362242391275e-12,
      "speaker_id": 10
    },
    {
      "id": 519,
      "seek": 240830,
      "start": 2429.4,
      "end": 2431.44,
      "text": " I'm just saying, like, you're creating a series of new problems.",
      "tokens": [
        51420,
        286,
        478,
        445,
        1566,
        11,
        411,
        11,
        291,
        434,
        4084,
        257,
        2638,
        295,
        777,
        2740,
        13,
        51522
      ],
      "temperature": 0,
      "avg_logprob": -0.19760477820108102,
      "compression_ratio": 1.7155963302752293,
      "no_speech_prob": 2.1589362242391275e-12,
      "speaker_id": 10
    },
    {
      "id": 520,
      "seek": 240830,
      "start": 2431.54,
      "end": 2433.78,
      "text": " I think we have to, I think that, like, demotivate this.",
      "tokens": [
        51527,
        286,
        519,
        321,
        362,
        281,
        11,
        286,
        519,
        300,
        11,
        411,
        11,
        1371,
        310,
        592,
        473,
        341,
        13,
        51639
      ],
      "temperature": 0,
      "avg_logprob": -0.19760477820108102,
      "compression_ratio": 1.7155963302752293,
      "no_speech_prob": 2.1589362242391275e-12,
      "speaker_id": 10
    },
    {
      "id": 521,
      "seek": 243378,
      "start": 2433.78,
      "end": 2442.42,
      "text": " I mean, like, I think it's one thing to say, like, I wanted to send, I wanted to send two chunks, but when you're like, I want to send, like, a, like, effective HL stream is, like, a different story.",
      "tokens": [
        50365,
        286,
        914,
        11,
        411,
        11,
        286,
        519,
        309,
        311,
        472,
        551,
        281,
        584,
        11,
        411,
        11,
        286,
        1415,
        281,
        2845,
        11,
        286,
        1415,
        281,
        2845,
        732,
        24004,
        11,
        457,
        562,
        291,
        434,
        411,
        11,
        286,
        528,
        281,
        2845,
        11,
        411,
        11,
        257,
        11,
        411,
        11,
        4942,
        389,
        43,
        4309,
        307,
        11,
        411,
        11,
        257,
        819,
        1657,
        13,
        50797
      ],
      "temperature": 0,
      "avg_logprob": -0.09510249296824137,
      "compression_ratio": 1.8125,
      "no_speech_prob": 1.6867912231918747e-12,
      "speaker_id": 10
    },
    {
      "id": 522,
      "seek": 243378,
      "start": 2442.5600000000004,
      "end": 2459.2000000000003,
      "text": " So, I think, at least, you know, the way I've been thinking about this is the alternative that you're starting with is you just have a very, like, you are using a OHDP, and you have a very large response that is being slow to generate.",
      "tokens": [
        50804,
        407,
        11,
        286,
        519,
        11,
        412,
        1935,
        11,
        291,
        458,
        11,
        264,
        636,
        286,
        600,
        668,
        1953,
        466,
        341,
        307,
        264,
        8535,
        300,
        291,
        434,
        2891,
        365,
        307,
        291,
        445,
        362,
        257,
        588,
        11,
        411,
        11,
        291,
        366,
        1228,
        257,
        13931,
        11373,
        11,
        293,
        291,
        362,
        257,
        588,
        2416,
        4134,
        300,
        307,
        885,
        2964,
        281,
        8460,
        13,
        51636
      ],
      "temperature": 0,
      "avg_logprob": -0.09510249296824137,
      "compression_ratio": 1.8125,
      "no_speech_prob": 1.6867912231918747e-12,
      "speaker_id": 1
    },
    {
      "id": 523,
      "seek": 245920,
      "start": 2459.2,
      "end": 2462.06,
      "text": " And the question is just, are you waiting for that or not?",
      "tokens": [
        50365,
        400,
        264,
        1168,
        307,
        445,
        11,
        366,
        291,
        3806,
        337,
        300,
        420,
        406,
        30,
        50508
      ],
      "temperature": 0,
      "avg_logprob": -0.20270270060717574,
      "compression_ratio": 1.6185185185185185,
      "no_speech_prob": 1.5080707949782779e-12,
      "speaker_id": 1
    },
    {
      "id": 524,
      "seek": 245920,
      "start": 2463.18,
      "end": 2468.2,
      "text": " And in that case, you know, whatever backpressure there is, those same number of bytes are going to go through that relay.",
      "tokens": [
        50564,
        400,
        294,
        300,
        1389,
        11,
        291,
        458,
        11,
        2035,
        646,
        11637,
        540,
        456,
        307,
        11,
        729,
        912,
        1230,
        295,
        36088,
        366,
        516,
        281,
        352,
        807,
        300,
        24214,
        13,
        50815
      ],
      "temperature": 0,
      "avg_logprob": -0.20270270060717574,
      "compression_ratio": 1.6185185185185185,
      "no_speech_prob": 1.5080707949782779e-12,
      "speaker_id": 1
    },
    {
      "id": 525,
      "seek": 245920,
      "start": 2468.54,
      "end": 2468.8599999999997,
      "text": " Yeah.",
      "tokens": [
        50832,
        865,
        13,
        50848
      ],
      "temperature": 0,
      "avg_logprob": -0.20270270060717574,
      "compression_ratio": 1.6185185185185185,
      "no_speech_prob": 1.5080707949782779e-12,
      "speaker_id": 10
    },
    {
      "id": 526,
      "seek": 245920,
      "start": 2468.98,
      "end": 2469.3599999999997,
      "text": " No matter what.",
      "tokens": [
        50854,
        883,
        1871,
        437,
        13,
        50873
      ],
      "temperature": 0,
      "avg_logprob": -0.20270270060717574,
      "compression_ratio": 1.6185185185185185,
      "no_speech_prob": 1.5080707949782779e-12,
      "speaker_id": 1
    },
    {
      "id": 527,
      "seek": 245920,
      "start": 2469.8399999999997,
      "end": 2478.18,
      "text": " I mean, but no, they'll, I mean, I mean, David sort of, and David indicated, like, that the price, the price you paid for this was for foreign secrecy.",
      "tokens": [
        50897,
        286,
        914,
        11,
        457,
        572,
        11,
        436,
        603,
        11,
        286,
        914,
        11,
        286,
        914,
        11,
        4389,
        1333,
        295,
        11,
        293,
        4389,
        16176,
        11,
        411,
        11,
        300,
        264,
        3218,
        11,
        264,
        3218,
        291,
        4835,
        337,
        341,
        390,
        337,
        5329,
        34432,
        1344,
        13,
        51314
      ],
      "temperature": 0,
      "avg_logprob": -0.20270270060717574,
      "compression_ratio": 1.6185185185185185,
      "no_speech_prob": 1.5080707949782779e-12,
      "speaker_id": 10
    },
    {
      "id": 528,
      "seek": 245920,
      "start": 2478.3399999999997,
      "end": 2481.08,
      "text": " There's a bigger price you pay, which is it doesn't work with unmodified servers.",
      "tokens": [
        51322,
        821,
        311,
        257,
        3801,
        3218,
        291,
        1689,
        11,
        597,
        307,
        309,
        1177,
        380,
        589,
        365,
        517,
        8014,
        2587,
        15909,
        13,
        51459
      ],
      "temperature": 0,
      "avg_logprob": -0.20270270060717574,
      "compression_ratio": 1.6185185185185185,
      "no_speech_prob": 1.5080707949782779e-12,
      "speaker_id": 10
    },
    {
      "id": 529,
      "seek": 248108,
      "start": 2481.08,
      "end": 2483.08,
      "text": " Oh, no, absolutely.",
      "tokens": [
        50365,
        876,
        11,
        572,
        11,
        3122,
        13,
        50465
      ],
      "temperature": 0,
      "avg_logprob": -0.15079111523098415,
      "compression_ratio": 1.611336032388664,
      "no_speech_prob": 1.8145618471340774e-12,
      "speaker_id": 1
    },
    {
      "id": 530,
      "seek": 248108,
      "start": 2484.24,
      "end": 2501.96,
      "text": " And the predicate for using OHDP in the first place is that you have a server system where you, they are cooperating to do this, that they are trying to do this, so that you can get better decoupling on a per-message basis than you would if you had to use mask or if you have to do a TLS session per every single day.",
      "tokens": [
        50523,
        400,
        264,
        3852,
        8700,
        337,
        1228,
        13931,
        11373,
        294,
        264,
        700,
        1081,
        307,
        300,
        291,
        362,
        257,
        7154,
        1185,
        689,
        291,
        11,
        436,
        366,
        13414,
        990,
        281,
        360,
        341,
        11,
        300,
        436,
        366,
        1382,
        281,
        360,
        341,
        11,
        370,
        300,
        291,
        393,
        483,
        1101,
        979,
        263,
        11970,
        322,
        257,
        680,
        12,
        76,
        442,
        609,
        5143,
        813,
        291,
        576,
        498,
        291,
        632,
        281,
        764,
        6094,
        420,
        498,
        291,
        362,
        281,
        360,
        257,
        314,
        19198,
        5481,
        680,
        633,
        2167,
        786,
        13,
        51409
      ],
      "temperature": 0,
      "avg_logprob": -0.15079111523098415,
      "compression_ratio": 1.611336032388664,
      "no_speech_prob": 1.8145618471340774e-12,
      "speaker_id": 1
    },
    {
      "id": 531,
      "seek": 248108,
      "start": 2501.96,
      "end": 2503.36,
      "text": " No, I understand that.",
      "tokens": [
        51409,
        883,
        11,
        286,
        1223,
        300,
        13,
        51479
      ],
      "temperature": 0,
      "avg_logprob": -0.15079111523098415,
      "compression_ratio": 1.611336032388664,
      "no_speech_prob": 1.8145618471340774e-12,
      "speaker_id": 10
    },
    {
      "id": 532,
      "seek": 248108,
      "start": 2503.36,
      "end": 2505.1,
      "text": " Is that performance privacy tradeoff?",
      "tokens": [
        51479,
        1119,
        300,
        3389,
        11427,
        4923,
        4506,
        30,
        51566
      ],
      "temperature": 0,
      "avg_logprob": -0.15079111523098415,
      "compression_ratio": 1.611336032388664,
      "no_speech_prob": 1.8145618471340774e-12,
      "speaker_id": 1
    },
    {
      "id": 533,
      "seek": 250510,
      "start": 2505.1,
      "end": 2505.9,
      "text": " No, I understand that.",
      "tokens": [
        50365,
        883,
        11,
        286,
        1223,
        300,
        13,
        50405
      ],
      "temperature": 0,
      "avg_logprob": -0.1470551459617864,
      "compression_ratio": 1.7142857142857142,
      "no_speech_prob": 1.865782268267724e-12,
      "speaker_id": 10
    },
    {
      "id": 534,
      "seek": 250510,
      "start": 2505.92,
      "end": 2511.92,
      "text": " What I'm saying is that when, that, the reason that tradeoff made sense was because you're sending one message.",
      "tokens": [
        50406,
        708,
        286,
        478,
        1566,
        307,
        300,
        562,
        11,
        300,
        11,
        264,
        1778,
        300,
        4923,
        4506,
        1027,
        2020,
        390,
        570,
        291,
        434,
        7750,
        472,
        3636,
        13,
        50706
      ],
      "temperature": 0,
      "avg_logprob": -0.1470551459617864,
      "compression_ratio": 1.7142857142857142,
      "no_speech_prob": 1.865782268267724e-12,
      "speaker_id": 10
    },
    {
      "id": 535,
      "seek": 250510,
      "start": 2512.16,
      "end": 2514.6,
      "text": " And when you're sending a crap ton of messages, a tradeoff no longer makes any sense.",
      "tokens": [
        50718,
        400,
        562,
        291,
        434,
        7750,
        257,
        12426,
        2952,
        295,
        7897,
        11,
        257,
        4923,
        4506,
        572,
        2854,
        1669,
        604,
        2020,
        13,
        50840
      ],
      "temperature": 0,
      "avg_logprob": -0.1470551459617864,
      "compression_ratio": 1.7142857142857142,
      "no_speech_prob": 1.865782268267724e-12,
      "speaker_id": 10
    },
    {
      "id": 536,
      "seek": 250510,
      "start": 2514.88,
      "end": 2514.96,
      "text": " Right.",
      "tokens": [
        50854,
        1779,
        13,
        50858
      ],
      "temperature": 0,
      "avg_logprob": -0.1470551459617864,
      "compression_ratio": 1.7142857142857142,
      "no_speech_prob": 1.865782268267724e-12,
      "speaker_id": 1
    },
    {
      "id": 537,
      "seek": 250510,
      "start": 2515.2999999999997,
      "end": 2520.2,
      "text": " There are also, well, there's also cases, again, kind of going into the parameters of the use case here.",
      "tokens": [
        50875,
        821,
        366,
        611,
        11,
        731,
        11,
        456,
        311,
        611,
        3331,
        11,
        797,
        11,
        733,
        295,
        516,
        666,
        264,
        9834,
        295,
        264,
        764,
        1389,
        510,
        13,
        51120
      ],
      "temperature": 0,
      "avg_logprob": -0.1470551459617864,
      "compression_ratio": 1.7142857142857142,
      "no_speech_prob": 1.865782268267724e-12,
      "speaker_id": 1
    },
    {
      "id": 538,
      "seek": 250510,
      "start": 2520.92,
      "end": 2521.7599999999998,
      "text": " Well, I think you need to.",
      "tokens": [
        51156,
        1042,
        11,
        286,
        519,
        291,
        643,
        281,
        13,
        51198
      ],
      "temperature": 0,
      "avg_logprob": -0.1470551459617864,
      "compression_ratio": 1.7142857142857142,
      "no_speech_prob": 1.865782268267724e-12,
      "speaker_id": 10
    },
    {
      "id": 539,
      "seek": 250510,
      "start": 2521.94,
      "end": 2522.3199999999997,
      "text": " Yes, I know.",
      "tokens": [
        51207,
        1079,
        11,
        286,
        458,
        13,
        51226
      ],
      "temperature": 0,
      "avg_logprob": -0.1470551459617864,
      "compression_ratio": 1.7142857142857142,
      "no_speech_prob": 1.865782268267724e-12,
      "speaker_id": 1
    },
    {
      "id": 540,
      "seek": 250510,
      "start": 2522.56,
      "end": 2526.52,
      "text": " Where upon making a request, you do not necessarily know the size of the response.",
      "tokens": [
        51238,
        2305,
        3564,
        1455,
        257,
        5308,
        11,
        291,
        360,
        406,
        4725,
        458,
        264,
        2744,
        295,
        264,
        4134,
        13,
        51436
      ],
      "temperature": 0,
      "avg_logprob": -0.1470551459617864,
      "compression_ratio": 1.7142857142857142,
      "no_speech_prob": 1.865782268267724e-12,
      "speaker_id": 1
    },
    {
      "id": 541,
      "seek": 250510,
      "start": 2526.52,
      "end": 2532.92,
      "text": " And it may be essentially an extremely short thing that's very appropriate for OHDP, but it may have some slower chunks.",
      "tokens": [
        51436,
        400,
        309,
        815,
        312,
        4476,
        364,
        4664,
        2099,
        551,
        300,
        311,
        588,
        6854,
        337,
        13931,
        11373,
        11,
        457,
        309,
        815,
        362,
        512,
        14009,
        24004,
        13,
        51756
      ],
      "temperature": 0,
      "avg_logprob": -0.1470551459617864,
      "compression_ratio": 1.7142857142857142,
      "no_speech_prob": 1.865782268267724e-12,
      "speaker_id": 1
    },
    {
      "id": 542,
      "seek": 253292,
      "start": 2532.92,
      "end": 2544.3,
      "text": " And having to commit to every single request being a full, you know, back and forth TLS handshake to then send the request in your HTTP session when it may end up being something that comes back in a single RTT is also expensive.",
      "tokens": [
        50365,
        400,
        1419,
        281,
        5599,
        281,
        633,
        2167,
        5308,
        885,
        257,
        1577,
        11,
        291,
        458,
        11,
        646,
        293,
        5220,
        314,
        19198,
        2377,
        34593,
        281,
        550,
        2845,
        264,
        5308,
        294,
        428,
        33283,
        5481,
        562,
        309,
        815,
        917,
        493,
        885,
        746,
        300,
        1487,
        646,
        294,
        257,
        2167,
        21797,
        51,
        307,
        611,
        5124,
        13,
        50934
      ],
      "temperature": 0,
      "avg_logprob": -0.17146927515665691,
      "compression_ratio": 1.669064748201439,
      "no_speech_prob": 1.7128709475094928e-12,
      "speaker_id": 1
    },
    {
      "id": 543,
      "seek": 253292,
      "start": 2544.96,
      "end": 2549.36,
      "text": " I mean, I suppose that might be the case, but I guess, like, I'm waiting to hear a motivating use case for why that would be the case.",
      "tokens": [
        50967,
        286,
        914,
        11,
        286,
        7297,
        300,
        1062,
        312,
        264,
        1389,
        11,
        457,
        286,
        2041,
        11,
        411,
        11,
        286,
        478,
        3806,
        281,
        1568,
        257,
        41066,
        764,
        1389,
        337,
        983,
        300,
        576,
        312,
        264,
        1389,
        13,
        51187
      ],
      "temperature": 0,
      "avg_logprob": -0.17146927515665691,
      "compression_ratio": 1.669064748201439,
      "no_speech_prob": 1.7128709475094928e-12,
      "speaker_id": 10
    },
    {
      "id": 544,
      "seek": 253292,
      "start": 2550,
      "end": 2552.42,
      "text": " And, like, we have some internal thing I'm not going to talk about.",
      "tokens": [
        51219,
        400,
        11,
        411,
        11,
        321,
        362,
        512,
        6920,
        551,
        286,
        478,
        406,
        516,
        281,
        751,
        466,
        13,
        51340
      ],
      "temperature": 0,
      "avg_logprob": -0.17146927515665691,
      "compression_ratio": 1.669064748201439,
      "no_speech_prob": 1.7128709475094928e-12,
      "speaker_id": 10
    },
    {
      "id": 545,
      "seek": 253292,
      "start": 2552.48,
      "end": 2553.1,
      "text": " It doesn't really do it for me.",
      "tokens": [
        51343,
        467,
        1177,
        380,
        534,
        360,
        309,
        337,
        385,
        13,
        51374
      ],
      "temperature": 0,
      "avg_logprob": -0.17146927515665691,
      "compression_ratio": 1.669064748201439,
      "no_speech_prob": 1.7128709475094928e-12,
      "speaker_id": 10
    },
    {
      "id": 546,
      "seek": 255310,
      "start": 2553.1,
      "end": 2555.5,
      "text": " So, like, I guess this isn't the web.",
      "tokens": [
        50365,
        407,
        11,
        411,
        11,
        286,
        2041,
        341,
        1943,
        380,
        264,
        3670,
        13,
        50485
      ],
      "temperature": 0,
      "avg_logprob": -0.27318065477454145,
      "compression_ratio": 1.5537190082644627,
      "no_speech_prob": 2.0012049743028948e-12,
      "speaker_id": 10
    },
    {
      "id": 547,
      "seek": 255310,
      "start": 2555.98,
      "end": 2557.2599999999998,
      "text": " Like, this is not the web.",
      "tokens": [
        50509,
        1743,
        11,
        341,
        307,
        406,
        264,
        3670,
        13,
        50573
      ],
      "temperature": 0,
      "avg_logprob": -0.27318065477454145,
      "compression_ratio": 1.5537190082644627,
      "no_speech_prob": 2.0012049743028948e-12,
      "speaker_id": 10
    },
    {
      "id": 548,
      "seek": 255310,
      "start": 2557.4,
      "end": 2557.8199999999997,
      "text": " These are corporate.",
      "tokens": [
        50580,
        1981,
        366,
        10896,
        13,
        50601
      ],
      "temperature": 0,
      "avg_logprob": -0.27318065477454145,
      "compression_ratio": 1.5537190082644627,
      "no_speech_prob": 2.0012049743028948e-12,
      "speaker_id": 10
    },
    {
      "id": 549,
      "seek": 255310,
      "start": 2558.24,
      "end": 2560.66,
      "text": " Because the endpoint is effectively controlled by both sides, right?",
      "tokens": [
        50622,
        1436,
        264,
        35795,
        307,
        8659,
        10164,
        538,
        1293,
        4881,
        11,
        558,
        30,
        50743
      ],
      "temperature": 0,
      "avg_logprob": -0.27318065477454145,
      "compression_ratio": 1.5537190082644627,
      "no_speech_prob": 2.0012049743028948e-12,
      "speaker_id": 10
    },
    {
      "id": 550,
      "seek": 255310,
      "start": 2560.7799999999997,
      "end": 2560.98,
      "text": " Yes.",
      "tokens": [
        50749,
        1079,
        13,
        50759
      ],
      "temperature": 0,
      "avg_logprob": -0.27318065477454145,
      "compression_ratio": 1.5537190082644627,
      "no_speech_prob": 2.0012049743028948e-12,
      "speaker_id": 10
    },
    {
      "id": 551,
      "seek": 255310,
      "start": 2561.2799999999997,
      "end": 2567.72,
      "text": " And so, like, you ought to have a fair amount of intelligence about what's going on, about what characters' response are likely to be.",
      "tokens": [
        50774,
        400,
        370,
        11,
        411,
        11,
        291,
        13416,
        281,
        362,
        257,
        3143,
        2372,
        295,
        7599,
        466,
        437,
        311,
        516,
        322,
        11,
        466,
        437,
        4342,
        6,
        4134,
        366,
        3700,
        281,
        312,
        13,
        51096
      ],
      "temperature": 0,
      "avg_logprob": -0.27318065477454145,
      "compression_ratio": 1.5537190082644627,
      "no_speech_prob": 2.0012049743028948e-12,
      "speaker_id": 10
    },
    {
      "id": 552,
      "seek": 255310,
      "start": 2568.94,
      "end": 2572.46,
      "text": " So, anyway, I guess I think I'll step back for a bit.",
      "tokens": [
        51157,
        407,
        11,
        4033,
        11,
        286,
        2041,
        286,
        519,
        286,
        603,
        1823,
        646,
        337,
        257,
        857,
        13,
        51333
      ],
      "temperature": 0,
      "avg_logprob": -0.27318065477454145,
      "compression_ratio": 1.5537190082644627,
      "no_speech_prob": 2.0012049743028948e-12,
      "speaker_id": 10
    },
    {
      "id": 553,
      "seek": 255310,
      "start": 2572.46,
      "end": 2572.56,
      "text": " Okay.",
      "tokens": [
        51333,
        1033,
        13,
        51338
      ],
      "temperature": 0,
      "avg_logprob": -0.27318065477454145,
      "compression_ratio": 1.5537190082644627,
      "no_speech_prob": 2.0012049743028948e-12,
      "speaker_id": 1
    },
    {
      "id": 554,
      "seek": 255310,
      "start": 2574.56,
      "end": 2575.04,
      "text": " All right.",
      "tokens": [
        51438,
        1057,
        558,
        13,
        51462
      ],
      "temperature": 0,
      "avg_logprob": -0.27318065477454145,
      "compression_ratio": 1.5537190082644627,
      "no_speech_prob": 2.0012049743028948e-12,
      "speaker_id": 1
    },
    {
      "id": 555,
      "seek": 255310,
      "start": 2577.58,
      "end": 2578.04,
      "text": " Let's see.",
      "tokens": [
        51589,
        961,
        311,
        536,
        13,
        51612
      ],
      "temperature": 0,
      "avg_logprob": -0.27318065477454145,
      "compression_ratio": 1.5537190082644627,
      "no_speech_prob": 2.0012049743028948e-12,
      "speaker_id": 1
    },
    {
      "id": 556,
      "seek": 257804,
      "start": 2578.04,
      "end": 2579.06,
      "text": " All right.",
      "tokens": [
        50365,
        1057,
        558,
        13,
        50416
      ],
      "temperature": 0,
      "avg_logprob": -0.15529321420072306,
      "compression_ratio": 1.5697211155378485,
      "no_speech_prob": 1.6894551079296716e-12,
      "speaker_id": 1
    },
    {
      "id": 557,
      "seek": 257804,
      "start": 2579.38,
      "end": 2590.72,
      "text": " So, going further on the chunk encapsulation, the proposal is essentially what Martin had written up in his PR, like, a year ago with slight modification.",
      "tokens": [
        50432,
        407,
        11,
        516,
        3052,
        322,
        264,
        16635,
        38745,
        2776,
        11,
        264,
        11494,
        307,
        4476,
        437,
        9184,
        632,
        3720,
        493,
        294,
        702,
        11568,
        11,
        411,
        11,
        257,
        1064,
        2057,
        365,
        4036,
        26747,
        13,
        50999
      ],
      "temperature": 0,
      "avg_logprob": -0.15529321420072306,
      "compression_ratio": 1.5697211155378485,
      "no_speech_prob": 1.6894551079296716e-12,
      "speaker_id": 1
    },
    {
      "id": 558,
      "seek": 257804,
      "start": 2591.02,
      "end": 2598.92,
      "text": " So, to preserve the integrity of chunks, HPKE, the sequence of it, HPKE already supports this.",
      "tokens": [
        51014,
        407,
        11,
        281,
        15665,
        264,
        16000,
        295,
        24004,
        11,
        12557,
        8522,
        11,
        264,
        8310,
        295,
        309,
        11,
        12557,
        8522,
        1217,
        9346,
        341,
        13,
        51409
      ],
      "temperature": 0,
      "avg_logprob": -0.15529321420072306,
      "compression_ratio": 1.5697211155378485,
      "no_speech_prob": 1.6894551079296716e-12,
      "speaker_id": 1
    },
    {
      "id": 559,
      "seek": 257804,
      "start": 2599,
      "end": 2602.18,
      "text": " It has its own sequence numbers that needs to be added.",
      "tokens": [
        51413,
        467,
        575,
        1080,
        1065,
        8310,
        3547,
        300,
        2203,
        281,
        312,
        3869,
        13,
        51572
      ],
      "temperature": 0,
      "avg_logprob": -0.15529321420072306,
      "compression_ratio": 1.5697211155378485,
      "no_speech_prob": 1.6894551079296716e-12,
      "speaker_id": 1
    },
    {
      "id": 560,
      "seek": 257804,
      "start": 2602.7,
      "end": 2606.2799999999997,
      "text": " You essentially need to add a counter to the AAD message nonce for responses.",
      "tokens": [
        51598,
        509,
        4476,
        643,
        281,
        909,
        257,
        5682,
        281,
        264,
        316,
        6112,
        3636,
        2107,
        384,
        337,
        13019,
        13,
        51777
      ],
      "temperature": 0,
      "avg_logprob": -0.15529321420072306,
      "compression_ratio": 1.5697211155378485,
      "no_speech_prob": 1.6894551079296716e-12,
      "speaker_id": 1
    },
    {
      "id": 561,
      "seek": 260628,
      "start": 2606.28,
      "end": 2610.02,
      "text": " That is essentially just cribbing what HPKE did.",
      "tokens": [
        50365,
        663,
        307,
        4476,
        445,
        47163,
        4324,
        437,
        12557,
        8522,
        630,
        13,
        50552
      ],
      "temperature": 0,
      "avg_logprob": -0.11541936426986883,
      "compression_ratio": 1.5545023696682465,
      "no_speech_prob": 2.389274975783673e-12,
      "speaker_id": 1
    },
    {
      "id": 562,
      "seek": 260628,
      "start": 2611.32,
      "end": 2622.0400000000004,
      "text": " And then for the final chunk integrity, the proposal here is to, you know, modify the AAD to say, yes, this is some sort of sentinel for marking that this is the final chunk.",
      "tokens": [
        50617,
        400,
        550,
        337,
        264,
        2572,
        16635,
        16000,
        11,
        264,
        11494,
        510,
        307,
        281,
        11,
        291,
        458,
        11,
        16927,
        264,
        316,
        6112,
        281,
        584,
        11,
        2086,
        11,
        341,
        307,
        512,
        1333,
        295,
        2279,
        40952,
        337,
        25482,
        300,
        341,
        307,
        264,
        2572,
        16635,
        13,
        51153
      ],
      "temperature": 0,
      "avg_logprob": -0.11541936426986883,
      "compression_ratio": 1.5545023696682465,
      "no_speech_prob": 2.389274975783673e-12,
      "speaker_id": 1
    },
    {
      "id": 563,
      "seek": 260628,
      "start": 2622.7000000000003,
      "end": 2627.5600000000004,
      "text": " And then with those two things, I believe that it should achieve the requirements on the previous slide.",
      "tokens": [
        51186,
        400,
        550,
        365,
        729,
        732,
        721,
        11,
        286,
        1697,
        300,
        309,
        820,
        4584,
        264,
        7728,
        322,
        264,
        3894,
        4137,
        13,
        51429
      ],
      "temperature": 0,
      "avg_logprob": -0.11541936426986883,
      "compression_ratio": 1.5545023696682465,
      "no_speech_prob": 2.389274975783673e-12,
      "speaker_id": 1
    },
    {
      "id": 564,
      "seek": 262756,
      "start": 2627.56,
      "end": 2644.52,
      "text": " The request response format is also what Martin had proposed, where you can have just a length var int that precedes the different chunks.",
      "tokens": [
        50365,
        440,
        5308,
        4134,
        7877,
        307,
        611,
        437,
        9184,
        632,
        10348,
        11,
        689,
        291,
        393,
        362,
        445,
        257,
        4641,
        1374,
        560,
        300,
        16969,
        279,
        264,
        819,
        24004,
        13,
        51213
      ],
      "temperature": 0,
      "avg_logprob": -0.2486168953680223,
      "compression_ratio": 1.2432432432432432,
      "no_speech_prob": 2.4257637998986725e-12,
      "speaker_id": 1
    },
    {
      "id": 565,
      "seek": 264452,
      "start": 2644.52,
      "end": 2655.48,
      "text": " And then the final chunk is indicated by a length of zero to say that this chunk extends to the end of the stream and that one also would need to have the final sentinel in its AAD.",
      "tokens": [
        50365,
        400,
        550,
        264,
        2572,
        16635,
        307,
        16176,
        538,
        257,
        4641,
        295,
        4018,
        281,
        584,
        300,
        341,
        16635,
        26448,
        281,
        264,
        917,
        295,
        264,
        4309,
        293,
        300,
        472,
        611,
        576,
        643,
        281,
        362,
        264,
        2572,
        2279,
        40952,
        294,
        1080,
        316,
        6112,
        13,
        50913
      ],
      "temperature": 0,
      "avg_logprob": -0.11199065946763562,
      "compression_ratio": 1.4782608695652173,
      "no_speech_prob": 3.0090233690133195e-12,
      "speaker_id": 1
    },
    {
      "id": 566,
      "seek": 264452,
      "start": 2656.78,
      "end": 2660.12,
      "text": " So, that seems to be a very simple, obvious thing to do.",
      "tokens": [
        50978,
        407,
        11,
        300,
        2544,
        281,
        312,
        257,
        588,
        2199,
        11,
        6322,
        551,
        281,
        360,
        13,
        51145
      ],
      "temperature": 0,
      "avg_logprob": -0.11199065946763562,
      "compression_ratio": 1.4782608695652173,
      "no_speech_prob": 3.0090233690133195e-12,
      "speaker_id": 1
    },
    {
      "id": 567,
      "seek": 266012,
      "start": 2660.12,
      "end": 2680.02,
      "text": " One note I'll make is that you don't strictly need to use this particular request response format with the internal crypto, that the length fields here are not covered and they don't necessarily need to be in order to achieve those properties.",
      "tokens": [
        50365,
        1485,
        3637,
        286,
        603,
        652,
        307,
        300,
        291,
        500,
        380,
        20792,
        643,
        281,
        764,
        341,
        1729,
        5308,
        4134,
        7877,
        365,
        264,
        6920,
        17240,
        11,
        300,
        264,
        4641,
        7909,
        510,
        366,
        406,
        5343,
        293,
        436,
        500,
        380,
        4725,
        643,
        281,
        312,
        294,
        1668,
        281,
        4584,
        729,
        7221,
        13,
        51360
      ],
      "temperature": 0,
      "avg_logprob": -0.09025137097227807,
      "compression_ratio": 1.4727272727272727,
      "no_speech_prob": 1.8604306463443354e-12,
      "speaker_id": 1
    },
    {
      "id": 568,
      "seek": 268002,
      "start": 2680.02,
      "end": 2694.14,
      "text": " And so, it does allow you to potentially have other media types or other formats where you, ways to move the chunks around that don't change the properties of the actual cryptographic stream here.",
      "tokens": [
        50365,
        400,
        370,
        11,
        309,
        775,
        2089,
        291,
        281,
        7263,
        362,
        661,
        3021,
        3467,
        420,
        661,
        25879,
        689,
        291,
        11,
        2098,
        281,
        1286,
        264,
        24004,
        926,
        300,
        500,
        380,
        1319,
        264,
        7221,
        295,
        264,
        3539,
        9844,
        12295,
        4309,
        510,
        13,
        51071
      ],
      "temperature": 0,
      "avg_logprob": -0.1608112793934496,
      "compression_ratio": 1.5660377358490567,
      "no_speech_prob": 2.3946144546427295e-12,
      "speaker_id": 1
    },
    {
      "id": 569,
      "seek": 268002,
      "start": 2694.8,
      "end": 2696.86,
      "text": " So, if you'll have concerns with that, we can talk about it.",
      "tokens": [
        51104,
        407,
        11,
        498,
        291,
        603,
        362,
        7389,
        365,
        300,
        11,
        321,
        393,
        751,
        466,
        309,
        13,
        51207
      ],
      "temperature": 0,
      "avg_logprob": -0.1608112793934496,
      "compression_ratio": 1.5660377358490567,
      "no_speech_prob": 2.3946144546427295e-12,
      "speaker_id": 1
    },
    {
      "id": 570,
      "seek": 268002,
      "start": 2696.94,
      "end": 2700.92,
      "text": " But I think we also talk about more of the overall use case going forward.",
      "tokens": [
        51211,
        583,
        286,
        519,
        321,
        611,
        751,
        466,
        544,
        295,
        264,
        4787,
        764,
        1389,
        516,
        2128,
        13,
        51410
      ],
      "temperature": 0,
      "avg_logprob": -0.1608112793934496,
      "compression_ratio": 1.5660377358490567,
      "no_speech_prob": 2.3946144546427295e-12,
      "speaker_id": 1
    },
    {
      "id": 571,
      "seek": 270092,
      "start": 2700.92,
      "end": 2714.7200000000003,
      "text": " And if we do this, we have different media types, we could call it this, and we don't want to call it streamed, we call it chunked, but this is a bike shed to be had.",
      "tokens": [
        50365,
        400,
        498,
        321,
        360,
        341,
        11,
        321,
        362,
        819,
        3021,
        3467,
        11,
        321,
        727,
        818,
        309,
        341,
        11,
        293,
        321,
        500,
        380,
        528,
        281,
        818,
        309,
        4309,
        292,
        11,
        321,
        818,
        309,
        16635,
        292,
        11,
        457,
        341,
        307,
        257,
        5656,
        14951,
        281,
        312,
        632,
        13,
        51055
      ],
      "temperature": 0,
      "avg_logprob": -0.17856087684631347,
      "compression_ratio": 1.5333333333333334,
      "no_speech_prob": 2.019462722047116e-12,
      "speaker_id": 1
    },
    {
      "id": 572,
      "seek": 270092,
      "start": 2716.54,
      "end": 2728.66,
      "text": " And I think, oh yes, also Mark Nottingham had brought up several good points about just like the ways you hold this thing apart from",
      "tokens": [
        51146,
        400,
        286,
        519,
        11,
        1954,
        2086,
        11,
        611,
        3934,
        1726,
        783,
        4822,
        632,
        3038,
        493,
        2940,
        665,
        2793,
        466,
        445,
        411,
        264,
        2098,
        291,
        1797,
        341,
        551,
        4936,
        490,
        51752
      ],
      "temperature": 0,
      "avg_logprob": -0.17856087684631347,
      "compression_ratio": 1.5333333333333334,
      "no_speech_prob": 2.019462722047116e-12,
      "speaker_id": 1
    },
    {
      "id": 573,
      "seek": 272866,
      "start": 2728.66,
      "end": 2734.7,
      "text": " the actual wire protocol, such as, you know, do you have to negotiate or indicate support?",
      "tokens": [
        50365,
        264,
        3539,
        6234,
        10336,
        11,
        1270,
        382,
        11,
        291,
        458,
        11,
        360,
        291,
        362,
        281,
        21713,
        420,
        13330,
        1406,
        30,
        50667
      ],
      "temperature": 0,
      "avg_logprob": -0.19957863105522408,
      "compression_ratio": 1.537117903930131,
      "no_speech_prob": 2.4399540547725973e-12,
      "speaker_id": 1
    },
    {
      "id": 574,
      "seek": 272866,
      "start": 2736.06,
      "end": 2746.14,
      "text": " It is generally OHDP is done kind of with some a priori configuration that is out of band that could indicate whether or not you want to do chunked-ness.",
      "tokens": [
        50735,
        467,
        307,
        5101,
        13931,
        11373,
        307,
        1096,
        733,
        295,
        365,
        512,
        257,
        4059,
        72,
        11694,
        300,
        307,
        484,
        295,
        4116,
        300,
        727,
        13330,
        1968,
        420,
        406,
        291,
        528,
        281,
        360,
        16635,
        292,
        12,
        1287,
        13,
        51239
      ],
      "temperature": 0,
      "avg_logprob": -0.19957863105522408,
      "compression_ratio": 1.537117903930131,
      "no_speech_prob": 2.4399540547725973e-12,
      "speaker_id": 1
    },
    {
      "id": 575,
      "seek": 272866,
      "start": 2747.66,
      "end": 2751.12,
      "text": " How you do this for discovered cases, I'm not sure.",
      "tokens": [
        51315,
        1012,
        291,
        360,
        341,
        337,
        6941,
        3331,
        11,
        286,
        478,
        406,
        988,
        13,
        51488
      ],
      "temperature": 0,
      "avg_logprob": -0.19957863105522408,
      "compression_ratio": 1.537117903930131,
      "no_speech_prob": 2.4399540547725973e-12,
      "speaker_id": 1
    },
    {
      "id": 576,
      "seek": 272866,
      "start": 2751.7999999999997,
      "end": 2755.22,
      "text": " I'm also not aware of use cases for that, particularly.",
      "tokens": [
        51522,
        286,
        478,
        611,
        406,
        3650,
        295,
        764,
        3331,
        337,
        300,
        11,
        4098,
        13,
        51693
      ],
      "temperature": 0,
      "avg_logprob": -0.19957863105522408,
      "compression_ratio": 1.537117903930131,
      "no_speech_prob": 2.4399540547725973e-12,
      "speaker_id": 1
    },
    {
      "id": 577,
      "seek": 275522,
      "start": 2755.22,
      "end": 2764.6,
      "text": " And then there's a question of, you know, can you have asymmetrical cases where the server could reply with chunks and the client did not request with chunks?",
      "tokens": [
        50365,
        400,
        550,
        456,
        311,
        257,
        1168,
        295,
        11,
        291,
        458,
        11,
        393,
        291,
        362,
        37277,
        32283,
        3331,
        689,
        264,
        7154,
        727,
        16972,
        365,
        24004,
        293,
        264,
        6423,
        630,
        406,
        5308,
        365,
        24004,
        30,
        50834
      ],
      "temperature": 0,
      "avg_logprob": -0.13435223523308248,
      "compression_ratio": 1.4782608695652173,
      "no_speech_prob": 2.402501808607127e-12,
      "speaker_id": 1
    },
    {
      "id": 578,
      "seek": 275522,
      "start": 2766.8399999999997,
      "end": 2767.8799999999997,
      "text": " Stuff to be discussed.",
      "tokens": [
        50946,
        31347,
        281,
        312,
        7152,
        13,
        50998
      ],
      "temperature": 0,
      "avg_logprob": -0.13435223523308248,
      "compression_ratio": 1.4782608695652173,
      "no_speech_prob": 2.402501808607127e-12,
      "speaker_id": 1
    },
    {
      "id": 579,
      "seek": 275522,
      "start": 2769.7,
      "end": 2775.3799999999997,
      "text": " Anyway, so the next steps are for me to use this discussion to actually write up the 0, 0.",
      "tokens": [
        51089,
        5684,
        11,
        370,
        264,
        958,
        4439,
        366,
        337,
        385,
        281,
        764,
        341,
        5017,
        281,
        767,
        2464,
        493,
        264,
        1958,
        11,
        1958,
        13,
        51373
      ],
      "temperature": 0,
      "avg_logprob": -0.13435223523308248,
      "compression_ratio": 1.4782608695652173,
      "no_speech_prob": 2.402501808607127e-12,
      "speaker_id": 1
    },
    {
      "id": 580,
      "seek": 277538,
      "start": 2775.38,
      "end": 2780,
      "text": " I've been playing around with implementation of this.",
      "tokens": [
        50365,
        286,
        600,
        668,
        2433,
        926,
        365,
        11420,
        295,
        341,
        13,
        50596
      ],
      "temperature": 0,
      "avg_logprob": -0.16716226967432166,
      "compression_ratio": 1.5246636771300448,
      "no_speech_prob": 2.4235448717324637e-12,
      "speaker_id": 1
    },
    {
      "id": 581,
      "seek": 277538,
      "start": 2780.12,
      "end": 2783.84,
      "text": " I know Chris Wood was also working on this.",
      "tokens": [
        50602,
        286,
        458,
        6688,
        11558,
        390,
        611,
        1364,
        322,
        341,
        13,
        50788
      ],
      "temperature": 0,
      "avg_logprob": -0.16716226967432166,
      "compression_ratio": 1.5246636771300448,
      "no_speech_prob": 2.4235448717324637e-12,
      "speaker_id": 1
    },
    {
      "id": 582,
      "seek": 277538,
      "start": 2784.02,
      "end": 2787.34,
      "text": " Martin Thompson had built a version of this a long time ago.",
      "tokens": [
        50797,
        9184,
        23460,
        632,
        3094,
        257,
        3037,
        295,
        341,
        257,
        938,
        565,
        2057,
        13,
        50963
      ],
      "temperature": 0,
      "avg_logprob": -0.16716226967432166,
      "compression_ratio": 1.5246636771300448,
      "no_speech_prob": 2.4235448717324637e-12,
      "speaker_id": 1
    },
    {
      "id": 583,
      "seek": 277538,
      "start": 2787.8,
      "end": 2792.34,
      "text": " So there's, if people are interested, we can, you know, play with Interop on that.",
      "tokens": [
        50986,
        407,
        456,
        311,
        11,
        498,
        561,
        366,
        3102,
        11,
        321,
        393,
        11,
        291,
        458,
        11,
        862,
        365,
        5751,
        404,
        322,
        300,
        13,
        51213
      ],
      "temperature": 0,
      "avg_logprob": -0.16716226967432166,
      "compression_ratio": 1.5246636771300448,
      "no_speech_prob": 2.4235448717324637e-12,
      "speaker_id": 1
    },
    {
      "id": 584,
      "seek": 277538,
      "start": 2793.04,
      "end": 2795.96,
      "text": " And then we should have more discussion once we have a 0, 0.",
      "tokens": [
        51248,
        400,
        550,
        321,
        820,
        362,
        544,
        5017,
        1564,
        321,
        362,
        257,
        1958,
        11,
        1958,
        13,
        51394
      ],
      "temperature": 0,
      "avg_logprob": -0.16716226967432166,
      "compression_ratio": 1.5246636771300448,
      "no_speech_prob": 2.4235448717324637e-12,
      "speaker_id": 1
    },
    {
      "id": 585,
      "seek": 277538,
      "start": 2796.26,
      "end": 2797.6600000000003,
      "text": " So thank you for the input, everyone.",
      "tokens": [
        51409,
        407,
        1309,
        291,
        337,
        264,
        4846,
        11,
        1518,
        13,
        51479
      ],
      "temperature": 0,
      "avg_logprob": -0.16716226967432166,
      "compression_ratio": 1.5246636771300448,
      "no_speech_prob": 2.4235448717324637e-12,
      "speaker_id": 1
    },
    {
      "id": 586,
      "seek": 279766,
      "start": 2797.66,
      "end": 2801.06,
      "text": " Thanks, Tommy.",
      "tokens": [
        50365,
        2561,
        11,
        19448,
        13,
        50535
      ],
      "temperature": 0,
      "avg_logprob": -0.2473742249724153,
      "compression_ratio": 1.6436781609195403,
      "no_speech_prob": 2.2452348140411527e-12,
      "speaker_id": 7
    },
    {
      "id": 587,
      "seek": 279766,
      "start": 2802.18,
      "end": 2811.3799999999997,
      "text": " So, yeah, I think we are looking for use cases that folks might have, which they feel would benefit from this work,",
      "tokens": [
        50591,
        407,
        11,
        1338,
        11,
        286,
        519,
        321,
        366,
        1237,
        337,
        764,
        3331,
        300,
        4024,
        1062,
        362,
        11,
        597,
        436,
        841,
        576,
        5121,
        490,
        341,
        589,
        11,
        51051
      ],
      "temperature": 0,
      "avg_logprob": -0.2473742249724153,
      "compression_ratio": 1.6436781609195403,
      "no_speech_prob": 2.2452348140411527e-12,
      "speaker_id": 7
    },
    {
      "id": 588,
      "seek": 279766,
      "start": 2811.64,
      "end": 2815.8599999999997,
      "text": " you know, as opposed to, like, masks or something.",
      "tokens": [
        51064,
        291,
        458,
        11,
        382,
        8851,
        281,
        11,
        411,
        11,
        11830,
        420,
        746,
        13,
        51275
      ],
      "temperature": 0,
      "avg_logprob": -0.2473742249724153,
      "compression_ratio": 1.6436781609195403,
      "no_speech_prob": 2.2452348140411527e-12,
      "speaker_id": 7
    },
    {
      "id": 589,
      "seek": 279766,
      "start": 2816.5,
      "end": 2822.14,
      "text": " So, yeah, so if folks have use cases right now that they think would benefit from this, please speak up.",
      "tokens": [
        51307,
        407,
        11,
        1338,
        11,
        370,
        498,
        4024,
        362,
        764,
        3331,
        558,
        586,
        300,
        436,
        519,
        576,
        5121,
        490,
        341,
        11,
        1767,
        1710,
        493,
        13,
        51589
      ],
      "temperature": 0,
      "avg_logprob": -0.2473742249724153,
      "compression_ratio": 1.6436781609195403,
      "no_speech_prob": 2.2452348140411527e-12,
      "speaker_id": 7
    },
    {
      "id": 590,
      "seek": 282214,
      "start": 2822.14,
      "end": 2836.7799999999997,
      "text": " And also, yeah, is there, I guess we can go to that question first, yeah.",
      "tokens": [
        50365,
        400,
        611,
        11,
        1338,
        11,
        307,
        456,
        11,
        286,
        2041,
        321,
        393,
        352,
        281,
        300,
        1168,
        700,
        11,
        1338,
        13,
        51097
      ],
      "temperature": 0,
      "avg_logprob": -0.4026511510213216,
      "compression_ratio": 1.2448979591836735,
      "no_speech_prob": 2.8553273027226433e-12,
      "speaker_id": 7
    },
    {
      "id": 591,
      "seek": 282214,
      "start": 2837.9,
      "end": 2841.62,
      "text": " Hi, I wanted to put in a question for Tommy.",
      "tokens": [
        51153,
        2421,
        11,
        286,
        1415,
        281,
        829,
        294,
        257,
        1168,
        337,
        19448,
        13,
        51339
      ],
      "temperature": 0,
      "avg_logprob": -0.4026511510213216,
      "compression_ratio": 1.2448979591836735,
      "no_speech_prob": 2.8553273027226433e-12,
      "speaker_id": 12
    },
    {
      "id": 592,
      "seek": 282214,
      "start": 2847.46,
      "end": 2847.96,
      "text": " So,",
      "tokens": [
        51631,
        407,
        11,
        51656
      ],
      "temperature": 0,
      "avg_logprob": -0.4026511510213216,
      "compression_ratio": 1.2448979591836735,
      "no_speech_prob": 2.8553273027226433e-12,
      "speaker_id": 12
    },
    {
      "id": 593,
      "seek": 284796,
      "start": 2847.96,
      "end": 2862.48,
      "text": " we, we heard some discussion about the lack of forward secrecy, basically because the client is constantly encrypting to a fixed public key for the gateway.",
      "tokens": [
        50365,
        321,
        11,
        321,
        2198,
        512,
        5017,
        466,
        264,
        5011,
        295,
        2128,
        34432,
        1344,
        11,
        1936,
        570,
        264,
        6423,
        307,
        6460,
        17972,
        662,
        278,
        281,
        257,
        6806,
        1908,
        2141,
        337,
        264,
        28532,
        13,
        51091
      ],
      "temperature": 0,
      "avg_logprob": -0.12065265489661176,
      "compression_ratio": 1.5172413793103448,
      "no_speech_prob": 1.8863268152341517e-12,
      "speaker_id": 12
    },
    {
      "id": 594,
      "seek": 284796,
      "start": 2865.84,
      "end": 2877.08,
      "text": " It's somewhat orthogonal to this specific proposal, but I wonder if we could enable some amount of forward secrecy with oblivious HTTP by, for example,",
      "tokens": [
        51259,
        467,
        311,
        8344,
        41488,
        281,
        341,
        2685,
        11494,
        11,
        457,
        286,
        2441,
        498,
        321,
        727,
        9528,
        512,
        2372,
        295,
        2128,
        34432,
        1344,
        365,
        47039,
        851,
        33283,
        538,
        11,
        337,
        1365,
        11,
        51821
      ],
      "temperature": 0,
      "avg_logprob": -0.12065265489661176,
      "compression_ratio": 1.5172413793103448,
      "no_speech_prob": 1.8863268152341517e-12,
      "speaker_id": 12
    },
    {
      "id": 595,
      "seek": 287708,
      "start": 2877.08,
      "end": 2884.88,
      "text": " providing the gateway a way to rapidly update, provide clients with fresh public keys.",
      "tokens": [
        50365,
        6530,
        264,
        28532,
        257,
        636,
        281,
        12910,
        5623,
        11,
        2893,
        6982,
        365,
        4451,
        1908,
        9317,
        13,
        50755
      ],
      "temperature": 0,
      "avg_logprob": -0.2004351225055632,
      "compression_ratio": 1.4385964912280702,
      "no_speech_prob": 2.1259339774704067e-12,
      "speaker_id": 12
    },
    {
      "id": 596,
      "seek": 287708,
      "start": 2888.04,
      "end": 2890.94,
      "text": " In a, in a chunked world specifically?",
      "tokens": [
        50913,
        682,
        257,
        11,
        294,
        257,
        16635,
        292,
        1002,
        4682,
        30,
        51058
      ],
      "temperature": 0,
      "avg_logprob": -0.2004351225055632,
      "compression_ratio": 1.4385964912280702,
      "no_speech_prob": 2.1259339774704067e-12,
      "speaker_id": 1
    },
    {
      "id": 597,
      "seek": 287708,
      "start": 2891.4,
      "end": 2898.88,
      "text": " I, I think it's orthogonal, except that it would go to one of the objections that's been raised just in this discussion.",
      "tokens": [
        51081,
        286,
        11,
        286,
        519,
        309,
        311,
        41488,
        11,
        3993,
        300,
        309,
        576,
        352,
        281,
        472,
        295,
        264,
        44649,
        300,
        311,
        668,
        6005,
        445,
        294,
        341,
        5017,
        13,
        51455
      ],
      "temperature": 0,
      "avg_logprob": -0.2004351225055632,
      "compression_ratio": 1.4385964912280702,
      "no_speech_prob": 2.1259339774704067e-12,
      "speaker_id": 12
    },
    {
      "id": 598,
      "seek": 289888,
      "start": 2898.88,
      "end": 2904.7000000000003,
      "text": " It feels like then you go down the route of, you're reinventing TLS?",
      "tokens": [
        50365,
        467,
        3417,
        411,
        550,
        291,
        352,
        760,
        264,
        7955,
        295,
        11,
        291,
        434,
        33477,
        278,
        314,
        19198,
        30,
        50656
      ],
      "temperature": 0,
      "avg_logprob": -0.31701045249825094,
      "compression_ratio": 1.423529411764706,
      "no_speech_prob": 2.123992171379485e-12,
      "speaker_id": 1
    },
    {
      "id": 599,
      "seek": 289888,
      "start": 2907.56,
      "end": 2909.48,
      "text": " Oh, oh, you go, right.",
      "tokens": [
        50799,
        876,
        11,
        1954,
        11,
        291,
        352,
        11,
        558,
        13,
        50895
      ],
      "temperature": 0,
      "avg_logprob": -0.31701045249825094,
      "compression_ratio": 1.423529411764706,
      "no_speech_prob": 2.123992171379485e-12,
      "speaker_id": 1
    },
    {
      "id": 600,
      "seek": 289888,
      "start": 2910.48,
      "end": 2913.44,
      "text": " So, it might be simpler than that.",
      "tokens": [
        50945,
        407,
        11,
        309,
        1062,
        312,
        18587,
        813,
        300,
        13,
        51093
      ],
      "temperature": 0,
      "avg_logprob": -0.31701045249825094,
      "compression_ratio": 1.423529411764706,
      "no_speech_prob": 2.123992171379485e-12,
      "speaker_id": 12
    },
    {
      "id": 601,
      "seek": 289888,
      "start": 2913.5,
      "end": 2923.58,
      "text": " For example, we could simply note that the gateway has the ability to create short-lived gateway key configurations",
      "tokens": [
        51096,
        1171,
        1365,
        11,
        321,
        727,
        2935,
        3637,
        300,
        264,
        28532,
        575,
        264,
        3485,
        281,
        1884,
        2099,
        12,
        46554,
        28532,
        2141,
        31493,
        51600
      ],
      "temperature": 0,
      "avg_logprob": -0.31701045249825094,
      "compression_ratio": 1.423529411764706,
      "no_speech_prob": 2.123992171379485e-12,
      "speaker_id": 12
    },
    {
      "id": 602,
      "seek": 292358,
      "start": 2923.58,
      "end": 2930.2599999999998,
      "text": " and very frequently rotated to public key, and then that touches on our consistency discussion.",
      "tokens": [
        50365,
        293,
        588,
        10374,
        42146,
        281,
        1908,
        2141,
        11,
        293,
        550,
        300,
        17431,
        322,
        527,
        14416,
        5017,
        13,
        50699
      ],
      "temperature": 0,
      "avg_logprob": -0.22395551204681396,
      "compression_ratio": 1.373134328358209,
      "no_speech_prob": 1.870189766939312e-12,
      "speaker_id": 12
    },
    {
      "id": 603,
      "seek": 292358,
      "start": 2930.48,
      "end": 2934.94,
      "text": " So, how fast could you rotate your key config while having a working consistency system?",
      "tokens": [
        50710,
        407,
        11,
        577,
        2370,
        727,
        291,
        13121,
        428,
        2141,
        6662,
        1339,
        1419,
        257,
        1364,
        14416,
        1185,
        30,
        50933
      ],
      "temperature": 0,
      "avg_logprob": -0.22395551204681396,
      "compression_ratio": 1.373134328358209,
      "no_speech_prob": 1.870189766939312e-12,
      "speaker_id": 12
    },
    {
      "id": 604,
      "seek": 293494,
      "start": 2934.94,
      "end": 2945.5,
      "text": " Given that, generically for OHDP, the mechanisms for key distribution are left as an exercise to the reader,",
      "tokens": [
        50365,
        18600,
        300,
        11,
        1337,
        984,
        337,
        13931,
        11373,
        11,
        264,
        15902,
        337,
        2141,
        7316,
        366,
        1411,
        382,
        364,
        5380,
        281,
        264,
        15149,
        11,
        50893
      ],
      "temperature": 0,
      "avg_logprob": -0.2400935490926107,
      "compression_ratio": 1.1368421052631579,
      "no_speech_prob": 3.0860314303993164e-12,
      "speaker_id": 1
    },
    {
      "id": 605,
      "seek": 294550,
      "start": 2945.5,
      "end": 2958.3,
      "text": " or, I think stuff like that would need to really be specific to some as yet unspecified protocol for distributing those,",
      "tokens": [
        50365,
        420,
        11,
        286,
        519,
        1507,
        411,
        300,
        576,
        643,
        281,
        534,
        312,
        2685,
        281,
        512,
        382,
        1939,
        2693,
        494,
        66,
        2587,
        10336,
        337,
        41406,
        729,
        11,
        51005
      ],
      "temperature": 0,
      "avg_logprob": -0.1411361075066901,
      "compression_ratio": 1.4955357142857142,
      "no_speech_prob": 2.057099282581909e-12,
      "speaker_id": 1
    },
    {
      "id": 606,
      "seek": 294550,
      "start": 2958.38,
      "end": 2964.32,
      "text": " because there aren't assumptions you can make generically on OHDP about how quickly they're rotating anyway.",
      "tokens": [
        51009,
        570,
        456,
        3212,
        380,
        17695,
        291,
        393,
        652,
        1337,
        984,
        322,
        13931,
        11373,
        466,
        577,
        2661,
        436,
        434,
        19627,
        4033,
        13,
        51306
      ],
      "temperature": 0,
      "avg_logprob": -0.1411361075066901,
      "compression_ratio": 1.4955357142857142,
      "no_speech_prob": 2.057099282581909e-12,
      "speaker_id": 1
    },
    {
      "id": 607,
      "seek": 294550,
      "start": 2965.32,
      "end": 2970.9,
      "text": " You know, it's very possible for existing deployments to rotate those keys every 15 minutes if they want.",
      "tokens": [
        51356,
        509,
        458,
        11,
        309,
        311,
        588,
        1944,
        337,
        6741,
        7274,
        1117,
        281,
        13121,
        729,
        9317,
        633,
        2119,
        2077,
        498,
        436,
        528,
        13,
        51635
      ],
      "temperature": 0,
      "avg_logprob": -0.1411361075066901,
      "compression_ratio": 1.4955357142857142,
      "no_speech_prob": 2.057099282581909e-12,
      "speaker_id": 1
    },
    {
      "id": 608,
      "seek": 297090,
      "start": 2970.9,
      "end": 2978.54,
      "text": " Sure. In that case, it could just be, for example, a BCP to OHDP implementers,",
      "tokens": [
        50365,
        4894,
        13,
        682,
        300,
        1389,
        11,
        309,
        727,
        445,
        312,
        11,
        337,
        1365,
        11,
        257,
        14359,
        47,
        281,
        13931,
        11373,
        4445,
        433,
        11,
        50747
      ],
      "temperature": 0,
      "avg_logprob": -0.23596165543895656,
      "compression_ratio": 1.5193798449612403,
      "no_speech_prob": 2.472053377972072e-12,
      "speaker_id": 12
    },
    {
      "id": 609,
      "seek": 297090,
      "start": 2978.6,
      "end": 2988.1800000000003,
      "text": " and that might also help people feel more comfortable that this is likely to be used in a way that's not as weak as some have been concerned.",
      "tokens": [
        50750,
        293,
        300,
        1062,
        611,
        854,
        561,
        841,
        544,
        4619,
        300,
        341,
        307,
        3700,
        281,
        312,
        1143,
        294,
        257,
        636,
        300,
        311,
        406,
        382,
        5336,
        382,
        512,
        362,
        668,
        5922,
        13,
        51229
      ],
      "temperature": 0,
      "avg_logprob": -0.23596165543895656,
      "compression_ratio": 1.5193798449612403,
      "no_speech_prob": 2.472053377972072e-12,
      "speaker_id": 12
    },
    {
      "id": 610,
      "seek": 297090,
      "start": 2988.46,
      "end": 2989.12,
      "text": " That makes sense.",
      "tokens": [
        51243,
        663,
        1669,
        2020,
        13,
        51276
      ],
      "temperature": 0,
      "avg_logprob": -0.23596165543895656,
      "compression_ratio": 1.5193798449612403,
      "no_speech_prob": 2.472053377972072e-12,
      "speaker_id": 1
    },
    {
      "id": 611,
      "seek": 297090,
      "start": 2989.6600000000003,
      "end": 2989.94,
      "text": " Jonathan.",
      "tokens": [
        51303,
        15471,
        13,
        51317
      ],
      "temperature": 0,
      "avg_logprob": -0.23596165543895656,
      "compression_ratio": 1.5193798449612403,
      "no_speech_prob": 2.472053377972072e-12,
      "speaker_id": 1
    },
    {
      "id": 612,
      "seek": 297090,
      "start": 2990.8,
      "end": 2991.98,
      "text": " Jonathan Neul and Kravler.",
      "tokens": [
        51360,
        15471,
        1734,
        425,
        293,
        591,
        13404,
        1918,
        13,
        51419
      ],
      "temperature": 0,
      "avg_logprob": -0.23596165543895656,
      "compression_ratio": 1.5193798449612403,
      "no_speech_prob": 2.472053377972072e-12,
      "speaker_id": 2
    },
    {
      "id": 613,
      "seek": 297090,
      "start": 2992.52,
      "end": 2994.08,
      "text": " You could put the...",
      "tokens": [
        51446,
        509,
        727,
        829,
        264,
        485,
        51524
      ],
      "temperature": 0,
      "avg_logprob": -0.23596165543895656,
      "compression_ratio": 1.5193798449612403,
      "no_speech_prob": 2.472053377972072e-12,
      "speaker_id": 2
    },
    {
      "id": 614,
      "seek": 297090,
      "start": 2994.08,
      "end": 2995.1600000000003,
      "text": " Wow, this mic is really loud.",
      "tokens": [
        51524,
        3153,
        11,
        341,
        3123,
        307,
        534,
        6588,
        13,
        51578
      ],
      "temperature": 0,
      "avg_logprob": -0.23596165543895656,
      "compression_ratio": 1.5193798449612403,
      "no_speech_prob": 2.472053377972072e-12,
      "speaker_id": 2
    },
    {
      "id": 615,
      "seek": 297090,
      "start": 2995.7400000000002,
      "end": 3000.7400000000002,
      "text": " You could put pre-keys, pre-shared DFY Almond keys in DNS, right?",
      "tokens": [
        51607,
        509,
        727,
        829,
        659,
        12,
        18847,
        11,
        659,
        12,
        2716,
        1642,
        48336,
        56,
        967,
        12171,
        9317,
        294,
        35153,
        11,
        558,
        30,
        51857
      ],
      "temperature": 0,
      "avg_logprob": -0.23596165543895656,
      "compression_ratio": 1.5193798449612403,
      "no_speech_prob": 2.472053377972072e-12,
      "speaker_id": 2
    },
    {
      "id": 616,
      "seek": 300090,
      "start": 3001.14,
      "end": 3005.04,
      "text": " And just change them, every single DNS request.",
      "tokens": [
        50377,
        400,
        445,
        1319,
        552,
        11,
        633,
        2167,
        35153,
        5308,
        13,
        50572
      ],
      "temperature": 0,
      "avg_logprob": -0.2670220502718227,
      "compression_ratio": 1.5769230769230769,
      "no_speech_prob": 2.3892179467494e-12,
      "speaker_id": 2
    },
    {
      "id": 617,
      "seek": 300090,
      "start": 3007.14,
      "end": 3009.02,
      "text": " And just mix that into your key, right?",
      "tokens": [
        50677,
        400,
        445,
        2890,
        300,
        666,
        428,
        2141,
        11,
        558,
        30,
        50771
      ],
      "temperature": 0,
      "avg_logprob": -0.2670220502718227,
      "compression_ratio": 1.5769230769230769,
      "no_speech_prob": 2.3892179467494e-12,
      "speaker_id": 2
    },
    {
      "id": 618,
      "seek": 300090,
      "start": 3009.08,
      "end": 3011.12,
      "text": " Just do, effectively, a DFY-Helman handshake.",
      "tokens": [
        50774,
        1449,
        360,
        11,
        8659,
        11,
        257,
        48336,
        56,
        12,
        39,
        338,
        1601,
        2377,
        34593,
        13,
        50876
      ],
      "temperature": 0,
      "avg_logprob": -0.2670220502718227,
      "compression_ratio": 1.5769230769230769,
      "no_speech_prob": 2.3892179467494e-12,
      "speaker_id": 2
    },
    {
      "id": 619,
      "seek": 300090,
      "start": 3011.7200000000003,
      "end": 3019.06,
      "text": " Yeah, I'm a little worried about the targeting there, but, yes, this is also going beyond the scope of the force.",
      "tokens": [
        50906,
        865,
        11,
        286,
        478,
        257,
        707,
        5804,
        466,
        264,
        17918,
        456,
        11,
        457,
        11,
        2086,
        11,
        341,
        307,
        611,
        516,
        4399,
        264,
        11923,
        295,
        264,
        3464,
        13,
        51273
      ],
      "temperature": 0,
      "avg_logprob": -0.2670220502718227,
      "compression_ratio": 1.5769230769230769,
      "no_speech_prob": 2.3892179467494e-12,
      "speaker_id": 1
    },
    {
      "id": 620,
      "seek": 300090,
      "start": 3019.06,
      "end": 3022.9,
      "text": " I think reinventing MLS is probably out of the scope of the training.",
      "tokens": [
        51273,
        286,
        519,
        33477,
        278,
        376,
        19198,
        307,
        1391,
        484,
        295,
        264,
        11923,
        295,
        264,
        3097,
        13,
        51465
      ],
      "temperature": 0,
      "avg_logprob": -0.2670220502718227,
      "compression_ratio": 1.5769230769230769,
      "no_speech_prob": 2.3892179467494e-12,
      "speaker_id": 6
    },
    {
      "id": 621,
      "seek": 300090,
      "start": 3023.3,
      "end": 3024.64,
      "text": " Hey, hey, hey, I wasn't going to use MLS.",
      "tokens": [
        51485,
        1911,
        11,
        4177,
        11,
        4177,
        11,
        286,
        2067,
        380,
        516,
        281,
        764,
        376,
        19198,
        13,
        51552
      ],
      "temperature": 0,
      "avg_logprob": -0.2670220502718227,
      "compression_ratio": 1.5769230769230769,
      "no_speech_prob": 2.3892179467494e-12,
      "speaker_id": 2
    },
    {
      "id": 622,
      "seek": 300090,
      "start": 3024.7200000000003,
      "end": 3025.54,
      "text": " I was going to use art.",
      "tokens": [
        51556,
        286,
        390,
        516,
        281,
        764,
        1523,
        13,
        51597
      ],
      "temperature": 0,
      "avg_logprob": -0.2670220502718227,
      "compression_ratio": 1.5769230769230769,
      "no_speech_prob": 2.3892179467494e-12,
      "speaker_id": 2
    },
    {
      "id": 623,
      "seek": 300090,
      "start": 3025.76,
      "end": 3026.94,
      "text": " We got rid of that in MLS.",
      "tokens": [
        51608,
        492,
        658,
        3973,
        295,
        300,
        294,
        376,
        19198,
        13,
        51667
      ],
      "temperature": 0,
      "avg_logprob": -0.2670220502718227,
      "compression_ratio": 1.5769230769230769,
      "no_speech_prob": 2.3892179467494e-12,
      "speaker_id": 2
    },
    {
      "id": 624,
      "seek": 302694,
      "start": 3026.94,
      "end": 3033.7400000000002,
      "text": " David Skenazi.",
      "tokens": [
        50365,
        4389,
        318,
        2653,
        26637,
        13,
        50705
      ],
      "temperature": 0,
      "avg_logprob": -0.28096266050596497,
      "compression_ratio": 1.1428571428571428,
      "no_speech_prob": 1.8125767813764737e-12,
      "speaker_id": 5
    },
    {
      "id": 625,
      "seek": 302694,
      "start": 3035,
      "end": 3039.82,
      "text": " Since we're designing the crypto at the microphone, let's do the name, too, and call this Oblivious Mask.",
      "tokens": [
        50768,
        4162,
        321,
        434,
        14685,
        264,
        17240,
        412,
        264,
        10952,
        11,
        718,
        311,
        360,
        264,
        1315,
        11,
        886,
        11,
        293,
        818,
        341,
        4075,
        45997,
        851,
        25414,
        13,
        51009
      ],
      "temperature": 0,
      "avg_logprob": -0.28096266050596497,
      "compression_ratio": 1.1428571428571428,
      "no_speech_prob": 1.8125767813764737e-12,
      "speaker_id": 5
    },
    {
      "id": 626,
      "seek": 303982,
      "start": 3039.82,
      "end": 3052.78,
      "text": " But, no, more seriously, David Benjamin pointed out to me that another security property that this lacks is replay protection.",
      "tokens": [
        50365,
        583,
        11,
        572,
        11,
        544,
        6638,
        11,
        4389,
        22231,
        10932,
        484,
        281,
        385,
        300,
        1071,
        3825,
        4707,
        300,
        341,
        31132,
        307,
        23836,
        6334,
        13,
        51013
      ],
      "temperature": 0,
      "avg_logprob": -0.19120089600725873,
      "compression_ratio": 1.2846153846153847,
      "no_speech_prob": 1.638992327274158e-12,
      "speaker_id": 5
    },
    {
      "id": 627,
      "seek": 303982,
      "start": 3053.1000000000004,
      "end": 3056.44,
      "text": " So, for the same reasons that OHDP does.",
      "tokens": [
        51029,
        407,
        11,
        337,
        264,
        912,
        4112,
        300,
        13931,
        11373,
        775,
        13,
        51196
      ],
      "temperature": 0,
      "avg_logprob": -0.19120089600725873,
      "compression_ratio": 1.2846153846153847,
      "no_speech_prob": 1.638992327274158e-12,
      "speaker_id": 5
    },
    {
      "id": 628,
      "seek": 305644,
      "start": 3056.44,
      "end": 3065.94,
      "text": " The OHDP specs has all of this, but it's not marketed very well.",
      "tokens": [
        50365,
        440,
        13931,
        11373,
        27911,
        575,
        439,
        295,
        341,
        11,
        457,
        309,
        311,
        406,
        49089,
        588,
        731,
        13,
        50840
      ],
      "temperature": 0,
      "avg_logprob": -0.16743008986763333,
      "compression_ratio": 1.2338709677419355,
      "no_speech_prob": 1.8440609282632803e-12,
      "speaker_id": 5
    },
    {
      "id": 629,
      "seek": 305644,
      "start": 3066.1,
      "end": 3070.38,
      "text": " Not everyone in the room knows this or knew this, maybe, initially.",
      "tokens": [
        50848,
        1726,
        1518,
        294,
        264,
        1808,
        3255,
        341,
        420,
        2586,
        341,
        11,
        1310,
        11,
        9105,
        13,
        51062
      ],
      "temperature": 0,
      "avg_logprob": -0.16743008986763333,
      "compression_ratio": 1.2338709677419355,
      "no_speech_prob": 1.8440609282632803e-12,
      "speaker_id": 5
    },
    {
      "id": 630,
      "seek": 305644,
      "start": 3070.66,
      "end": 3071.76,
      "text": " It's easy to forget.",
      "tokens": [
        51076,
        467,
        311,
        1858,
        281,
        2870,
        13,
        51131
      ],
      "temperature": 0,
      "avg_logprob": -0.16743008986763333,
      "compression_ratio": 1.2338709677419355,
      "no_speech_prob": 1.8440609282632803e-12,
      "speaker_id": 5
    },
    {
      "id": 631,
      "seek": 307176,
      "start": 3071.76,
      "end": 3079.86,
      "text": " And this sounds like it's building something that has these weaker properties that people might accidentally use.",
      "tokens": [
        50365,
        400,
        341,
        3263,
        411,
        309,
        311,
        2390,
        746,
        300,
        575,
        613,
        24286,
        7221,
        300,
        561,
        1062,
        15715,
        764,
        13,
        50770
      ],
      "temperature": 0,
      "avg_logprob": -0.14560141707911636,
      "compression_ratio": 1.2410714285714286,
      "no_speech_prob": 1.9441739884662468e-12,
      "speaker_id": 5
    },
    {
      "id": 632,
      "seek": 307176,
      "start": 3079.9,
      "end": 3081.1400000000003,
      "text": " So, there's a risk there.",
      "tokens": [
        50772,
        407,
        11,
        456,
        311,
        257,
        3148,
        456,
        13,
        50834
      ],
      "temperature": 0,
      "avg_logprob": -0.14560141707911636,
      "compression_ratio": 1.2410714285714286,
      "no_speech_prob": 1.9441739884662468e-12,
      "speaker_id": 5
    },
    {
      "id": 633,
      "seek": 308114,
      "start": 3081.14,
      "end": 3092.7599999999998,
      "text": " And another point that Ben made is the security properties of OHDP and this look a lot like 0RTT.",
      "tokens": [
        50365,
        400,
        1071,
        935,
        300,
        3964,
        1027,
        307,
        264,
        3825,
        7221,
        295,
        13931,
        11373,
        293,
        341,
        574,
        257,
        688,
        411,
        1958,
        49,
        28178,
        13,
        50946
      ],
      "temperature": 0,
      "avg_logprob": -0.17850859100754196,
      "compression_ratio": 1.4224598930481283,
      "no_speech_prob": 1.6937164561484086e-12,
      "speaker_id": 5
    },
    {
      "id": 634,
      "seek": 308114,
      "start": 3093.3599999999997,
      "end": 3097.2799999999997,
      "text": " And, actually, it's exactly the same construct as Google Quick 0RTT.",
      "tokens": [
        50976,
        400,
        11,
        767,
        11,
        309,
        311,
        2293,
        264,
        912,
        7690,
        382,
        3329,
        12101,
        1958,
        49,
        28178,
        13,
        51172
      ],
      "temperature": 0,
      "avg_logprob": -0.17850859100754196,
      "compression_ratio": 1.4224598930481283,
      "no_speech_prob": 1.6937164561484086e-12,
      "speaker_id": 5
    },
    {
      "id": 635,
      "seek": 308114,
      "start": 3098.24,
      "end": 3107.58,
      "text": " And Ben's proposal of then you inject another key is exactly how GQuick switches from 0RTT to 1RTT.",
      "tokens": [
        51220,
        400,
        3964,
        311,
        11494,
        295,
        550,
        291,
        10711,
        1071,
        2141,
        307,
        2293,
        577,
        460,
        8547,
        618,
        19458,
        490,
        1958,
        49,
        28178,
        281,
        502,
        49,
        28178,
        13,
        51687
      ],
      "temperature": 0,
      "avg_logprob": -0.17850859100754196,
      "compression_ratio": 1.4224598930481283,
      "no_speech_prob": 1.6937164561484086e-12,
      "speaker_id": 5
    },
    {
      "id": 636,
      "seek": 310758,
      "start": 3107.58,
      "end": 3111.86,
      "text": " But, again, this is reinventing TLS 1.3, which is not the point.",
      "tokens": [
        50365,
        583,
        11,
        797,
        11,
        341,
        307,
        33477,
        278,
        314,
        19198,
        502,
        13,
        18,
        11,
        597,
        307,
        406,
        264,
        935,
        13,
        50579
      ],
      "temperature": 0,
      "avg_logprob": -0.14228384017944337,
      "compression_ratio": 1.5491071428571428,
      "no_speech_prob": 2.2400276078471393e-12,
      "speaker_id": 5
    },
    {
      "id": 637,
      "seek": 310758,
      "start": 3112.74,
      "end": 3112.86,
      "text": " Yeah.",
      "tokens": [
        50623,
        865,
        13,
        50629
      ],
      "temperature": 0,
      "avg_logprob": -0.14228384017944337,
      "compression_ratio": 1.5491071428571428,
      "no_speech_prob": 2.2400276078471393e-12,
      "speaker_id": 1
    },
    {
      "id": 638,
      "seek": 310758,
      "start": 3113.52,
      "end": 3123.7599999999998,
      "text": " But, so, because of all these things, it really feels like this is building a weaker version of TLS, which is TLS that only sends early data, kind of.",
      "tokens": [
        50662,
        583,
        11,
        370,
        11,
        570,
        295,
        439,
        613,
        721,
        11,
        309,
        534,
        3417,
        411,
        341,
        307,
        2390,
        257,
        24286,
        3037,
        295,
        314,
        19198,
        11,
        597,
        307,
        314,
        19198,
        300,
        787,
        14790,
        2440,
        1412,
        11,
        733,
        295,
        13,
        51174
      ],
      "temperature": 0,
      "avg_logprob": -0.14228384017944337,
      "compression_ratio": 1.5491071428571428,
      "no_speech_prob": 2.2400276078471393e-12,
      "speaker_id": 5
    },
    {
      "id": 639,
      "seek": 310758,
      "start": 3124.1,
      "end": 3126.2,
      "text": " And that's not a good thing.",
      "tokens": [
        51191,
        400,
        300,
        311,
        406,
        257,
        665,
        551,
        13,
        51296
      ],
      "temperature": 0,
      "avg_logprob": -0.14228384017944337,
      "compression_ratio": 1.5491071428571428,
      "no_speech_prob": 2.2400276078471393e-12,
      "speaker_id": 5
    },
    {
      "id": 640,
      "seek": 310758,
      "start": 3126.6,
      "end": 3131.7799999999997,
      "text": " So, maybe let's chat offline about these use cases because I think that is the bottom line here.",
      "tokens": [
        51316,
        407,
        11,
        1310,
        718,
        311,
        5081,
        21857,
        466,
        613,
        764,
        3331,
        570,
        286,
        519,
        300,
        307,
        264,
        2767,
        1622,
        510,
        13,
        51575
      ],
      "temperature": 0,
      "avg_logprob": -0.14228384017944337,
      "compression_ratio": 1.5491071428571428,
      "no_speech_prob": 2.2400276078471393e-12,
      "speaker_id": 5
    },
    {
      "id": 641,
      "seek": 313178,
      "start": 3131.78,
      "end": 3141.26,
      "text": " They will have to be incredibly compelling in order to propose something like this with weaker security that people might use and accidentally, in cases where they shouldn't be.",
      "tokens": [
        50365,
        814,
        486,
        362,
        281,
        312,
        6252,
        20050,
        294,
        1668,
        281,
        17421,
        746,
        411,
        341,
        365,
        24286,
        3825,
        300,
        561,
        1062,
        764,
        293,
        15715,
        11,
        294,
        3331,
        689,
        436,
        4659,
        380,
        312,
        13,
        50839
      ],
      "temperature": 0,
      "avg_logprob": -0.17115343377945272,
      "compression_ratio": 1.371069182389937,
      "no_speech_prob": 1.8691688821736996e-12,
      "speaker_id": 5
    },
    {
      "id": 642,
      "seek": 313178,
      "start": 3142.46,
      "end": 3144.26,
      "text": " Like the big file transfer, for example.",
      "tokens": [
        50899,
        1743,
        264,
        955,
        3991,
        5003,
        11,
        337,
        1365,
        13,
        50989
      ],
      "temperature": 0,
      "avg_logprob": -0.17115343377945272,
      "compression_ratio": 1.371069182389937,
      "no_speech_prob": 1.8691688821736996e-12,
      "speaker_id": 5
    },
    {
      "id": 643,
      "seek": 314426,
      "start": 3144.26,
      "end": 3161.5800000000004,
      "text": " One other conclusion that will come from this is that I am very glad that when this was originally just a PR to be like, oh, yeah, we're just going to have all of OHDP be streamed, that I was like, maybe we should not do that.",
      "tokens": [
        50365,
        1485,
        661,
        10063,
        300,
        486,
        808,
        490,
        341,
        307,
        300,
        286,
        669,
        588,
        5404,
        300,
        562,
        341,
        390,
        7993,
        445,
        257,
        11568,
        281,
        312,
        411,
        11,
        1954,
        11,
        1338,
        11,
        321,
        434,
        445,
        516,
        281,
        362,
        439,
        295,
        13931,
        11373,
        312,
        4309,
        292,
        11,
        300,
        286,
        390,
        411,
        11,
        1310,
        321,
        820,
        406,
        360,
        300,
        13,
        51231
      ],
      "temperature": 0,
      "avg_logprob": -0.1340865135192871,
      "compression_ratio": 1.4303797468354431,
      "no_speech_prob": 2.00997074886744e-12,
      "speaker_id": 1
    },
    {
      "id": 644,
      "seek": 316158,
      "start": 3161.58,
      "end": 3171.18,
      "text": " But, so, this conversation bears that out that that was the right call because this does warrant much more discussion and thought.",
      "tokens": [
        50365,
        583,
        11,
        370,
        11,
        341,
        3761,
        17276,
        300,
        484,
        300,
        300,
        390,
        264,
        558,
        818,
        570,
        341,
        775,
        16354,
        709,
        544,
        5017,
        293,
        1194,
        13,
        50845
      ],
      "temperature": 0,
      "avg_logprob": -0.15892099079332853,
      "compression_ratio": 1.4973821989528795,
      "no_speech_prob": 2.078365908195212e-12,
      "speaker_id": 1
    },
    {
      "id": 645,
      "seek": 316158,
      "start": 3171.2799999999997,
      "end": 3174.98,
      "text": " I do think it is still useful for use cases, so, like, you know, we'll work on that.",
      "tokens": [
        50850,
        286,
        360,
        519,
        309,
        307,
        920,
        4420,
        337,
        764,
        3331,
        11,
        370,
        11,
        411,
        11,
        291,
        458,
        11,
        321,
        603,
        589,
        322,
        300,
        13,
        51035
      ],
      "temperature": 0,
      "avg_logprob": -0.15892099079332853,
      "compression_ratio": 1.4973821989528795,
      "no_speech_prob": 2.078365908195212e-12,
      "speaker_id": 1
    },
    {
      "id": 646,
      "seek": 316158,
      "start": 3175.7,
      "end": 3181.94,
      "text": " But it is not yet nearly as obvious, I think, as the simple OHDP case.",
      "tokens": [
        51071,
        583,
        309,
        307,
        406,
        1939,
        6217,
        382,
        6322,
        11,
        286,
        519,
        11,
        382,
        264,
        2199,
        13931,
        11373,
        1389,
        13,
        51383
      ],
      "temperature": 0,
      "avg_logprob": -0.15892099079332853,
      "compression_ratio": 1.4973821989528795,
      "no_speech_prob": 2.078365908195212e-12,
      "speaker_id": 1
    },
    {
      "id": 647,
      "seek": 318194,
      "start": 3181.94,
      "end": 3186.7200000000003,
      "text": " All right, so, Tommy, you're welcome to send in a 0-0 draft.",
      "tokens": [
        50365,
        1057,
        558,
        11,
        370,
        11,
        19448,
        11,
        291,
        434,
        2928,
        281,
        2845,
        294,
        257,
        1958,
        12,
        15,
        11206,
        13,
        50604
      ],
      "temperature": 0,
      "avg_logprob": -0.18077158044885705,
      "compression_ratio": 1.6901408450704225,
      "no_speech_prob": 1.7590469011952159e-12,
      "speaker_id": 6
    },
    {
      "id": 648,
      "seek": 318194,
      "start": 3186.84,
      "end": 3191.36,
      "text": " I think you've gotten a lot of good feedback here, and so we'll look forward to that and handle questions on the list.",
      "tokens": [
        50610,
        286,
        519,
        291,
        600,
        5768,
        257,
        688,
        295,
        665,
        5824,
        510,
        11,
        293,
        370,
        321,
        603,
        574,
        2128,
        281,
        300,
        293,
        4813,
        1651,
        322,
        264,
        1329,
        13,
        50836
      ],
      "temperature": 0,
      "avg_logprob": -0.18077158044885705,
      "compression_ratio": 1.6901408450704225,
      "no_speech_prob": 1.7590469011952159e-12,
      "speaker_id": 6
    },
    {
      "id": 649,
      "seek": 318194,
      "start": 3192.52,
      "end": 3195.98,
      "text": " All right, that brings us to the end of our planned agenda for the day.",
      "tokens": [
        50894,
        1057,
        558,
        11,
        300,
        5607,
        505,
        281,
        264,
        917,
        295,
        527,
        8589,
        9829,
        337,
        264,
        786,
        13,
        51067
      ],
      "temperature": 0,
      "avg_logprob": -0.18077158044885705,
      "compression_ratio": 1.6901408450704225,
      "no_speech_prob": 1.7590469011952159e-12,
      "speaker_id": 6
    },
    {
      "id": 650,
      "seek": 318194,
      "start": 3196.14,
      "end": 3200.3,
      "text": " We have, oh, a little while left in the session in case anyone has any other business.",
      "tokens": [
        51075,
        492,
        362,
        11,
        1954,
        11,
        257,
        707,
        1339,
        1411,
        294,
        264,
        5481,
        294,
        1389,
        2878,
        575,
        604,
        661,
        1606,
        13,
        51283
      ],
      "temperature": 0,
      "avg_logprob": -0.18077158044885705,
      "compression_ratio": 1.6901408450704225,
      "no_speech_prob": 1.7590469011952159e-12,
      "speaker_id": 6
    },
    {
      "id": 651,
      "seek": 318194,
      "start": 3200.8,
      "end": 3202,
      "text": " But otherwise, we'll conclude.",
      "tokens": [
        51308,
        583,
        5911,
        11,
        321,
        603,
        16886,
        13,
        51368
      ],
      "temperature": 0,
      "avg_logprob": -0.18077158044885705,
      "compression_ratio": 1.6901408450704225,
      "no_speech_prob": 1.7590469011952159e-12,
      "speaker_id": 6
    },
    {
      "id": 652,
      "seek": 318194,
      "start": 3202.16,
      "end": 3204,
      "text": " Is there any other business for the good of the order?",
      "tokens": [
        51376,
        1119,
        456,
        604,
        661,
        1606,
        337,
        264,
        665,
        295,
        264,
        1668,
        30,
        51468
      ],
      "temperature": 0,
      "avg_logprob": -0.18077158044885705,
      "compression_ratio": 1.6901408450704225,
      "no_speech_prob": 1.7590469011952159e-12,
      "speaker_id": 6
    },
    {
      "id": 653,
      "seek": 318194,
      "start": 3208.2000000000003,
      "end": 3209.64,
      "text": " All right, then I declare us concluded.",
      "tokens": [
        51678,
        1057,
        558,
        11,
        550,
        286,
        19710,
        505,
        22960,
        13,
        51750
      ],
      "temperature": 0,
      "avg_logprob": -0.18077158044885705,
      "compression_ratio": 1.6901408450704225,
      "no_speech_prob": 1.7590469011952159e-12,
      "speaker_id": 6
    },
    {
      "id": 654,
      "seek": 318194,
      "start": 3209.78,
      "end": 3210.14,
      "text": " Thank you.",
      "tokens": [
        51757,
        1044,
        291,
        13,
        51775
      ],
      "temperature": 0,
      "avg_logprob": -0.18077158044885705,
      "compression_ratio": 1.6901408450704225,
      "no_speech_prob": 1.7590469011952159e-12,
      "speaker_id": 6
    },
    {
      "id": 655,
      "seek": 318194,
      "start": 3210.14,
      "end": 3210.16,
      "text": " Woo!",
      "tokens": [
        51775,
        10468,
        0,
        51776
      ],
      "temperature": 0,
      "avg_logprob": -0.18077158044885705,
      "compression_ratio": 1.6901408450704225,
      "no_speech_prob": 1.7590469011952159e-12
    },
    {
      "id": 656,
      "seek": 321194,
      "start": 3211.94,
      "end": 3214.62,
      "text": " Thanks, Ben, for the notes.",
      "tokens": [
        50365,
        2561,
        11,
        3964,
        11,
        337,
        264,
        5570,
        13,
        50499
      ],
      "temperature": 0,
      "avg_logprob": -0.4758805274963379,
      "compression_ratio": 0.875,
      "no_speech_prob": 1.626081908012722e-11,
      "speaker_id": 7
    },
    {
      "id": 657,
      "seek": 321194,
      "start": 3232.14,
      "end": 3232.52,
      "text": " Thanks.",
      "tokens": [
        51375,
        2561,
        13,
        51394
      ],
      "temperature": 0,
      "avg_logprob": -0.4758805274963379,
      "compression_ratio": 0.875,
      "no_speech_prob": 1.626081908012722e-11
    },
    {
      "id": 658,
      "seek": 324194,
      "start": 3241.94,
      "end": 3242.94,
      "text": " Thanks.",
      "tokens": [
        50365,
        2561,
        13,
        50415
      ],
      "temperature": 1,
      "avg_logprob": -2.136268424987793,
      "compression_ratio": 0.8333333333333334,
      "no_speech_prob": 1.1628193199997305e-11
    },
    {
      "id": 659,
      "seek": 324194,
      "start": 3242.94,
      "end": 3269,
      "text": " Thanks.",
      "tokens": [
        50415,
        2561,
        13,
        51718
      ],
      "temperature": 1,
      "avg_logprob": -2.136268424987793,
      "compression_ratio": 0.8333333333333334,
      "no_speech_prob": 1.1628193199997305e-11
    }
  ],
  "language": "en"
}